{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from enum import auto\n",
    "from enum import Enum\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Finite Markov Decision Processes (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Key Concepts\n",
    "\n",
    "- TODO\n",
    "  - Returns\n",
    "  - Value functions\n",
    "  - Bellman equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Definitions\n",
    "\n",
    "**Agent**: Learner/decision maker.\n",
    "\n",
    "**Envrionment**: Everything outside of the agent that it interacts with (i.e. anything that cannot be changed arbitrarily by the agent). \n",
    "\n",
    "**Action**: Any decision we want the agent to learn how to make.\n",
    "\n",
    "**State**: Anything we can know that might be useful when choosing an action.\n",
    "\n",
    "**Reward**: Numerical values - always scalar - from the environment that the agent seeks to maximize over time through its choice of actions.\n",
    "\n",
    "**Trajectory**: A sequence of actions, states, and rewards that begins like $S_0$, $A_0$, $R_1$, $S_1$, $A_1$, $\\dots$\n",
    "\n",
    "**Markov Decision Process**:\n",
    "\n",
    "**Finite MDP**: An MDP where the possible state, action, and reward sets ($\\mathcal{S}$, $\\mathcal{A}$, $\\mathcal{R}$) are finite.\n",
    "\n",
    "**Markov Property**: \n",
    "\n",
    "**Reward Hypothesis**: All goals can be described by the maximisation of expected cumulative reward.\n",
    "\n",
    "**MDP Dynamics**: \n",
    "\n",
    "**Transition Graph**:\n",
    "\n",
    "**State Nodes**: \n",
    "\n",
    "**Action Nodes**:\n",
    "\n",
    "**Return**: \n",
    "\n",
    "**Episode**:\n",
    "\n",
    "**Terminal State**:\n",
    "\n",
    "**Starting State**:\n",
    "\n",
    "**Absorbing State**: A state that transitions only to itself and generates only rewards of zero.\n",
    "\n",
    "**Episodic Task**:\n",
    "\n",
    "**Continuing Task**:\n",
    "\n",
    "**Discounted Return**:\n",
    "\n",
    "**Discount Rate**:\n",
    "\n",
    "**Value Function**:\n",
    "\n",
    "**Policy**: A mapping from states to probabilities of selecting each possible action. If the agent is following policy $\\pi$ at time $t$, then $\\pi(a | s)$ is the probability \n",
    "\n",
    "**State Value Function**:\n",
    "\n",
    "**Monte Carlo methods**:\n",
    "\n",
    "**Bellman Equation**:\n",
    "\n",
    "**Partial Order**: A set together with a binary relation indicating that, for certain elements in the set, one of the elements precedes the other in the ordering. The relation must be reflexive, antisymmetric, and transitive. (Not all elements need be comparable)\n",
    "\n",
    "**Optimal Policy**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Notation\n",
    "\n",
    "$p(s', r | s, a) \\doteq \\text{Pr}\\{S_t = s', R_t = r | S_{t - 1} = s, A_{t - 1} = a\\}$: The dynamics function for an MDP. Defines the probability of transitioning to state $s'$ and receiving reward $r$ when taking action $a$ in state $s$.\n",
    "\n",
    "$q_*(s, a)$: The value of action $a$ in state $s$.\n",
    "\n",
    "$v_\\pi(s)$: The value function of a state s under policy $\\pi$.\n",
    "\n",
    "$v_*(s)$: The value of each state given optimal action selections.\n",
    "\n",
    "$\\pi(a | s)$: The policy; probability of choosing $A_t = a$ at time $t$ given $S_t = s$.\n",
    "\n",
    "$\\mathcal{S}$: Set of all nonterminal states.\n",
    "\n",
    "$\\mathcal{A}(s)$: Set of all actions available in state $s$. Available actions can depend on the current time step: $\\mathcal{A}_t(s)$.\n",
    "\n",
    "$\\mathcal{R}$: Set of all rewards, a finite subset of the real numbers $\\mathbb{R}$.\n",
    "\n",
    "$\\mathcal{S}$: Set of all nonterminal states. \n",
    "\n",
    "$\\mathcal{S}^+$: Set of all states plus the terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Initial Notes\n",
    "\n",
    "MDPs are a mathematically idealised formalism for sequential decision making.\n",
    "\n",
    "They involved evaluative feedback and are also associative (choose different actions in different situations). \n",
    "\n",
    "Actions influence not just the immediate reward but future situations and therefore future rewards.\n",
    "\n",
    "### 3.1\n",
    "\n",
    "An agent interacts with an environment in a sequence of discrete time steps. \n",
    "\n",
    "The agent selects actions and the environment responds to the actions, presenting new situations and yielding rewards.\n",
    "\n",
    "At time step $t$, the agent receives some representation of the environment state, $S_t \\in \\mathcal{S}$, and selections an action, $A_t \\in \\mathcal{A}(s)$.\n",
    "\n",
    "One step later, it receives a numerical rewards, $R_{t + 1} \\in \\mathcal{R} \\subset \\mathbb{R}$, and finds itself in a new state, $S_{t + 1}$.\n",
    "\n",
    "The MDP and agent together give rise to a _trajectory_ that begins like:\n",
    "\n",
    "$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\dots$\n",
    "\n",
    "Finite MDP: Set of all states, actions and rewards ($\\mathcal{S}$, $\\mathcal{A}$, $\\mathcal{R}$) have a finite number of elements. The random variables $R_t$ and $S_t$ have well defined discrete probability distributions depending only on the preceding state and action. For some $s' \\in \\mathcal{S}$ and $r \\in \\mathcal{R}$:\n",
    "\n",
    "\\begin{equation}\n",
    "    p(s', r | s, a) \\doteq P\\{S_t = s', R_t = r | S_{t - 1} = s, A_{t - 1} = a\\}\n",
    "\\end{equation}\n",
    "\n",
    "$p$ defines the _dynamics_ of the MDP. It is a probability distribution so the following must hold:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r | s, a) = 1, \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\n",
    "\\end{equation}\n",
    "\n",
    "The probability of each possible value for the next state and reward depends only on the immediately preceding state and action. This is best viewed as a restribtion not on the decision process but on the state (i.e. if more information is required about previous time steps, store it in the state). \n",
    "\n",
    "Given the dynamics function $p$ you can compute anything you want to know about the environment.\n",
    "\n",
    "---\n",
    "\n",
    "_state-transition probabilities_:\n",
    "\n",
    "$p(s' | s, a) = \\sum_{r \\in \\mathcal{R}} p(s', r | s, a)$\n",
    "\n",
    "_expected rewards for state-action pairs_: \n",
    "\n",
    "$r(s, a) \\doteq \\mathbb{E}\\left[R_t | S_{t - 1}=s, A_{t - 1} = a \\right] = \\sum_{r \\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s', r | s, a)$\n",
    "\n",
    "_expected rewards for state-action-next-state_: \n",
    "\n",
    "\\begin{align}\n",
    "    r(s, a, s') &\\doteq \\mathbb{E}[R_t | S_{t - 1} = s, A_{t - 1} = a, S_t = s'] \\\\\n",
    "                &= \\sum_{r \\in \\mathcal{R}} P(R_t = r | S_{t - 1} = s, A_{t - 1} = a, S_t = s') \\\\\n",
    "                &= \\sum_{r \\in \\mathcal{R}} \n",
    "                        \\frac{P(R_t = r, S_t = s'| S_{t - 1} = s, A_{t - 1} = a)}\n",
    "                             {P(S_t = s'| S_{t - 1} = s, A_{t - 1} = a)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "---\n",
    "\n",
    "We do not assume that everything in the environment is unknown to the agent - the agent often knows quite a bit about how its rewards are computed as a function of its actions and the states in which they are taken. In some cases, the agent may know everything about how its environment works and still face a difficult reinforcement learning task (e.g. we know the rules of a Rubik's cube, but may still be unable to solve it).\n",
    "\n",
    "The boundary between agent and environment represents the limit of the agents _absolute control_, not of its knowledge.\n",
    "\n",
    "MDP framework is a considerable abstraction. It says that whatever the details of the sensory, memory, and control apparatus, and whatever objective one is trying to achieve, any problem of learning goal-directed behaviour can be reduced to three signals passing back and forth between an agent an its environment: one signal to represent the choices made by the agent (the actions), one signal to represent the basis on which the choices are made (the states), and one signal to define the agent's goal (the rewards). \n",
    "\n",
    "Selecting a representation for the states and actions is more art than science at this point. This book offers practical advice and examples regarding good ways of representing states and actions, but the primary goal is on general principles for learning how to behave once representations have been selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Example 3.3\n",
    "\n",
    "states = {\"high\", \"low\"}\n",
    "\n",
    "actions = {\n",
    "    \"high\": {\"search\", \"wait\"},\n",
    "    \"low\":  {\"search\", \"wait\", \"recharge\"}\n",
    "}\n",
    "\n",
    "alpha = 0.8   # prob. remain high energy after search\n",
    "beta  = 0.4   # prob. remain low  energy after search\n",
    "\n",
    "# transition_probs[current_state][action][new_state]\n",
    "#     - defines the probability of transitioning to new_state\n",
    "#       given action is taken in current_state\n",
    "transition_probs = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "transition_probs[\"high\"][\"search\"  ][\"high\"] = alpha\n",
    "transition_probs[\"high\"][\"search\"  ][\"low\" ] = 1 - alpha\n",
    "transition_probs[\"high\"][\"wait\"    ][\"high\"] = 1.0\n",
    "transition_probs[\"low\" ][\"search\"  ][\"low\" ] = beta\n",
    "transition_probs[\"low\" ][\"search\"  ][\"high\"] = 1 - beta\n",
    "transition_probs[\"low\" ][\"wait\"    ][\"low\" ] = 1.0\n",
    "transition_probs[\"low\" ][\"recharge\"][\"high\"] = 1.0\n",
    "\n",
    "rewards = {\n",
    "    \"can\": 1,\n",
    "    \"rescue\": -3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"449pt\" height=\"250pt\"\n",
       " viewBox=\"0.00 0.00 449.39 250.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 246)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-246 445.3864,-246 445.3864,4 -4,4\"/>\n",
       "<!-- high -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>high</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"27\" cy=\"-83\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-79.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">high</text>\n",
       "</g>\n",
       "<!-- highsearch -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>highsearch</title>\n",
       "<ellipse fill=\"#c0c0c0\" stroke=\"#c0c0c0\" cx=\"148.4469\" cy=\"-165\" rx=\"34.394\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"148.4469\" y=\"-161.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">search</text>\n",
       "</g>\n",
       "<!-- high&#45;&gt;highsearch -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>high&#45;&gt;highsearch</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-dasharray=\"5,2\" d=\"M33.7992,-100.4506C40.8928,-116.2411 53.5719,-138.8298 72,-151 84.2724,-159.1048 99.9531,-162.8459 113.9149,-164.47\"/>\n",
       "</g>\n",
       "<!-- highwait -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>highwait</title>\n",
       "<ellipse fill=\"#c0c0c0\" stroke=\"#c0c0c0\" cx=\"148.4469\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"148.4469\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">wait</text>\n",
       "</g>\n",
       "<!-- high&#45;&gt;highwait -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>high&#45;&gt;highwait</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-dasharray=\"5,2\" d=\"M37.9652,-66.483C46.7822,-57.7161 59.6872,-47.9616 72,-41 87.781,-32.0774 106.5467,-24.1861 121.5362,-19.5454\"/>\n",
       "</g>\n",
       "<!-- low -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>low</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"269.8939\" cy=\"-180\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"269.8939\" y=\"-176.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">low</text>\n",
       "</g>\n",
       "<!-- lowrecharge -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>lowrecharge</title>\n",
       "<ellipse fill=\"#c0c0c0\" stroke=\"#c0c0c0\" cx=\"399.1401\" cy=\"-81\" rx=\"42.4939\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"399.1401\" y=\"-77.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">recharge</text>\n",
       "</g>\n",
       "<!-- low&#45;&gt;lowrecharge -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>low&#45;&gt;lowrecharge</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-dasharray=\"5,2\" d=\"M287.865,-166.2345C311.5868,-148.0641 353.2668,-116.1381 378.3296,-96.9404\"/>\n",
       "</g>\n",
       "<!-- lowsearch -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>lowsearch</title>\n",
       "<ellipse fill=\"#c0c0c0\" stroke=\"#c0c0c0\" cx=\"399.1401\" cy=\"-152\" rx=\"34.394\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"399.1401\" y=\"-148.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">search</text>\n",
       "</g>\n",
       "<!-- low&#45;&gt;lowsearch -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>low&#45;&gt;lowsearch</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-dasharray=\"5,2\" d=\"M294.0138,-171.829C300.7231,-169.7391 308.0479,-167.6257 314.8939,-166 331.4832,-162.0606 350.1688,-158.8353 365.7195,-156.4776\"/>\n",
       "</g>\n",
       "<!-- lowwait -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>lowwait</title>\n",
       "<ellipse fill=\"#c0c0c0\" stroke=\"#c0c0c0\" cx=\"399.1401\" cy=\"-224\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"399.1401\" y=\"-220.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">wait</text>\n",
       "</g>\n",
       "<!-- low&#45;&gt;lowwait -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>low&#45;&gt;lowwait</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-dasharray=\"5,2\" d=\"M296.6738,-183.5241C321.0738,-190.1325 356.5873,-202.3934 378.752,-211.9788\"/>\n",
       "</g>\n",
       "<!-- highsearch&#45;&gt;high -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>highsearch&#45;&gt;high</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M127.1535,-150.6229C106.976,-136.9992 76.5654,-116.4662 54.5493,-101.6011\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"56.2224,-98.5076 45.9761,-95.8125 52.3053,-104.3091 56.2224,-98.5076\"/>\n",
       "<text text-anchor=\"middle\" x=\"84\" y=\"-131.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0.80</text>\n",
       "</g>\n",
       "<!-- highsearch&#45;&gt;low -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>highsearch&#45;&gt;low</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M182.2767,-169.1783C198.0521,-171.1268 216.8621,-173.45 233.0023,-175.4435\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"232.8107,-178.9463 243.1644,-176.6986 233.6688,-171.9991 232.8107,-178.9463\"/>\n",
       "<text text-anchor=\"middle\" x=\"212.8939\" y=\"-177.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0.20</text>\n",
       "</g>\n",
       "<!-- highwait&#45;&gt;high -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>highwait&#45;&gt;high</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M130.2736,-31.3396C114.4184,-38.9732 91.0374,-48.2363 72,-59 67.4776,-61.557 62.8753,-64.4907 58.425,-67.4485\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"56.4346,-64.5692 50.1152,-73.0731 60.3584,-70.3662 56.4346,-64.5692\"/>\n",
       "</g>\n",
       "<!-- lowrecharge&#45;&gt;high -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>lowrecharge&#45;&gt;high</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M356.6005,-80.4495C316.4558,-80.0095 254.5843,-79.5369 200.8939,-80 153.8301,-80.4059 99.6094,-81.4298 64.3959,-82.1685\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"64.0299,-78.6754 54.1067,-82.3877 64.179,-85.6739 64.0299,-78.6754\"/>\n",
       "</g>\n",
       "<!-- lowsearch&#45;&gt;high -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>lowsearch&#45;&gt;high</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M366.4903,-145.9463C297.1314,-133.0861 135.545,-103.1258 63.1765,-89.7076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"63.6387,-86.2337 53.1682,-87.8519 62.3625,-93.1164 63.6387,-86.2337\"/>\n",
       "<text text-anchor=\"middle\" x=\"212.8939\" y=\"-122.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0.60</text>\n",
       "</g>\n",
       "<!-- lowsearch&#45;&gt;low -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>lowsearch&#45;&gt;low</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M366.9203,-158.9801C348.2903,-163.0161 324.8129,-168.1023 305.6189,-172.2605\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"304.7704,-168.8631 295.7382,-174.4011 306.2525,-175.7044 304.7704,-168.8631\"/>\n",
       "<text text-anchor=\"middle\" x=\"326.8939\" y=\"-173.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0.40</text>\n",
       "</g>\n",
       "<!-- lowwait&#45;&gt;low -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>lowwait&#45;&gt;low</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M372.4827,-220.509C351.3492,-214.793 321.8225,-204.8132 299.8695,-196.0055\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"300.9509,-192.6648 290.3733,-192.0607 298.2655,-199.1292 300.9509,-192.6648\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f59f861e3d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = Digraph(graph_attr={\"rankdir\": \"LR\"}, strict=True)\n",
    "\n",
    "# create node for each state\n",
    "for state in states:\n",
    "    dot.node(state)\n",
    "\n",
    "for current_state, state_actions in actions.items():\n",
    "    for action in state_actions:\n",
    "        # create node for taking action in current_state\n",
    "        act_id = current_state + action\n",
    "        dot.node(act_id, style=\"filled\", color=\"gray\", label=action)\n",
    "        # create edge between current_state and node for taking action in current_state\n",
    "        dot.edge(current_state, act_id, arrowhead=\"none\", style=\"dashed\")\n",
    "        \n",
    "        # create nodes for each possible new_state after taking action\n",
    "        # in current_state\n",
    "        for new_state in states:\n",
    "            prob = transition_probs[current_state][action][new_state]\n",
    "            if prob == 0:\n",
    "                # only show transitions that can occur\n",
    "                continue\n",
    "            dot.edge(\n",
    "                act_id, \n",
    "                new_state,\n",
    "                # only show probabilities if not certain\n",
    "                label=None if prob == 1 else f\"{prob:.2f}\"\n",
    "            )\n",
    "    \n",
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "| $s$  | $a$      | $s'$ | $r$ | $p(s', r | s, a)$  |\n",
    "|------|----------|------|-----|--------------------|\n",
    "| high | search   | high | 0   | $\\alpha/2$         | \n",
    "| high | search   | high | 1   | $\\alpha/2$         | \n",
    "| high | search   | low  | 0   | $(1 - \\alpha)/2$   |\n",
    "| high | search   | low  | 1   | $(1 - \\alpha)/2$   |\n",
    "| high | wait     | high | 0   | 0.5                | \n",
    "| high | wait     | high | 1   | 0.5                |\n",
    "| low  | search   | low  | 0   | $\\beta/2$          |\n",
    "| low  | search   | low  | 1   | $\\beta/2$          |\n",
    "| low  | search   | high | -3  | $1 - \\beta$        |\n",
    "| low  | wait     | low  | 0   | 0.5                |\n",
    "| low  | wait     | low  | 1   | 0.5                |\n",
    "| low  | recharge | high | 0   | 1                  |\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 3.2 Goals and Rewards\n",
    "\n",
    "The agent receives a scalar reward signal - $R_t \\in \\mathbb{R}$ - at each time step.\n",
    "\n",
    "The goal is to maximize the total amount of reward it receives, not the immediate reward but the cumulative reward in the long run.\n",
    "\n",
    "_Reward Hypothesis_: All we mean by goals and purposes can be thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\n",
    "\n",
    "The reward signal is the way to tell the agent _what_ it should be achieving. It is not the place to impart prior knowledge about how an agent should achieve a goal. e.g. A chess-playing agent should be rewarded only for winning, not for achieving subgoals such as taking its opponent's pieces. If achieving these sorts of subgoals were rewarded, then the agent might find a way to achieve them without achieving the real goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 3.3 Returns and Episodes\n",
    "\n",
    "In general the aim is to maximize the _expected return_ where return - $G_t$ - is defined as some function of the reward signal.\n",
    "\n",
    "One such function for the return value, where $T$ is the final step:\n",
    "\n",
    "\\begin{equation}\n",
    "    G_t \\doteq R_{t + 1} + R_{t + 2} + R_{t + 3} + \\dots + R_T\n",
    "\\end{equation}\n",
    "\n",
    "The above function is good for agent-environment interaction that breaks naturally into subsequences called **episodes**. Each episode ends in a special state called the **terminal state**, followed by a reset to a standard **starting state** or to a sample from a standard distribution of starting states. \n",
    "\n",
    "Tasks with episodes are called **episodic tasks**.\n",
    "\n",
    "The time of termination - $T$ - is a random variable that normally varies from episode to episode.\n",
    "\n",
    "Tasks that go on continually without limit are **continuing tasks** ($T = \\infty$).\n",
    "\n",
    "The above return formulation would be infinite also. This leads to the concept of _discounting_. The agent tries to select actions so that the _sum of discounted rewards_ it receives over the future is maximized.\n",
    "\n",
    "The agent chooses $A_t$ to maximize the expected **discounted return**:\n",
    "\n",
    "\\begin{align}\n",
    "    G_t &\\doteq R_{t + 1} + \\gamma R_{t + 2} + \\gamma^2R_{t + 3} + \\dots \\\\\n",
    "        &= \\sum_{k = 0}^{\\infty} \\gamma^k R_{t + k + 1}\n",
    "\\end{align}\n",
    "\n",
    "where $\\gamma$ is the **discount rate** with $0 \\leq \\gamma \\leq 1$.\n",
    "\n",
    "This determines the present value of future rewards: a reward received $k$ time steps in the future is worth only $\\gamma^{k - 1}$ times what it would be worth if it were received immediately. \n",
    "\n",
    "If $\\gamma < 1$ the infinite sum has a finite value as long as the reward sequence is bounded (all terms between two values).\n",
    "\n",
    "If $\\gamma = 0$ the agent only maximizes immediate rewards.\n",
    "\n",
    "As $\\gamma$ increases from 0 the agent becomes more farsighted. \n",
    "\n",
    "Returns are related to each other:\n",
    "\n",
    "\\begin{align}\n",
    "    G_t &\\doteq R_{t + 1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots \\\\\n",
    "        &=      R_{t + 1} + \\gamma G_{t + 1}\n",
    "\\end{align}\n",
    "\n",
    "with $G_T \\doteq 0$. \n",
    "\n",
    "For example, if the reward is a constant $+1$ then the return is:\n",
    "\n",
    "\\begin{equation}\n",
    "    G_t = \\sum_{k = 0}^{\\infty} \\gamma^k = \\frac{1}{1 - \\gamma}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Exercise 3.7: If the maze is solvable - which we assume it is - then the agent's expected return is always 1 and doesn't change with each time step. Therefore there is no goal for the agent to actually escape from the maze. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 3.4 Unified Notation for Episodic and Continuing Tasks\n",
    "\n",
    "The previous section discussed _episodic tasks_ and _continuing tasks_. \n",
    "\n",
    "It is useful to establish one notation that can be used for both cases.\n",
    "\n",
    "---\n",
    "\n",
    "Episodic tasks are made up of a series of episodes, each of which consists of a finite sequence of time steps. \n",
    "\n",
    "The time steps for each new episode start from zero. \n",
    "\n",
    "We now index the state $S_{t,i}$ to refer to time step as $t$ in the $i$th episode. \n",
    "\n",
    "When discussing episodic tasks, normally have no need to distinguish between different episodes so often the episode index $i$ is omitted.\n",
    "\n",
    "---\n",
    "\n",
    "Episodic tasks can use the formulation of return that is a sum of infinite rewards by adding a special _absorbing state_ that transitions only to itself and generates only rewards of zero.\n",
    "\n",
    "This means we can write:\n",
    "\n",
    "\\begin{equation}\n",
    "    G_t \\doteq \\sum_{k=t+1}^{T} \\gamma^{k - t - 1} R_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 3.5 Policies and Value Functions\n",
    "\n",
    "Almost all reinforcement learning algorithms involve estimating _value functions_ - functions of states (or of state-action pairs) that estimate how good it is for the agent to be in a given state (expected return).\n",
    "\n",
    "The rewards the agent can expect to receive in the future depend on what actions it will take. Therefore value functions are defined with respect to particular ways of acting, called policies. \n",
    "\n",
    "A _policy_ is a mapping from states to probabilities of selecting each possible action. \n",
    "\n",
    "If the agent following policy $\\pi$ at time $t$ then $\\pi(a | s)$ is the probability that $A_t = a$ if $S_t = s$. \n",
    "\n",
    "Reinforcement learning methods specify how the agent's policy is changed as a result of its experience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "---\n",
    "\n",
    "The expected reward given the current state $S_t$ is:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}_\\pi[ R_{t + 1} | S_t = s] \n",
    "    &= \\sum_r r p(r | s) \\\\\n",
    "    &= \\sum_{r, a} r p(r | s, a) p(a | s) \\\\\n",
    "    &= \\sum_a \\pi(a | s) \\sum_r r p(r | s, a) \\\\\n",
    "    &= \\sum_a \\pi(a | s) \\sum_{r, s'} p(r, s' | s, a) r\n",
    "\\end{align}\n",
    "\n",
    "As a transition graph where there are three possible actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"422pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 422.00 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-184 418,-184 418,4 -4,4\"/>\n",
       "<!-- state -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>state</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"207\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"207\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">s</text>\n",
       "</g>\n",
       "<!-- a0 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>a0</title>\n",
       "<ellipse fill=\"#c0c0c0\" stroke=\"#c0c0c0\" cx=\"99\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">a0</text>\n",
       "</g>\n",
       "<!-- state&#45;&gt;a0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>state&#45;&gt;a0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M187.6918,-149.1278C170.6445,-137.763 145.5981,-121.0654 126.4656,-108.3104\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"128.4031,-105.3956 118.1411,-102.7607 124.5201,-111.2199 128.4031,-105.3956\"/>\n",
       "<text text-anchor=\"middle\" x=\"143.2932\" y=\"-127.8621\" font-family=\"Times,serif\" font-size=\"5.00\" fill=\"#000000\">pi(a0|s)</text>\n",
       "</g>\n",
       "<!-- a1 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>a1</title>\n",
       "<ellipse fill=\"#c0c0c0\" stroke=\"#c0c0c0\" cx=\"207\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"207\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">a1</text>\n",
       "</g>\n",
       "<!-- state&#45;&gt;a1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>state&#45;&gt;a1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M207,-143.8314C207,-136.131 207,-126.9743 207,-118.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"210.5001,-118.4132 207,-108.4133 203.5001,-118.4133 210.5001,-118.4132\"/>\n",
       "<text text-anchor=\"middle\" x=\"197.5\" y=\"-128.1429\" font-family=\"Times,serif\" font-size=\"5.00\" fill=\"#000000\">pi(a1|s)</text>\n",
       "</g>\n",
       "<!-- a2 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>a2</title>\n",
       "<ellipse fill=\"#c0c0c0\" stroke=\"#c0c0c0\" cx=\"315\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"315\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">a2</text>\n",
       "</g>\n",
       "<!-- state&#45;&gt;a2 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>state&#45;&gt;a2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M226.3082,-149.1278C243.3555,-137.763 268.4019,-121.0654 287.5344,-108.3104\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"289.4799,-111.2199 295.8589,-102.7607 285.5969,-105.3956 289.4799,-111.2199\"/>\n",
       "<text text-anchor=\"middle\" x=\"251.7068\" y=\"-127.8621\" font-family=\"Times,serif\" font-size=\"5.00\" fill=\"#000000\">pi(a2|s)</text>\n",
       "</g>\n",
       "<!-- s&#39;0a0 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>s&#39;0a0</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">s&#39;0</text>\n",
       "</g>\n",
       "<!-- a0&#45;&gt;s&#39;0a0 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>a0&#45;&gt;s&#39;0a0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-dasharray=\"5,2\" d=\"M83.7307,-74.7307C73.803,-64.803 60.6847,-51.6847 49.5637,-40.5637\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"51.7933,-37.8436 42.2473,-33.2473 46.8436,-42.7933 51.7933,-37.8436\"/>\n",
       "<text text-anchor=\"middle\" x=\"49.2012\" y=\"-56.2012\" font-family=\"Times,serif\" font-size=\"5.00\" fill=\"#000000\">r(s, a0, s&#39;0)</text>\n",
       "</g>\n",
       "<!-- s&#39;1a0 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>s&#39;1a0</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"99\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">s&#39;1</text>\n",
       "</g>\n",
       "<!-- a0&#45;&gt;s&#39;1a0 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>a0&#45;&gt;s&#39;1a0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-dasharray=\"5,2\" d=\"M99,-71.8314C99,-64.131 99,-54.9743 99,-46.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"102.5001,-46.4132 99,-36.4133 95.5001,-46.4133 102.5001,-46.4132\"/>\n",
       "<text text-anchor=\"middle\" x=\"85\" y=\"-56.1429\" font-family=\"Times,serif\" font-size=\"5.00\" fill=\"#000000\">r(s, a0, s&#39;1)</text>\n",
       "</g>\n",
       "<!-- s&#39;0a1 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>s&#39;0a1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"171\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">s&#39;0</text>\n",
       "</g>\n",
       "<!-- a1&#45;&gt;s&#39;0a1 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>a1&#45;&gt;s&#39;0a1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-dasharray=\"5,2\" d=\"M198.2854,-72.5708C194.0403,-64.0807 188.8464,-53.6929 184.1337,-44.2674\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"187.237,-42.6477 179.6343,-35.2687 180.976,-45.7782 187.237,-42.6477\"/>\n",
       "<text text-anchor=\"middle\" x=\"174.8777\" y=\"-55.7554\" font-family=\"Times,serif\" font-size=\"5.00\" fill=\"#000000\">r(s, a1, s&#39;0)</text>\n",
       "</g>\n",
       "<!-- s&#39;1a1 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>s&#39;1a1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"243\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"243\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">s&#39;1</text>\n",
       "</g>\n",
       "<!-- a1&#45;&gt;s&#39;1a1 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>a1&#45;&gt;s&#39;1a1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-dasharray=\"5,2\" d=\"M215.7146,-72.5708C219.9597,-64.0807 225.1536,-53.6929 229.8663,-44.2674\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"233.024,-45.7782 234.3657,-35.2687 226.763,-42.6477 233.024,-45.7782\"/>\n",
       "<text text-anchor=\"middle\" x=\"211.1223\" y=\"-55.7554\" font-family=\"Times,serif\" font-size=\"5.00\" fill=\"#000000\">r(s, a1, s&#39;1)</text>\n",
       "</g>\n",
       "<!-- s&#39;0a2 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>s&#39;0a2</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"315\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"315\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">s&#39;0</text>\n",
       "</g>\n",
       "<!-- a2&#45;&gt;s&#39;0a2 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>a2&#45;&gt;s&#39;0a2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-dasharray=\"5,2\" d=\"M315,-71.8314C315,-64.131 315,-54.9743 315,-46.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"318.5001,-46.4132 315,-36.4133 311.5001,-46.4133 318.5001,-46.4132\"/>\n",
       "<text text-anchor=\"middle\" x=\"301\" y=\"-56.1429\" font-family=\"Times,serif\" font-size=\"5.00\" fill=\"#000000\">r(s, a2, s&#39;0)</text>\n",
       "</g>\n",
       "<!-- s&#39;1a2 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>s&#39;1a2</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"387\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"387\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">s&#39;1</text>\n",
       "</g>\n",
       "<!-- a2&#45;&gt;s&#39;1a2 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>a2&#45;&gt;s&#39;1a2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" stroke-dasharray=\"5,2\" d=\"M330.2693,-74.7307C340.197,-64.803 353.3153,-51.6847 364.4363,-40.5637\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"367.1564,-42.7933 371.7527,-33.2473 362.2067,-37.8436 367.1564,-42.7933\"/>\n",
       "<text text-anchor=\"middle\" x=\"336.7988\" y=\"-56.2012\" font-family=\"Times,serif\" font-size=\"5.00\" fill=\"#000000\">r(s, a2, s&#39;1)</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f59f85b5bd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = Digraph(edge_attr={\"fontsize\": \"5\"})\n",
    "\n",
    "dot.node(\"state\", label=\"s\")\n",
    "\n",
    "n_actions = 3\n",
    "n_states = 2\n",
    "\n",
    "for act_id in [f\"a{act_id}\" for act_id in range(n_actions)]:\n",
    "    dot.node(act_id, style=\"filled\", color=\"grey\")\n",
    "    dot.edge(\n",
    "        \"state\", \n",
    "        act_id,\n",
    "        xlabel=f\"pi({act_id}|s)\"\n",
    "    )\n",
    "    \n",
    "    for s_id in [f\"s'{s_id}\" for s_id in range(n_states)]:\n",
    "        dot.edge(\n",
    "            act_id, \n",
    "            s_id + act_id, \n",
    "            xlabel=f\"r(s, {act_id}, {s_id})\", \n",
    "            style=\"dashed\"\n",
    "        )\n",
    "        dot.node(s_id + act_id, label=s_id)\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The _state value function_ of a state _s_ under policy _$\\pi$_, denoted $v_\\pi(s)$ is the expected return when starting in $s$ and following $\\pi$ thereafter:\n",
    "\n",
    "\\begin{align}\n",
    "v_\\pi(s) &\\doteq \\mathbb{E}[G_t | S_t = s] \\\\\n",
    "         &= \\mathbb{E}\\left[ \\sum_{k = 0}^{\\infty} \\gamma^k R_{t + k + 1} \\bigg| S_t = s \\right]\n",
    "\\end{align}\n",
    "\n",
    "Similarly, we define the _action-value function for policy $\\pi$ as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\\pi$:\n",
    "\n",
    "\\begin{align}\n",
    "q_\\pi(s, a) &\\doteq \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] \\\\\n",
    "            &= \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} y^k R_{t + k + 1} \\bigg| S_t = s, A_t = a \\right]\n",
    "\\end{align}\n",
    "\n",
    "---\n",
    "\n",
    "The state-value function can be expressed in terms of the action-value function:\n",
    "\n",
    "\\begin{align}\n",
    "v_\\pi(s) &\\doteq \\mathbb{E}_\\pi[G_t | S_t = s] \\\\\n",
    "         &= \\mathbb{E}_\\pi \\left[ \\sum_{k = 0}^{\\infty} \\gamma^k R_{t + k + 1} \\bigg| S_t = s \\right] \\\\\n",
    "         &= \\sum_{k = 0}^{\\infty} \\gamma^k \\mathbb{E}_\\pi\\left[ R_{t + k + 1} \\bigg| S_t = s \\right] \\\\\n",
    "         &= \\sum_{k = 0}^{\\infty} \\gamma^k \\sum_a \\mathbb{E}_\\pi\\left[ R_{t + k + 1} \\bigg| S_t = s, A_t = a \\right] p(a | s) \\\\\n",
    "         &= \\sum_{k = 0}^{\\infty} \\gamma^k \\sum_a \\pi(a | s) q_\\pi(a, s) \\\\\n",
    "\\end{align}\n",
    "\n",
    "i.e. the state-value function is the sum of the expected value at each step, discounted so that later steps have less weighting. The expected value at each time step is the epxectation where the probability is given by the policy, $\\pi$, and the value is given by the action-value function $q_\\pi(s, a)$.\n",
    "\n",
    "The action-value function can be expressed in temrs of the state-value function:\n",
    "\n",
    "TODO\n",
    "\n",
    "---\n",
    "\n",
    "The state-value and action-value functions can be estimated from experience - e.g. agent maintains average of actual returns that have followed that state. Methods that involve averaging over many random samples of actual returns are called **Monte Carlo methods**.\n",
    "\n",
    "If there are many states it is better to store $v_\\pi$ and $q_\\pi$ as parameterised functions and adjust the parameters to better match the observed returns. \n",
    "\n",
    "#### Bellman Equation for $v_\\pi$\n",
    "\n",
    "A fundamental property of value functions used throughout reinforcement learning and dynamic programming is that they satisfy a recursive relationship. \n",
    "\n",
    "For any policy $\\pi$ and any state $s$, the following consistency condition holds betweenthe value of $s$ and the value of its possible successor states:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "v_\\pi(s) &\\doteq \\mathbb{E}\\pi \\left[G_t | S_t = s \\right] \\\\\n",
    "         &= \\mathbb{E}_\\pi \\left[ R_{t + 1} + \\gamma G_{t + 1} | S_t = s \\right] \\\\\n",
    "         &= \\mathbb{E}_\\pi \\left[ R_{t + 1} | S_t = s \\right] + \\gamma \\mathbb{E}_\\pi \\left[G_{t + 1} | S_t = s \\right] \\\\\n",
    "         &= \\mathbb{E}_\\pi \\left[ R_{t + 1} | S_t = s \\right] + \\gamma \\mathbb{E}_\\pi \\left[ \\mathbb{E}_\\pi \\left[ G_{t + 1} | S_{t + 1} = s' \\right ]| S_t = s \\right] \\\\\n",
    "         &= \\mathbb{E}_\\pi \\left[ R_{t + 1} | S_t = s \\right] + \\gamma \\mathbb{E}_\\pi \\left[ v_\\pi(s') | S_t = s \\right] \\\\\n",
    "         &= \\mathbb{E}_\\pi \\left[ R_{t + 1} + \\gamma v_\\pi(s') | S_t = s \\right] \\\\\n",
    "         &= \\sum_{r \\in R_{t + 1}, s' \\in S_{t + 1}} (r + \\gamma v_\\pi(s')) p(r, s' | s) \\\\\n",
    "         &= \\sum_{r \\in R_{t + 1}, s' \\in S_{t + 1}} (r + \\gamma v_\\pi(s')) \\sum_a p(r, s' | s, a) \\pi(a | s) \\\\\n",
    "         &= \\sum_a \\pi(a | s) \\sum_{r, s'} p(r, s' | s, a) (r + \\gamma v_\\pi(s'))\n",
    "\\end{align}\n",
    "\n",
    "The final expression is really a sum of all values of the three variables $a$, $s'$, $r$. For each triple, we compute its probability $\\pi(a | s)p(s', r | s, a)$, weight the quantity in brackets by that probability, then sum over all possibilities to get an expected value. \n",
    "\n",
    "The last line is the **Bellman equation for $v_\\pi$**.\n",
    "\n",
    "i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{a, r, s'} \\pi(a | s) p(r, s' | s, a) (r + \\gamma v_\\pi(s'))\n",
    "\\end{equation}\n",
    "\n",
    "It expresses the relationship between a value of a state and the values of its successor states:\n",
    "\n",
    "---\n",
    "\n",
    "The Bellman equation states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward along the way.\n",
    "\n",
    "---\n",
    "\n",
    "The value function $v_\\pi$ is a unique solution to its Bellman equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Example 3.5, Gridworld\n",
    "#\n",
    "# The following performs policy evaluation for the policy that \n",
    "# chooses each action from a uniform probability distribution.\n",
    "#\n",
    "# It does this using:\n",
    "#\n",
    "#     1. Temporal difference\n",
    "#     2. Analytic solution (system of linear equations)\n",
    "\n",
    "\n",
    "# 25 states aranged into a 5x5 grid:\n",
    "#\n",
    "# |  0 |  1 |  2 |  3 |  4 |\n",
    "# |  5 |  6 |  7 |  8 |  9 |\n",
    "# | 10 | 11 | 12 | 13 | 14 |\n",
    "# | 15 | 16 | 17 | 18 | 19 |\n",
    "# | 20 | 21 | 22 | 23 | 24 |\n",
    "states = np.arange(5*5).reshape(5, 5).tolist()\n",
    "\n",
    "\n",
    "# Four possible moves at each state\n",
    "class Action(Enum):\n",
    "    NORTH = auto()\n",
    "    SOUTH = auto()\n",
    "    EAST = auto()\n",
    "    WEST = auto()\n",
    "\n",
    "    \n",
    "# Dynamics of the environment\n",
    "def dynamics(state: int, action: Action) -> Tuple[int, int]:\n",
    "    if state == 1:\n",
    "        return 21, 10\n",
    "    \n",
    "    if state == 3:\n",
    "        return 13, 5\n",
    "    \n",
    "    if action == Action.NORTH:\n",
    "        if state - 5 < 0:\n",
    "            return state, -1\n",
    "        return state - 5, 0\n",
    "    \n",
    "    if action == Action.SOUTH:\n",
    "        if state + 5 >= 25:\n",
    "            return state, -1\n",
    "        return state + 5, 0\n",
    "    \n",
    "    if action == Action.EAST:\n",
    "        if (state + 1) % 5 == 0:\n",
    "            return state, -1\n",
    "        return state + 1, 0\n",
    "    \n",
    "    if action == Action.WEST:\n",
    "        if state % 5 == 0:\n",
    "            return state, -1\n",
    "        return state - 1, 0\n",
    "    \n",
    "    raise ValueError(f\"unknown action {action}\")\n",
    "\n",
    "\n",
    "# uniform random policy\n",
    "def uniform_policy(state: int) -> Action:\n",
    "    return np.random.choice(Action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd82ba5bd654620a9c580f4b31abd9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|  3.30 | 8.78 | 4.48 | 5.34 | 1.49 |\n",
      "|  1.53 | 3.02 | 2.29 | 1.92 | 0.57 |\n",
      "|  0.07 | 0.74 | 0.69 | 0.38 |-0.40 |\n",
      "| -0.96 |-0.43 |-0.34 |-0.57 |-1.17 |\n",
      "| -1.86 |-1.34 |-1.23 |-1.40 |-1.95 |\n"
     ]
    }
   ],
   "source": [
    "# Computational solution using the temporal difference method\n",
    "# with cosine decay learning rate\n",
    "\n",
    "def cosine_lr(lr_min, lr_max, max_step):\n",
    "    \"\"\"Returns a func that given a step returns a learning rate with cosine decay.\"\"\"\n",
    "    def get_lr(step):\n",
    "        if step >= max_step:\n",
    "            return lr_min\n",
    "        return lr_min + 0.5 * (lr_max - lr_min) * (1 + np.cos(step / max_step * np.pi))\n",
    "    return np.vectorize(get_lr)\n",
    "\n",
    "exp_ret = np.zeros(5*5)     # current estimate of the return for a given state \n",
    "tot_steps = int(5e6)        # total number of steps to take in the random walk\n",
    "s = np.random.choice(5*5)   # random starting position\n",
    "\n",
    "lr_fn = cosine_lr(lr_min=0.0, lr_max=0.25, max_step=tot_steps)\n",
    "\n",
    "# execute random walk\n",
    "for step in tqdm.trange(tot_steps):\n",
    "    s_prime, r = dynamics(s, uniform_policy(s))\n",
    "    exp_ret[s] += lr_fn(step) * ((r + 0.9*exp_ret[s_prime]) - exp_ret[s])\n",
    "    visits[s] += 1\n",
    "    s = s_prime\n",
    "    \n",
    "def display(returns):\n",
    "    \"\"\"Prints grid showing returns for each state.\"\"\"\n",
    "    for row in returns.reshape(5, 5).tolist():\n",
    "        print(\"| \", end=\"\")\n",
    "        for val in row:\n",
    "            print(f\"{val: 2.2f}\", end=\" |\")\n",
    "        print()\n",
    "        \n",
    "display(exp_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  3.31 | 8.79 | 4.43 | 5.32 | 1.49 |\n",
      "|  1.52 | 2.99 | 2.25 | 1.91 | 0.55 |\n",
      "|  0.05 | 0.74 | 0.67 | 0.36 |-0.40 |\n",
      "| -0.97 |-0.44 |-0.35 |-0.59 |-1.18 |\n",
      "| -1.86 |-1.35 |-1.23 |-1.42 |-1.98 |\n"
     ]
    }
   ],
   "source": [
    "# Analytical solution using system of linear equations\n",
    "# 25 equations, one for each state, and 25 unknowns (expected returns)\n",
    "# \n",
    "# Many of the states thankfully share the same equation so we can \n",
    "# fill in their coefficients and dependent value using a for loop\n",
    "\n",
    "coefs = np.zeros((25, 25))\n",
    "deps = np.zeros(25)\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "# Let rx be the expected return for state x and \"rx: f\" \n",
    "# have two parts. \"f\" is the xth row of the resulting system\n",
    "# of linear equations and is derived from the equation for \"rx\"\n",
    "#\n",
    "# r0: 2 = (-4 + 2*gamma)*r0 + gamma*r1 + gamma*r5\n",
    "coefs[0][0] = (-4 + 2*gamma)\n",
    "coefs[0][1] = gamma\n",
    "coefs[0][5] = gamma\n",
    "deps[0] = 2\n",
    "\n",
    "# r1: 10 = r1 - gamma*r21\n",
    "coefs[1][1]  = 1\n",
    "coefs[1][21] = -gamma\n",
    "deps[1] = 10\n",
    "\n",
    "# r2: 1 = (-4 + gamma)*r2 + gamma*r7 + gamma*r3 + gamma*r1\n",
    "coefs[2][2] = (-4 + gamma)\n",
    "coefs[2][7] = gamma\n",
    "coefs[2][3] = gamma\n",
    "coefs[2][1] = gamma\n",
    "deps[2] = 1\n",
    "\n",
    "# r3: 5 = r3 - gamma*r13\n",
    "coefs[3][3] = 1\n",
    "coefs[3][13] = -gamma\n",
    "deps[3] = 5\n",
    "\n",
    "# r4: 2 = (-4 + 2*gamma)*r4 + gamma*r9 + gamma*r3\n",
    "coefs[4][3] = gamma\n",
    "coefs[4][4] = (-4 + 2*gamma)\n",
    "coefs[4][9] = gamma\n",
    "deps[4] = 2\n",
    "\n",
    "# r5, r10, r15: 1 = (-4 + gamma)*rx + gamma*r(x-5) + gamma*r(x+5) + gamma*r(x+1)\n",
    "for x in [5, 10, 15]:\n",
    "    coefs[x][x]   = (-4 + gamma)\n",
    "    coefs[x][x-5] = gamma\n",
    "    coefs[x][x+5] = gamma\n",
    "    coefs[x][x+1] = gamma\n",
    "    deps[x] = 1\n",
    "\n",
    "# r6,  r7,  r8, \n",
    "# r11, r12, r13,\n",
    "# r16, r17, r18: 0 = -4rx + gamma*r(x-5) + gamma*r(x+5) + gamma*r(x+1) + gamma*r(x-1)\n",
    "for x in [6, 7, 8, 11, 12, 13, 16, 17, 18]:\n",
    "    coefs[x][x] = -4\n",
    "    coefs[x][x-5] = gamma\n",
    "    coefs[x][x+5] = gamma\n",
    "    coefs[x][x+1] = gamma\n",
    "    coefs[x][x-1] = gamma\n",
    "    deps[x] = 0\n",
    "    \n",
    "# r9, r14, r19: 1 = (-4 + gamma)*rx + gamma*r(x-5) + gamma*r(x+5) + gamma*r(x-1)\n",
    "for x in [9, 14, 19]:\n",
    "    coefs[x][x]   = (-4 + gamma)\n",
    "    coefs[x][x-5] = gamma\n",
    "    coefs[x][x+5] = gamma\n",
    "    coefs[x][x-1] = gamma\n",
    "    deps[x] = 1\n",
    "    \n",
    "# r20: 2 = (-4 + 2*gamma)*r20 + gamma*r15 + gamma*r21\n",
    "coefs[20][20] = (-4 + 2*gamma)\n",
    "coefs[20][15] = gamma\n",
    "coefs[20][21] = gamma\n",
    "deps[20] = 2\n",
    "\n",
    "# r21, r22, r23: 1 = (-4 + gamma)*rx + gamma*r(x-5) + gamma*r(x+1) + gamma*r(x-1)\n",
    "for x in [21, 22, 23]:\n",
    "    coefs[x][x]   = (-4 + gamma)\n",
    "    coefs[x][x-5] = gamma\n",
    "    coefs[x][x+1] = gamma\n",
    "    coefs[x][x-1] = gamma\n",
    "    deps[x] = 1\n",
    "    \n",
    "# r24: 2 = (-4 + 2*gamma)*r24 + gamma*r19 + gamma*r23\n",
    "coefs[24][24] = (-4 + 2*gamma)\n",
    "coefs[24][19] = gamma\n",
    "coefs[24][23] = gamma\n",
    "deps[24] = 2\n",
    "\n",
    "ret = np.linalg.solve(coefs, deps)\n",
    "\n",
    "display(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational:\n",
      "|  3.30 | 8.78 | 4.48 | 5.34 | 1.49 |\n",
      "|  1.53 | 3.02 | 2.29 | 1.92 | 0.57 |\n",
      "|  0.07 | 0.74 | 0.69 | 0.38 |-0.40 |\n",
      "| -0.96 |-0.43 |-0.34 |-0.57 |-1.17 |\n",
      "| -1.86 |-1.34 |-1.23 |-1.40 |-1.95 |\n",
      "\n",
      "Analytical:\n",
      "|  3.31 | 8.79 | 4.43 | 5.32 | 1.49 |\n",
      "|  1.52 | 2.99 | 2.25 | 1.91 | 0.55 |\n",
      "|  0.05 | 0.74 | 0.67 | 0.36 |-0.40 |\n",
      "| -0.97 |-0.44 |-0.35 |-0.59 |-1.18 |\n",
      "| -1.86 |-1.35 |-1.23 |-1.42 |-1.98 |\n",
      "\n",
      "Absolute difference:\n",
      "|  0.01 | 0.01 | 0.05 | 0.02 | 0.00 |\n",
      "|  0.01 | 0.02 | 0.04 | 0.02 | 0.02 |\n",
      "|  0.02 | 0.01 | 0.02 | 0.02 | 0.00 |\n",
      "|  0.01 | 0.00 | 0.01 | 0.01 | 0.01 |\n",
      "|  0.00 | 0.01 | 0.00 | 0.02 | 0.03 |\n"
     ]
    }
   ],
   "source": [
    "# Showing the expected return from temporal difference with \n",
    "# learning rate annealing to the analytical solution\n",
    "\n",
    "print(\"Computational:\")\n",
    "display(exp_ret)\n",
    "print(\"\\nAnalytical:\")\n",
    "display(ret)\n",
    "print(\"\\nAbsolute difference:\")\n",
    "display(np.abs(exp_ret - ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 3.6: Optimal Policies and Optimal Value Functions\n",
    "\n",
    "Solving a reinforcement learning task means finding a policy that achieves a long of reward over the long run.\n",
    "\n",
    "Value functions define a partial ordering over policies.\n",
    "\n",
    "A policy $\\pi$ is defined to be better than or equal to a policy $\\pi'$ if its expected return is greater than or equal to that of $\\pi'$ for all states:\n",
    "\n",
    "\\begin{equation}\n",
    "    v_{\\pi}(s) \\geq v_{\\pi'}(s), \\space \\forall s \\in \\mathcal{S}\n",
    "\\end{equation}\n",
    "\n",
    "There is at least one **optimal policy** that is better than or equal to all other policies.\n",
    "\n",
    "All the optimal policies are denoted $\\pi_*$ and share the same **optimal state-value function** $v_*$:\n",
    "\n",
    "\\begin{equation}\n",
    "    v_*(s) \\doteq \\max_{\\pi}v_\\pi(s)\n",
    "\\end{equation}\n",
    "\n",
    "All optimal polocies also share the same **optimal action-value function** $q_*(s, a)$:\n",
    "\n",
    "\\begin{equation}\n",
    "    q_*(s, a) \\doteq \\max_\\pi q_\\pi(s, a)\n",
    "\\end{equation}\n",
    "\n",
    "The optimal action-value function can be written in terms of the state-value function:\n",
    "\n",
    "\\begin{equation}\n",
    "    q_*(s, a) = \\mathbb{E}[ R_{t + 1} + \\gamma v_*(S_{t + 1}) | S_t = s, A_t = a ]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

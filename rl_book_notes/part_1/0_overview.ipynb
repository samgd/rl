{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Part 1: Tabular Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Part 1 describes almost all the core ideas of reinforcement learning algorithms in their simplest form.\n",
    "\n",
    "It does this by constraining state and action spaces to be small such that approximate value functions can be represented as arrays/tables.\n",
    "\n",
    "These constraints often allow **tabular methods** to find the optimal value function and optimal policy. **Approximate methods** only find approximate solutions but can be applied effectively to much larger problems.\n",
    "\n",
    "Contents:\n",
    "\n",
    "1. Part 1, Chapter 1, **Bandit Problems**: A solution for methods in which there is only a single state.\n",
    "\n",
    "1. Part 1, Chapter 2, **Finite Markov Decision Processes**: The general reinforcement learning formulation and its main ideas including _Bellman equations_ and _value functions_. \n",
    "\n",
    "1. Part 1, Chapter {3, 4, 5}, **Dynamic Programming**, **Monte Carlo methods**, **temporal-difference learning**: Three fundamental classes of methods for solving finite Markov decision processes. \n",
    "    - Dynamic programming methods: well developed mathematically but require a complete and accurate model of the environment.\n",
    "    - Monte Carlo methods: don't require a model and are conceptually simple, but are not well suited for step-by-step incremental computation.\n",
    "    - Temporal-difference methods: don't require a model and are fully incremental, but are more complex to analyze.\n",
    "    \n",
    "1. Part 1, Chapter 6, **multi-step bootstrapping methods**: Combining the strengths of Monte Carlo methods with the strengths of temporal-difference methods.\n",
    "\n",
    "1. Part 1, Chapter 7, **Complete and unified solution to the tabular reinforcement learning problem**: Show how temporal-difference learning methods can be combined with model learning and planning methods (such as dynamic programming) for a complete and unified solution to the tabular reinforcement learning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interacting with our environment to learn is a natural idea and a fundamental one in nearly all theories of learning and intelligence.\n",
    "\n",
    "The focus is on the _computational_ approach to learning from iteraction, exploring designs for machines that are effective in solving learning problems of scientific or economic interest and evaluating the designs through mathematical analysis or computational experiments.\n",
    "\n",
    "---\n",
    "\n",
    "**Reinforcement Learning**: Learning how to map situations to actions in order to maximize a numerical reward signal.\n",
    "\n",
    "**Reinforcement Learning**: A computational approach to understanding and automating goal-directed learning and decision making.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Reinforcement learning is not supervised learning. Supervised learning is learning from a training set of labeled examples provided by a knowledgable external supervisor/instructor. Each example is a description of a situation as well as which correct action the system should take in that situation. The objective is for the system to generalize its responses so that it acts correctly in situations not present in the training set.\n",
    "\n",
    "Reinforcement learning agents receive evaluation feedback instead of instructive feedback that gives the correct action. Evaluative feedback states how good or bad the taken action was, not whether it was the best or not. Instructive feedback indicates the correct action to take, independently of the action actually taken. \n",
    "\n",
    "In iteractive problems it is often impractical to obtain examples of desired behaviour that are both correct and representative of all the situations in which the agent has to act. Therefore an agent must be able to learn from its own experience. <font color=red>Why is it difficult to obtain?</font>\n",
    "\n",
    "\n",
    "Reinforcement learning is not unsupervised learning. Unsupervised learning aims to find structure hidden in collections of unlabeled data. Reinforcement learning attempts to maximize a reward signal instead of trying to find hidden structure.\n",
    "\n",
    "\n",
    "\n",
    "Reinforcement learning is a third machine learning paradigm and the name actually refers to three distinct things:\n",
    "\n",
    "1. A problem\n",
    "1. A class of solution methods that work well on the problem\n",
    "1. The field that studies this problem and its solution methods\n",
    "\n",
    "\n",
    "The reinforcement learning framework contains an agent and an enviroment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "A complete, iteractive, goal-seeking, decision-making learning agent must be able to:\n",
    "\n",
    "1. Sense the state of its environment to some extent\n",
    "1. Take actions that affect the state\n",
    "1. Have a goal or goals relating to the state of its environment\n",
    "\n",
    "The agent must learn which actions to take in order to maximize the reward. Actions are permitted to influence future situations/the future state of the environment and therefore also impact future rewards. The correct action choice requires taking into account indirect, delayed consequences of actions, and thus may require foresight or planning. \n",
    "\n",
    "The agent must operate despite significant uncertainty about the environment it faces. When planning is involved the agent has to address the interplay between planning and real-time action selection.\n",
    "\n",
    "The agent can use its experience to improve performance its over time.\n",
    "\n",
    "The agent contains some **state**. The state is a signal conveying to the agent some sense of how the environment is at a particular time. Informally, this can be thought of as whatever information is available to the agent about its environment. \n",
    "\n",
    "### Exploration and Exploitation\n",
    "\n",
    "To obtain a lot of reward, a reinforcement learning agent must prefer actions that it has tried in the past and found to be effective in producing reward. However, to discover such actions it has to try actions that it has not selected before.\n",
    "\n",
    "**Explore** in order to make better action selections in the future.\n",
    "\n",
    "**Exploit** what it has already experienced in order to obtain reward.\n",
    "\n",
    "The agent cannot exclusively do one of these without failing at the task, therefore it must try a variety of actions _and_ progressively favour those actions that appear to be best. On a stochastic task, the action must be tried many times to gain a reliable estimate of its expected reward.\n",
    "\n",
    "---\n",
    "\n",
    "**Stochastic**/**Stochastic process**: A family (indexed set) of random variables where the indices are usually viewed as points in time giving the interpretation of a stochastic process representing numerical values of some system randomly changing over time. \n",
    "\n",
    "---\n",
    "\n",
    "This exploration and exploitation trade-off does not appear in supervised or unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning System\n",
    "\n",
    "Beyond the agent and the environment there are four main parts of a reinforcement learning system:\n",
    "\n",
    "1. **Policy**: a mapping from perceived states of the environment to actions to be taken when in those states. It may be deterministic or stochastic and may be simple, such as a lookup table, or require a lot of computation such as a search. It is the core of a reinforcement learning agent. It alone is sufficient to determine behaviour.\n",
    "\n",
    "1. **Reward Signal**: defines the goal of a reinforcement learning problem. On each time step, the environment sends to the reinforcement learning agent a single number called the _reward_. The agent's goal is to maximize the total reward it receives over the long run. Reward signals may be stochastic functions of the state of the environment and the actions taken. \n",
    "\n",
    "1. **Value Function**: indicates what is good in the long run (whereas the reward signal indicates what is good in an immediate sense). The _value_ of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state. Values indicates the long-term desirability of states after taking into account the states that are likely to follow and the rewards available in those states (e.g. a state may yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards). It is values we are most concerned about when making and evaluating decisions as this leads to the greatest reward over the long term.\n",
    "\n",
    "1. Optional **Environment Model**: a model that mimics the behaviour of the environment, or more generally, that allows inferences to be made about how the environment will behave (e.g. given a state and action, the model might predict the resultant next state and next reward). Models are used for **planning**: any way of deciding on a course of action by considering possible future situations before they are actually experied. Methods that use models and planning are called **model-based** methods. **Model-free** methods are explictily trial-and-error learners. \n",
    "\n",
    "\n",
    "Most reinforcement learning methods are structured around estimating value functions, however it is not strictly necessary to do this to solve reinforcement learning problems. \n",
    "\n",
    "**Evoluationary methods** never estimate value functions. e.g. Genetic algorithms, genetic programming, simulated annealing, and other optimization methods. These methods apply multiple static policies each interacting over an extended period of time with a separate instance of the environment. The policies that obtain the most reward, and random variations of them, are carried over to the next generation of policies, and the process repeats. \n",
    "\n",
    "Evoluationary methods can be effective if at least one of the following is true:\n",
    "\n",
    "1. The space of policies is sufficiently small\n",
    "1. The space of policies can be structured so that good policies are common or easy to find\n",
    "1. A lot of time is available for the search\n",
    "\n",
    "They also have advantages on problems in which the learning agent cannot sense the complete state of its environment. \n",
    "\n",
    "Methods that can take advantage of the details of each individual behaviour are much more efficient than evolutionary methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

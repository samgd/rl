{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import enum\n",
    "import multiprocessing\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from itertools import product\n",
    "from typing import Callable\n",
    "from typing import Iterable\n",
    "from typing import Mapping\n",
    "from typing import Optional\n",
    "from typing import Sequence\n",
    "from typing import Tuple\n",
    "from typing import TypeVar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # doctest to catch regressions, copy below relevant code section\n",
    "# import doctest\n",
    "# doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods\n",
    "\n",
    "## <p style=\"color:red\">Text content copied near verbatim from: <a href=\"http://incompleteideas.net/book/the-book-2nd.html\">Sutton and Barto</a>. Code is my own unless otherwise stated.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "**Monte Carlo (MC) Methods**: \n",
    "- Generic definition: Repeated random sampling to obtain numerical results. \n",
    "- RL definition: A way of solving the RL problem based on averaging sample returns.\n",
    "\n",
    "**Control Problem** (Reminder?):\n",
    "\n",
    "**First-visit MC Method**:\n",
    "\n",
    "**Every-visit MC Method**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo methods only require experience - sample sequences of states, actions, and rewards from actual or simulated interaction with an environment. They do _not_ require complete knowledge of the environment dynamics. \n",
    "\n",
    "For simulated learning a model is required but it only needs to generate sample transitions and not the complete probability distribution of all possible transitions. There are a surprising number of places where it is easy to generate experience sampled according to the desired probability distribution but infeasible to obtain the distributions in explicit form.\n",
    "\n",
    "Monte Carlo methods solve the RL problem by averaging sample returns. Here we only consider Monte Carlo methods for episodic tasks. Only on completion of an episode are value estimates and policies changed. This means they can be used incrementally in an episode-by-episode nature but not in an online step-by-step way. \n",
    "\n",
    "In the previous notebook we looked at dynamic programming which _computed_ value functions from knowledge of the MDP. Monte Carlo methods here _learn_ value functions from sample returns with the MDP. The value functions and corresponding policies still interact to obtain optimality via general policy iteration. \n",
    "\n",
    "As per the previous chapter, we start by looking at the prediction problem, then the control problem and its solution by GPI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1: Monte Carlo Prediction\n",
    "\n",
    "The \"obvious\" way to learn the state-value function - expected cumulative sum of future discounted reward - for a given policy is to average the returns observed after visits to a state. As more returns are observed, the average should converge to the expected value.\n",
    "\n",
    "To estimate $v_\\pi(s)$ we define:\n",
    "\n",
    "- _visit_ to $s$: each occurrence of state $s$ in an episode\n",
    "- _first visit_: the first time s is visited in an episode\n",
    "\n",
    "This leads to the following MC methods:\n",
    "\n",
    "- _first visit MC method_: estimates $v_\\pi(s)$ as the average returns following first visits to $s$\n",
    "- _every visit MC method_: averages returns following _all_ visits to $s$\n",
    "\n",
    "These are similar but have different theoretical properties.\n",
    "\n",
    "This chapter looks at first-visit MC. Every-visit MC extends more naturally to function approximation and eligibility tracing (ch 9/12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "StateT = TypeVar(\"StateT\")\n",
    "ActionT = TypeVar(\"ActionT\")\n",
    "RewardT = TypeVar(\"RewardT\", int, float)\n",
    "\n",
    "TraceT = Sequence[Tuple[StateT, ActionT, RewardT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc(\n",
    "    states: Sequence[StateT],\n",
    "    episodes: Iterable[TraceT],\n",
    "    gamma: float = 0.9\n",
    ") -> Mapping[StateT, RewardT]:\n",
    "    \"\"\"\n",
    "    \n",
    "    Example:\n",
    "        >>> def gen_ep(n_episodes):\n",
    "        ...     for _ in range(n_episodes):\n",
    "        ...         yield [(0, 0, 0.5), (0, 0, 0.75), (1, 0, 1.0), (0, 0, 0.75)]\n",
    "        >>> value_map = first_visit_mc(states=[0, 1], episodes=gen_ep(1), gamma=0.5)\n",
    "        >>> value_map[0] == 1.21875\n",
    "        True\n",
    "        >>> value_map[1] == 1.375\n",
    "        True\n",
    "    \"\"\"\n",
    "    values = dict((s, 0.0) for s in states)                    \n",
    "    returns = defaultdict(list)\n",
    "    \n",
    "    for trace in tqdm.tqdm(episodes):\n",
    "        ret = 0.0\n",
    "        fv = first_visits(trace)\n",
    "        for first_visit, (state, _, reward) in reversed(list(zip(fv, trace))):\n",
    "            ret = reward + gamma*ret\n",
    "            \n",
    "            if first_visit:\n",
    "                returns[state].append(ret)\n",
    "                \n",
    "    for s in states:\n",
    "        rets = returns[s]\n",
    "        if len(rets) == 0:\n",
    "            continue\n",
    "        values[s] = sum(rets) / len(rets)\n",
    "                \n",
    "    return values\n",
    "\n",
    "\n",
    "def first_visits(trace: TraceT) -> Sequence[bool]:\n",
    "    \"\"\"\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "        >>> first_visits([(0, 0, 0.5)])\n",
    "        [True]\n",
    "        >>> first_visits([(0, 0, 0.5), (1, 0, 0.1), (0, 0, 0.5)])\n",
    "        [True, True, False]\n",
    "    \"\"\"\n",
    "    visited = set()\n",
    "    fv = []\n",
    "    for s, _, _ in trace:\n",
    "        fv.append(s not in visited)\n",
    "        visited.add(s)\n",
    "    return fv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both first-visit MC and every-visit MC converge to $v_\\pi(s)$ as the number of visits (or first visits) to $s$ goes to infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5.1: Blackjack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BJAction(enum.Enum):\n",
    "    STICK = 0\n",
    "    HIT = 1\n",
    "    \n",
    "    \n",
    "class BJCard(enum.IntEnum):\n",
    "    ACE   = 11\n",
    "    TWO   = 2\n",
    "    THREE = 3\n",
    "    FOUR  = 4\n",
    "    FIVE  = 5\n",
    "    SIX   = 6\n",
    "    SEVEN = 7\n",
    "    EIGHT = 8\n",
    "    NINE  = 9\n",
    "    TEN   = 10\n",
    "    \n",
    "    @staticmethod\n",
    "    def _prob(card) -> float:\n",
    "        if card == BJCard.TEN:\n",
    "            return 4.0 / 13\n",
    "        return 1.0 / 13\n",
    "\n",
    "    @staticmethod\n",
    "    def hit(n_cards=None):\n",
    "        return np.random.choice(BJCard, size=n_cards, p=[BJCard._prob(c) for c in BJCard]).tolist()\n",
    "\n",
    "    \n",
    "@dataclass\n",
    "class Hand:\n",
    "    value: int\n",
    "    usable_ace: bool\n",
    "        \n",
    "    def __init__(self, cards):\n",
    "        self.value = 0\n",
    "        self.usable_ace = False\n",
    "        for card in cards:\n",
    "            if card != BJCard.ACE:\n",
    "                self.value += card\n",
    "            else:\n",
    "                if self.usable_ace:\n",
    "                    # cannot have more than one usable ace (11 + 11 > 21)\n",
    "                    self.value += 1\n",
    "                else:\n",
    "                    # attempt to use max value, correct later if > 21\n",
    "                    self.value += 11\n",
    "                    self.usable_ace = True\n",
    "\n",
    "            if self.value > 21 and self.usable_ace:\n",
    "                self.value -= 10\n",
    "                self.usable_ace = False\n",
    "        \n",
    "    def add(self, card):\n",
    "        self.value += card\n",
    "        if self.usable_ace and self.value > 21:\n",
    "            self.value -= 10\n",
    "            self.usable_ace = False        \n",
    "        return self\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.value) + hash(self.usable_ace)\n",
    "\n",
    "    \n",
    "@dataclass\n",
    "class BJState:\n",
    "    player: Hand\n",
    "    dealer: Hand\n",
    "        \n",
    "    @staticmethod\n",
    "    def new():\n",
    "        cards = BJCard.hit(3)\n",
    "        player = Hand(cards[:2])\n",
    "        dealer = Hand(cards[2:])\n",
    "        return BJState(player, dealer)\n",
    "    \n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.player) + hash(self.dealer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bj_states() -> Sequence[BJState]:\n",
    "    states = []\n",
    "    for player_value in range(12, 21 + 1):\n",
    "        for usable_ace in [False, True]:\n",
    "            for dealer_value in range(2, 11 + 1):\n",
    "                player = Hand([])\n",
    "                player.value = player_value\n",
    "                player.usable_ace = usable_ace\n",
    "                \n",
    "                dealer = Hand([])\n",
    "                dealer.value = dealer_value\n",
    "                dealer.usable_ace = dealer_value == 11\n",
    "                \n",
    "                states.append(BJState(player, dealer))\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stick_20_21(state: BJState, action: BJAction) -> float:\n",
    "    if state.player.value in {20, 21}:\n",
    "        if action == BJAction.STICK:\n",
    "            return 1.0\n",
    "        return 0.0\n",
    "    if action == BJAction.HIT:\n",
    "        return 1.0\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bj_simulate(\n",
    "    policy: Callable[[BJState, BJAction], float]\n",
    ") -> Sequence[Tuple[BJState, BJAction, int]]:\n",
    "    \"\"\"...\"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    state = BJState.new()\n",
    "    \n",
    "    # simulate player until sticks or goes bust\n",
    "    while True:\n",
    "        states.append(state.copy())\n",
    "        \n",
    "        action = np.random.choice(BJAction, p=[policy(state, a) for a in BJAction])\n",
    "        actions.append(action)\n",
    "        \n",
    "        if action == BJAction.HIT:\n",
    "            state.player.add(BJCard.hit())\n",
    "        \n",
    "        if action == BJAction.STICK or state.player.value > 21:\n",
    "            break\n",
    "            \n",
    "        rewards.append(0)   # 0 whilst player still playing\n",
    "    \n",
    "    # compute final reward\n",
    "    if state.player.value > 21:\n",
    "        # player bust\n",
    "        reward = -1\n",
    "    else:\n",
    "        # player not bust, compute dealer value using fixed policy\n",
    "        while state.dealer.value < 17:\n",
    "            state.dealer.add(BJCard.hit())\n",
    "        if state.dealer.value > 21:\n",
    "            # dealer bust\n",
    "            reward = 1\n",
    "        else:\n",
    "            if state.player.value == state.dealer.value:\n",
    "                reward = 0\n",
    "            elif state.player.value > state.dealer.value:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "           \n",
    "    states.append(state.copy())\n",
    "    actions.append(None)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    return list(zip(states, actions, rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(f, q_in, q_out, seed):\n",
    "    # force different seed per process to avoid \n",
    "    # processes simulating the same episodes\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    to_gen = q_in.get()\n",
    "    for _ in range(to_gen):\n",
    "        q_out.put(f())\n",
    "        \n",
    "\n",
    "def bj_simulate_n(policy, n_episodes, n_procs=10):\n",
    "    assert n_procs > 0\n",
    "    if n_procs == 1:\n",
    "        for _ in range(n_episodes):\n",
    "            yield bj_simulate(policy)\n",
    "        return\n",
    "        \n",
    "    q_in = multiprocessing.Queue()\n",
    "    q_out = multiprocessing.Queue()\n",
    "\n",
    "    proc = [\n",
    "        multiprocessing.Process(\n",
    "            target=generate, \n",
    "            args=(lambda: bj_simulate(policy), q_in, q_out, seed)\n",
    "        )\n",
    "        for seed in range(n_procs)\n",
    "    ]\n",
    "    \n",
    "    for p in proc:\n",
    "        p.daemon = True\n",
    "        p.start()\n",
    "        \n",
    "    for pid in range(n_procs):\n",
    "        to_proc = n_episodes // n_procs\n",
    "        if pid == n_procs - 1:\n",
    "            to_proc += n_episodes % n_procs\n",
    "        q_in.put(to_proc)\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        yield q_out.get()\n",
    "\n",
    "    [p.join() for p in proc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(n_procs, n_episodes):\n",
    "    state_values = first_visit_mc(\n",
    "        states=bj_states(),\n",
    "        episodes=bj_simulate_n(stick_20_21, n_episodes, n_procs=n_procs),\n",
    "        gamma=1.0\n",
    "    )\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:01, 7550.00it/s]\n",
      "500000it [00:58, 8574.73it/s] \n"
     ]
    }
   ],
   "source": [
    "state_values = {}\n",
    "for n_episodes in [10000, 500000]:\n",
    "    state_values[n_episodes] = run(n_procs=6, n_episodes=n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65a5a5b7066489ea8b72399d731065e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close(\"all\")\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i, n_episodes in enumerate(sorted(state_values.keys())):\n",
    "    for j, usable_ace in enumerate([True, False]):\n",
    "        values = np.empty((10, 10))\n",
    "\n",
    "        for state, value in state_values[n_episodes].items():\n",
    "            if state.player.usable_ace != usable_ace:\n",
    "                continue\n",
    "            values[ state.player.value - 12, state.dealer.value - 2] = value\n",
    "\n",
    "        X, Y = np.meshgrid(range(2, 11 + 1), range(12, 21 + 1))\n",
    "        xs = X.flatten()\n",
    "        ys = Y.flatten()\n",
    "        zs = values.flatten()\n",
    "\n",
    "        ax = fig.add_subplot(2, 2, i + j*2 + 1, projection=\"3d\")\n",
    "        surf = ax.plot_trisurf(xs, ys, zs)\n",
    "        ax.set_zlim3d(-1, 1)\n",
    "        \n",
    "        ax.set_xlabel(\"Dealer showing\")\n",
    "        ax.set_ylabel(\"Player sum\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Exercise 5.1: The value function jumps up for the last two rows as the policy is to stick at a value of 20 or 21 and the probability of the player winning when in either of these states is high. It drops off for the far right row (left in the book's figures) as the dealer has an ace (valued 11). An ace combined with a 10 means the player cannot and the probability of drawing a card with a value of 10 is relatively high (16/52). The frontmost values are higher when there is a usable ace as the player has less of a chance of going bust - if the player draws a high card to go bust in the lower diagrams they lose but in the upper diagrams the ace takes the value of 1 rather than 10 allowing the player to continue.\n",
    "\n",
    "Exercise 5.2: The results will be identical as no state is visited twice in an episode of black jack (assuming \"usable ace\" is part of the state).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backup Diagrams for MC Methods\n",
    "\n",
    "The general idea of backup diagrams is to show at the top the root node to be updated and to show below all the transitions and leaf nodes whose rewards and estimated values contribute to the update. \n",
    "\n",
    "For MC estimation of $v_\\pi$ the root node is a state node and below is the trace for a single episode. \n",
    "\n",
    "This is in contrast to DP algorithms that include only one-step transitions.\n",
    "\n",
    "Another difference is that for MC methods the estimate of each state is independent - one estimate does not build upon another estimate. MC methods do not bootstrap.\n",
    "\n",
    "Estimation of a state's value using MC methods is therefore independent of all other states. This allows only the state space of interest to be explored (which may be significantly smaller than the entire space). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Estimation of Action Values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import enum\n",
    "from typing import Callable\n",
    "from typing import Mapping\n",
    "from typing import Optional\n",
    "from typing import Sequence\n",
    "from typing import Tuple\n",
    "from typing import TypeVar\n",
    "\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Difference Learning\n",
    "\n",
    "## <p style=\"color:red\">Text content copied near verbatim from: <a href=\"http://incompleteideas.net/book/the-book-2nd.html\">Sutton and Barto</a>. Code is my own unless otherwise stated.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "- TD relation to Monte Carlo and DP methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "**TD(0) / _n-step_ TD**: \n",
    "\n",
    "**TD error**:\n",
    "\n",
    "**Markov Reward Process**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Temporal-difference (TD) learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas.\n",
    "\n",
    "Like Monte Carlo methods, TD methods can directly learn from raw experience without a model of the environment's dynamics.\n",
    "\n",
    "Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they _bootstrap_).\n",
    "\n",
    "DP, TD, and Monte Carlo methods all use some form of generalized policy iteration (GPI) to solve the control problem. The differences in the methods are primarily due to differences in their approaches to the prediction problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Prediction\n",
    "\n",
    "Both TD and Monte Carlo methods use experience to solve the prediction problem. Given some experience following a policy $\\pi$, both methods update their estimate $V$ of $v_\\pi$ for the nonterminal states $S_t$ occurring in that experience. \n",
    "\n",
    "---\n",
    "\n",
    "Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V(S_t)$. A simple every-visit Monte Carlo method suitable for nonstationary environments is the _constant-$\\alpha$ MC_:\n",
    "\n",
    "\\begin{equation}\n",
    "    V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ G_t - V(S_t) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "where $G_t$ is the actual return following time $t$ and $\\alpha$ is a constant step-size parameter.\n",
    "\n",
    "Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_t)$ (only then is $G_t$ known).\n",
    "\n",
    "---\n",
    "\n",
    "TD methods only need to wait until the next time step to determine the increment. At time $t + 1$ they immediately form a target and make a useful update using the observed reward $R_{t + 1}$ and the estimate $V(S_{t + 1})$. The simplest TD method makes the update\n",
    "\n",
    "\\begin{equation}\n",
    "    V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "immediately on transition to $S_{t + 1}$ and receiving $R_{t + 1}$. \n",
    "\n",
    "---\n",
    "\n",
    "In effect, the target for the Monte Carlo update is $G_t$ whereas the target for the TD update is $R_{t + 1} + \\gamma V(S_{t + 1})$. \n",
    "\n",
    "This TD method is called $TD(0)$ or _one-step_ TD because it is a special case of the TD($\\lambda$) and _n-step_ TD methods developed in Chapters 12 and 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "StateT = TypeVar(\"StateT\")\n",
    "ActionT = TypeVar(\"ActionT\")\n",
    "RewardT = TypeVar(\"RewardT\", int, float)\n",
    "\n",
    "TraceT = Sequence[Tuple[StateT, ActionT, RewardT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_td0(\n",
    "    policy: Mapping[StateT, Mapping[ActionT, float]],\n",
    "    init_state: Callable[[], StateT],\n",
    "    is_terminal: Mapping[StateT, bool],\n",
    "    simulate: Callable[[StateT, ActionT], Tuple[StateT, RewardT]],\n",
    "    n_episode: int,\n",
    "    alpha: float = 0.1,\n",
    "    gamma: float = 1.0,\n",
    "    init_fn: Optional[Callable[[StateT], float]] = None\n",
    ") -> None:\n",
    "    state_values = {}\n",
    "    \n",
    "    for state in policy.keys():\n",
    "        state_values[state] = 0 if init_fn is None else init_fn(state) \n",
    "        \n",
    "    for _ in tqdm.trange(n_episode):\n",
    "        state = init_state()\n",
    "        \n",
    "        while not is_terminal[state]:\n",
    "            action = random.choices(\n",
    "                population=list(policy[state].keys()), \n",
    "                weights=policy[state].values()\n",
    "            )[0]   # random.choices returns a 1-element list\n",
    "            next_state, reward = simulate(state, action)\n",
    "\n",
    "            state_values[state] += alpha*(reward + gamma*state_values[next_state] - state_values[state])\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Confirming this works on the simple grid-world environment from Example 4.1 in Chapter 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_states: Sequence[int] = list(range(4*4))   # 4x4 grid, states numbered left to right top to bottom \n",
    "\n",
    "    \n",
    "class GWAction(enum.Enum):\n",
    "    UP    = enum.auto()\n",
    "    DOWN  = enum.auto()\n",
    "    RIGHT = enum.auto()\n",
    "    LEFT  = enum.auto()\n",
    "\n",
    "    \n",
    "def gw_init_state() -> int:\n",
    "    return random.choice(gw_states)\n",
    "\n",
    "\n",
    "gw_is_terminal: Mapping[int, bool] = {\n",
    "    state: state == 0 or state == 15\n",
    "    for state in gw_states\n",
    "}\n",
    "\n",
    "\n",
    "def gw_simulate(state: int, action: GWAction) -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "        \n",
    "    \"\"\"\n",
    "    if action == GWAction.LEFT:\n",
    "        if state % 4 == 0:\n",
    "            #  unable to move further left\n",
    "            return state, -1\n",
    "        return state - 1, -1\n",
    "    \n",
    "    if action == GWAction.RIGHT:\n",
    "        if (state + 1) % 4 == 0:\n",
    "            # unable to move further right\n",
    "            return state, -1\n",
    "        return state + 1, -1\n",
    "    \n",
    "    if action == GWAction.UP:\n",
    "        if state < 4:\n",
    "            return state, -1\n",
    "        return state - 4, -1\n",
    "    \n",
    "    if action == GWAction.DOWN:\n",
    "        if state > 11:\n",
    "            return state, -1\n",
    "        return (state + 4) % 15, -1 \n",
    "\n",
    "    raise ValueError(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_equiprob_policy = {}\n",
    "for state in gw_states:\n",
    "    gw_equiprob_policy[state] = {}\n",
    "    for action in GWAction:\n",
    "        gw_equiprob_policy[state][action] = 1.0 / len(GWAction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:45<00:00, 22136.10it/s]\n"
     ]
    }
   ],
   "source": [
    "state_values = tabular_td0(\n",
    "    policy=gw_equiprob_policy,\n",
    "    init_state=gw_init_state,\n",
    "    is_terminal=gw_is_terminal,\n",
    "    simulate=gw_simulate,\n",
    "    n_episode=int(1e6),\n",
    "    alpha=0.0001,\n",
    "    gamma=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000 -14.090 -20.001 -21.981 \n",
      "-13.972 -18.012 -19.994 -19.978 \n",
      "-19.946 -19.995 -17.980 -14.001 \n",
      "-21.984 -20.044 -14.042   0.000 \n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        print(f\"{state_values[i*4 + j]: 7.3f}\", end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the actual state-value function for this problem is:\n",
    "\n",
    "```\n",
    "| 0.0 | -14 | -20 | -22 |\n",
    "| -14 | -18 | -20 | -20 |\n",
    "| -20 | -20 | -18 | -14 |\n",
    "| -22 | -20 | -14 | 0.0 |\n",
    "```\n",
    "\n",
    "so we see that the method works!\n",
    "\n",
    "Notice that $\\alpha$ must be small for the computed values to get close to their true value.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD(0) bases its update in part on an existing estimate so it is a _bootstrapping_ method like DP. We know from Chapter 3 that:\n",
    "\n",
    "\\begin{align}\n",
    "    v_\\pi(s) &\\doteq \\mathbb{E}_\\pi \\left[ G_t | S_t = s \\right] \\\\\n",
    "             &= \\mathbb{E}_\\pi \\left[ R_{t + 1} + \\gamma G_{t + 1} | S_t = s \\right] \\\\\n",
    "             &= \\mathbb{E}_\\pi \\left[ R_{t + 1} + \\gamma v_\\pi(S_{t + 1}) | S_t = s \\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo methods use an estimate of $\\mathbb{E}_\\pi[G_t | S_t = s]$ as the target. It is an estimate as the expected value is not known; a sample return is used in place of the real expected return.\n",
    "\n",
    "DP methods use an estimate of $\\mathbb{E}_\\pi[R_{t + 1} + \\gamma v_\\pi(S_{t + 1}) | S_t = s]$ as the target. It is an estimate not because of the expected values, which are assumed to be completely provided by a model of the environment, but because $v_\\pi(S_{t + 1})$ is not known and the current estimate $V(S_{t + 1})$ is used instead.\n",
    "\n",
    "The TD target is an estimate for both reasons:\n",
    "\n",
    "1. It samples the expected values.\n",
    "2. It uses the current estimate $V$ instead of the true $v_\\pi$.\n",
    "\n",
    "**TD methods combine the sampling of Monte Carlo with the bootstrapping of DP.**\n",
    "\n",
    "---\n",
    "\n",
    "Note that in the update rule for TD the quantity in brackets, $\\left[ R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t) \\right]$, is a sort of error measuring the difference between the estimated value of $S_t$ and the better estimate $R_{t + 1} + \\gamma V(S_{t + 1})$. This is called the **TD error** and arises in various forms throughout reinforcement learning:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_t \\doteq R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t)\n",
    "\\end{equation}\n",
    "\n",
    "This quantity depends on the next state and reward so is not available until one time step later. If the array V does not change during the episode (e.g. Monte Carlo methods) then the Monte Carlo error can be written as a sum of TD errors:\n",
    "\n",
    "\\begin{align}\n",
    "    G_t - V(S_t) &= R_{t + 1} + \\gamma G_{t + 1} - V(S_t) \\\\\n",
    "                 &= R_{t + 1} + \\gamma G_{t + 1} - V(S_t) + \\gamma V(S_{t + 1}) - \\gamma V(S_{t + 1}) \\\\\n",
    "                 &= \\delta_t + \\gamma \\delta_{t + 1} + \\gamma^2(G_{t + 2} - V(S_{t + 2})) \\\\\n",
    "                 &= \\delta_t + \\gamma \\delta_{t + 1} + \\gamma^2\\delta_{t + 2} + \\dots + \\gamma^{T - t - 1} \\delta_{T - 1} + \\gamma^{T - t}(0 - 0) \\\\\n",
    "                 &= \\sum_{k = t}^{T - 1} \\gamma^{k - t} \\delta_k\n",
    "\\end{align}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 6.1: If V changes during the episode then the above only holds approximately; what would the difference be between the two sides? Let $V_t$ denote the array of state values used at time $t$ in the TD error and in the TD update. Redo the derivation above to determine the additional amount that must be added to the sum of TD errors in order to equal the Monte Carlo error.\n",
    "\n",
    "\\begin{align}\n",
    "    V_{t + 1}(S_t) &= V_t(S_t) + \\alpha [R_{t + 1} + \\gamma V_t(S_{t + 1}) - V_t(S_t)] \\\\\n",
    "                   &= V_t(S_t) + \\alpha [R_{t + 1} + \\gamma V_t(S_{t + 1}) - V_t(S_t)] \\\\\n",
    "                   &= V_t(S_t) + \\alpha \\delta_t \\\\\n",
    "\\end{align}\n",
    "\n",
    "Rearranging for $V_t(S_t)$\n",
    "\n",
    "\\begin{align}\n",
    "    V_t(S_t) &= V_{t + 1}(S_t) - \\alpha \\delta_t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following through with the proof similar to the steps in (6.6):\n",
    "\n",
    "\\begin{align}\n",
    "    G_t - V_t(S_t) &\\doteq R_{t + 1} + \\gamma G_{t + 1} - V_t(S_t) \\\\\n",
    "                   &=      R_{t + 1} + \\gamma V_t(S_{t + 1}) - V_t(S_t) + \\gamma G_{t + 1} - \\gamma V_t(S_{t + 1}) \\\\\n",
    "                   &= \\delta_t + \\gamma(G_{t + 1} - V_t(S_{t + 1})) \\\\\n",
    "                   &= \\delta_t + \\gamma(G_{t + 1} - V_{t + 1}(S_{t + 1}) + \\alpha \\delta_t) \\\\\n",
    "                   &= \\delta_t + \\gamma \\alpha \\delta_t + \\gamma(G_{t + 1} - V_{t + 1}(S_{t + 1})) \\\\\n",
    "                   &= \\delta_t(1 + \\gamma \\alpha) + \\gamma(G_{t + 1} - V_{t + 1}(S_{t + 1})) \\\\\n",
    "                   &= \\delta_t(1 + \\gamma \\alpha) + \\gamma(\\delta_{t + 1}(1 + \\gamma \\alpha) + \\gamma(G_{t + 2} - V_{t + 2}(S_{t + 2}))) \\\\\n",
    "                   &= \\delta_t(1 + \\gamma \\alpha) + \\gamma(\\delta_{t + 1}(1 + \\gamma \\alpha)) + \\gamma(G_{t + 2} - V_{t + 2}(S_{t + 2})) \\\\\n",
    "                   &= (1 + \\gamma \\alpha)(\\delta_t + \\gamma\\delta_{t + 1}) + \\gamma^2(\\delta_{t + 2}(1 + \\gamma \\alpha) + \\gamma(G_{t + 3} - V_{t + 3}(S_{t + 3}))) \\\\\n",
    "                   &= (1 + \\gamma \\alpha)(\\delta_t + \\gamma\\delta_{t + 1}) + \\gamma^2\\delta_{t + 2}(1 + \\gamma \\alpha) + \\gamma^3(G_{t + 3} - V_{t + 3}(S_{t + 3})) \\\\\n",
    "                   &= (1 + \\gamma \\alpha)(\\delta_t + \\gamma\\delta_{t + 1} + \\gamma^2\\delta_{t + 2}) + \\gamma^3(G_{t + 3} - V_{t + 3}(S_{t + 3})) \\\\\n",
    "                   &= (1 + \\gamma \\alpha)\\sum_{k=t}^{T - 1}\\gamma^{k - t} \\delta_t \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively this makes sense.\n",
    "\n",
    "To see why, first note that the original equation _without_ the time subscript telescopes to produce the sum of discounted rewards equation. This is a convoluted way of getting from the RHS back to the LHS :-)\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_t \\doteq R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    G_t - V(S_t) &= \\sum_{k = t}^{T - 1} \\gamma^{k - t} \\delta_k \\\\\n",
    "                 &= R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t) + \\gamma ( R_{t + 2} + \\gamma V(S_{t + 2}) - V(S_{t + 1})) + \\sum_{k = t + 2}^{T - 1} \\gamma^{k - t} \\delta_k \\\\\n",
    "                 &= R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t) + \\gamma R_{t + 2} + \\gamma^2 V(S_{t + 2}) - \\gamma  V(S_{t + 1}) + \\sum_{k = t + 2}^{T - 1} \\gamma^{k - t} \\delta_k \\\\\n",
    "                 &= R_{t + 1} + \\gamma R_{t + 2} - V(S_t) + \\gamma^2 V(S_{t + 2})+ \\sum_{k = t + 2}^{T - 1} \\gamma^{k - t} \\delta_k \\\\\n",
    "                 &= \\left[\\sum_{k = t + 1}^{T} \\gamma^{k - t - 1}R_{k}\\right] - V(S_t)\n",
    "\\end{align}\n",
    "\n",
    "Now, from the above we also know:\n",
    "\n",
    "\\begin{equation}\n",
    "    V_{t + 1}(S_t) - V_t(S_t) = \\alpha \\delta_t\n",
    "\\end{equation}\n",
    "\n",
    "Distributing the summation gives:\n",
    "\n",
    "\\begin{align}\n",
    "    G_t - V_t(S_t) &= (1 + \\gamma \\alpha) \\sum_{k = t}^{T - 1} \\gamma^{k - t}\\delta_t \\\\\n",
    "                   &= \\sum_{k = t}^{T - 1} \\gamma^{k - t}\\delta_t + \\gamma \\alpha \\sum_{k = t}^{T - 1} \\gamma^{k - t}\\delta_t \\\\\n",
    "                   &= \\sum_{k = t}^{T - 1} \\gamma^{k - t}\\delta_t + \\sum_{k = t}^{T - 1} \\gamma^{k - t + 1} \\alpha \\delta_t \\\\\n",
    "                   &= \\sum_{k = t}^{T - 1} \\gamma^{k - t} \\delta_t + \\gamma^{k - t + 1} \\alpha \\delta_t\n",
    "\\end{align}\n",
    "\n",
    "It is the original sum of discounted TD errors plus another term. This other term accounts for the sum of discounted differences/deltas between the state-value estimate at step $t + 1$ and at step $t$. i.e. it is the difference, $\\alpha \\delta_k$, weighted by the discounting factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Example 6.1: Driving Home (/Exercise 6.2)\n",
    "\n",
    "Consider driving home and at various points estimating the time remaining. This can be considered an RL problem where the state is the weather, your location, current time etc. The rewards are the negated time taken to get to the point on your journey home. The value function is the negated estimate of how long you think it should take to get home from the current state.\n",
    "\n",
    "Let $\\gamma = 1$ and $\\alpha = 1$. \n",
    "\n",
    "For Monte Carlo methods the error, $G_t - V(S_t)$ is the difference between the actual time taken and the estimated time remaining. This requires knowing how long the journey took in total so updates can only be done offline. It also means that changes that occur in just one state can have big impacts as to the predicted times of all other states, even if in practice they are unchanged. \n",
    "\n",
    "For methods that use temporal difference learning the error is $R_t + \\gamma V(S_{t + 1}) - V(S_t)$. The estimated return, in this case time remaining, can be updated as soon as the next state and reward are known. In the case of driving home, if you predict it will take 30 minutes to get home and it takes 5 minutes to reach the next state where you now think it will take 20 minutes, the prior estimate can be updated by $5 + 20 - 30 = -5$ minutes _without_ needing to actually reach your house.\n",
    "\n",
    "Note that this means if your next state has an accurate estimate of the time taken to reach home then you are able to quickly converge the current state's estimate to the true expected value. Consider changing jobs to one that is in a different location but after a slightly different, longer start uses the same route home. For MC Methods the error for all states will initially be increased despite it only being the few starting states that have changed and they will need to be decreased again as the starting states converge.For TD methods only these initial states will be updated. \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of TD Prediction Methods\n",
    "\n",
    "**Bootstrap**: Learning to guess from a guess. TD methods update their estimates based in part on other estimates.\n",
    "\n",
    "TD Method Advantages:\n",
    "\n",
    "1. Compared to DP methods, they do not require a model of the environment (DP methods use to compute expectation over all possible next states and rewards). \n",
    "2. Compared to Monte Carlo methods, they can be implemented in an online, fully incremental fashion (Monte Carlo methods must wait until the end of the episode to get the final reward). Some applications have very long episodes where delaying all learning until the end of the episode is too slow. Other applications have no episodes at all! \n",
    "3. Compared to Monte Carlo methods, TD methods can learn from each transition regardless of what subsequent actions are taken. Some Monte Carlo methods must ignore or discount episodes on which experimental actions are taken, which can greatly slow learning (if action is non-greedy towards end of episode, importance sampling ratio is zero and therefore all updates at time before the step become zero).\n",
    "\n",
    "\n",
    "Convergence can be guaranteed for TD methods: for any fixed policy $\\pi$, $TD(0)$ has been proved to converged to $v_\\pi$. In the mean for a constant step size parameter if it is sufficiently small or with probability 1 if it decreases. \n",
    "\n",
    "\n",
    "It is an open question as to which of Monte Carlo or TD methods are more data efficient/learns faster. In practice TD methods have usually been found to converge faster than constant-$\\alpha$ MC methods on stochastic tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"662pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 662.00 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-40 658,-40 658,4 -4,4\"/>\n",
       "<!-- endL -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>endL</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"54,-36 0,-36 0,0 54,0 54,-36\"/>\n",
       "</g>\n",
       "<!-- A -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>A</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"127\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"127\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">A</text>\n",
       "</g>\n",
       "<!-- endL&#45;&gt;A -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>endL&#45;&gt;A</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M64.1466,-18C72.4881,-18 81.3415,-18 89.6896,-18\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"64.0615,-14.5001 54.0614,-18 64.0614,-21.5001 64.0615,-14.5001\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"89.917,-21.5001 99.9169,-18 89.9169,-14.5001 89.917,-21.5001\"/>\n",
       "<text text-anchor=\"middle\" x=\"77\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0</text>\n",
       "</g>\n",
       "<!-- B -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>B</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"227\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"227\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">B</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;B -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>A&#45;&gt;B</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M164.1466,-18C172.4881,-18 181.3415,-18 189.6896,-18\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"164.0615,-14.5001 154.0614,-18 164.0614,-21.5001 164.0615,-14.5001\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"189.917,-21.5001 199.9169,-18 189.9169,-14.5001 189.917,-21.5001\"/>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0</text>\n",
       "</g>\n",
       "<!-- C -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>C</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"327\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"327\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">C</text>\n",
       "</g>\n",
       "<!-- B&#45;&gt;C -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>B&#45;&gt;C</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M264.1466,-18C272.4881,-18 281.3415,-18 289.6896,-18\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"264.0615,-14.5001 254.0614,-18 264.0614,-21.5001 264.0615,-14.5001\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"289.917,-21.5001 299.9169,-18 289.9169,-14.5001 289.917,-21.5001\"/>\n",
       "<text text-anchor=\"middle\" x=\"277\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0</text>\n",
       "</g>\n",
       "<!-- D -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>D</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"427\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"427\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">D</text>\n",
       "</g>\n",
       "<!-- C&#45;&gt;D -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>C&#45;&gt;D</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M364.1466,-18C372.4881,-18 381.3415,-18 389.6896,-18\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"364.0615,-14.5001 354.0614,-18 364.0614,-21.5001 364.0615,-14.5001\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"389.917,-21.5001 399.9169,-18 389.9169,-14.5001 389.917,-21.5001\"/>\n",
       "<text text-anchor=\"middle\" x=\"377\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0</text>\n",
       "</g>\n",
       "<!-- E -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>E</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"527\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"527\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">E</text>\n",
       "</g>\n",
       "<!-- D&#45;&gt;E -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>D&#45;&gt;E</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M464.1466,-18C472.4881,-18 481.3415,-18 489.6896,-18\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"464.0615,-14.5001 454.0614,-18 464.0614,-21.5001 464.0615,-14.5001\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"489.917,-21.5001 499.9169,-18 489.9169,-14.5001 489.917,-21.5001\"/>\n",
       "<text text-anchor=\"middle\" x=\"477\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0</text>\n",
       "</g>\n",
       "<!-- endR -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>endR</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"654,-36 600,-36 600,0 654,0 654,-36\"/>\n",
       "</g>\n",
       "<!-- E&#45;&gt;endR -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>E&#45;&gt;endR</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M564.1466,-18C572.4881,-18 581.3415,-18 589.6896,-18\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"564.0615,-14.5001 554.0614,-18 564.0614,-21.5001 564.0615,-14.5001\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"589.917,-21.5001 599.9169,-18 589.9169,-14.5001 589.917,-21.5001\"/>\n",
       "<text text-anchor=\"middle\" x=\"577\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f13859a5c10>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 6.2: Random Walk\n",
    "\n",
    "# A Markov reward process (MRP) is a MDP without actions.\n",
    "#\n",
    "# MRPs are often useful when focusing on the prediction problem is which there is no need\n",
    "# to distinguish between the dynamics due to the environment and those due to the agent.\n",
    "# (i.e. the state->action, action->next state probabilities collapse to state->next state)\n",
    "#\n",
    "# Consider the MRP:\n",
    "\n",
    "g = graphviz.Digraph(graph_attr={\"rankdir\": \"LR\"})\n",
    "\n",
    "nodes = [\n",
    "    \"endL\", \"A\", \"B\", \"C\", \"D\", \"E\", \"endR\"\n",
    "]\n",
    "for n in nodes:\n",
    "    end = \"end\" in n\n",
    "    g.node(\n",
    "        n,\n",
    "        label=None if not end else \"\",\n",
    "        shape=\"box\" if end else None,\n",
    "        style=\"filled\" if end else None,\n",
    "    )\n",
    "    \n",
    "for l, r in zip(nodes, nodes[1:]):\n",
    "    g.edge(\n",
    "        l, r,\n",
    "        dir=\"both\", \n",
    "        label=\"0\" if r != \"endR\" else \"1\"\n",
    "    )\n",
    "    \n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The starting state is always C and it proceeds left or right both with probability 0.5 until\n",
    "# either end state is reaching. The reward is always 0 except when the episode terminates on\n",
    "# the right where it is +1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandWalkAction(enum.Enum):\n",
    "    LEFT = enum.auto()\n",
    "    RIGHT = enum.auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "TERMINAL = \".\"\n",
    "rnd_walk_state_idxs = {\n",
    "    s: i for i, s in enumerate([TERMINAL] + list(\"ABCDE\"))\n",
    "}\n",
    "rnd_walk_states = list(rnd_walk_state_idxs.values())\n",
    "\n",
    "rnd_walk_policy = {\n",
    "    k: {a: 0.5 for a in RandWalkAction}\n",
    "    for k in rnd_walk_states\n",
    "}\n",
    "\n",
    "rnd_init_state = lambda: rnd_walk_state_idxs[\"C\"]\n",
    "\n",
    "rnd_is_terminal = {\n",
    "    s: s == rnd_walk_state_idxs[TERMINAL] \n",
    "    for s in rnd_walk_states\n",
    "}\n",
    "\n",
    "def rnd_simulate(state, action):\n",
    "    if state == rnd_walk_state_idxs[TERMINAL]:\n",
    "        return rnd_walk_state_idxs[TERMINAL], 0\n",
    "    \n",
    "    if action == RandWalkAction.LEFT:\n",
    "        return state - 1, 0\n",
    "    \n",
    "    if state == rnd_walk_state_idxs[\"E\"] and action == RandWalkAction.RIGHT:\n",
    "        return rnd_walk_state_idxs[TERMINAL], 1\n",
    "    \n",
    "    return state + 1, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rnd_td0(n_episode):\n",
    "    return tabular_td0(\n",
    "        policy=rnd_walk_policy,\n",
    "        init_state=rnd_init_state,\n",
    "        is_terminal=rnd_is_terminal,\n",
    "        simulate=rnd_simulate,\n",
    "        n_episode=n_episode,\n",
    "        alpha=0.1,\n",
    "        gamma=1.0,\n",
    "        init_fn=lambda s: 0.0 if s == rnd_walk_state_idxs[TERMINAL] else 0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3603.35it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18363.85it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 45353.63it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXDU530/8Pfeq9WtXXRfSEhgGQw6QNwyRsgUzDF14rZJ82vHk59DIaWOG0+C7TZuE1JNbE9cp1CnNcXTiZu0Tqf2ZBJsfqqROIRAgMQhO6wOQALdWh0rrfb6fr+/PxbWrA5WAq2+q9X7NcPA6vus9qNn0FuPnv0+z6OQJEkCERHNeUq5CyAiopnBQCciChEMdCKiEMFAJyIKEQx0IqIQwUAnIgoRan8NDh8+jEuXLiE6OhpvvfXWuOuSJOHo0aOoq6uDTqfD3r17kZWVFZBiiYhocn5H6E8++SReeeWVSa/X1dWhs7MT77zzDl544QW89957M1ogERFNjd9Az8vLQ0RExKTXL1y4gI0bN0KhUCA3NxcjIyPo7++f0SKJiMg/v1Mu/lgsFphMJu9jo9EIi8WC2NjYcW0rKipQUVEBACgvL3/UlyYiovs8cqBPtHOAQqGYsG1paSlKS0u9j9vb2x/qNU0mE3p7ex/quYEUrHUBwVsb65oe1jU9oVhXcnLypNce+S4Xo9HoU1hfX9+Eo3MiIgqsRw70oqIinDx5EpIkwWw2w2AwMNCJiGTgd8rl7bffxueffw6r1Yo9e/bgueeeg9vtBgCUlZUhPz8fly5dwv79+6HVarF3796AF01EROP5DfQXX3zxgdcVCgW++c1vzkgxkiTBbrdDFMVJ5+EBoKurCw6HY0ZecyZNVJckSVAqldDr9Q/8moiIHtUjvyk6k+x2OzQaDdTqB5elVquhUqlmqaqpm6wut9sNu92OsLAwGaoiovkiqJb+i6LoN8znIrVaDVEU5S6DiEJcUAV6KE9JhPLXRkTBIagCnYiIHh4DfQInTpzAhg0bsG7dOvzTP/2T3OUQEU0JA30MQRDw6quv4he/+AVOnDiBjz76CGazWe6yiIj8YqCPUVdXh8zMTGRkZECr1WLXrl349NNP5S6LiMivoL2lRPzVv0JquzHxNYViwj1k/FGkLYTyj//vA9t0dnb67JWQlJSEurq6ab8WEdFs4wh9jOlsNkZEFEyCdoT+oJG0Wq32bj8w05KSknx2gezo6EBCQkJAXouIaCZxhD7GihUrcOPGDbS2tsLpdOLjjz9GWVmZ3GUREfkVtCN0uajVavzoRz/C1772NYiiiD/6oz/C4sWL5S6LiMgvBvoENm/ejM2bN8tdBhHRtHDKhYgoRDDQiYhCBAOdiChEMNCJiEIEA52IKEQw0ImIQgQDfYyXXnoJTzzxBJ566im5SyEimhYG+hjPPfccPvjgA7nLICKaNgb6GKtXr0ZMTIzcZRARTVvQrhR970IXbvTbJ7ymeMjtcxfG6vHNIm60RUShiSN0IqIQEbQj9AeNpAO5fS4R0VzFEToRUYhgoI+xd+9e7Ny5E83NzSgsLMQvf/lLuUsiIpqSoJ1ykcvhw4flLoGI6KFwhE5ENIskSYIoigH53Ax0IqJZ0tvbi//+7/9GXV1dQD4/p1yIiALM6XTi3LlzqK+vh06ng0ajCcjrMNCJiAJEkiQ0Njbi1KlTGBkZweOPP461a9ciLS0Nvb29M/56DHQiogCwWCyoqqpCW1sbFixYgO3btyMxMTGgr8lAJyKaQS6XC+fPn0ddXR3UajVKSkqwbNkyKJWBf8uSgT7GSy+9hIqKCphMJnz22WcAgP7+fvzFX/wF2trakJaWhnfffZcbeBGRD0mS0NLSgpMnT8JqtWLJkiVYv349DAbDrNUwpR8Z9fX1+Ku/+iv85V/+JT766KNx1202G8rLy/Hyyy/jpZdewokTJ2a80Nky0fa5hw4dwvr163HmzBmsX78ehw4dkqk6IgpGAwMD+M1vfoPf/va30Gq1ePbZZ1FWVjarYQ5MIdBFUcSRI0fwyiuv4Kc//SnOnDmD27dv+7T55JNPkJqaijfeeAOvv/46/v3f/33O7rUy0fa5n376Kb761a8CAL761a/ik08+kaM0Igoybrcb586dwwcffIA7d+5g/fr1+OM//mOkpKTIUo/fKZempiYkJiYiIcGzWdbatWtRW1uL1NRUbxuFQgG73Q5JkmC32xEREfHI80XXLtkwNCBMeO1ht8+NilFhacH0f2L29vZ6v/6EhAT09fVN+3MQUWi5efMmKisrMTQ0hJycHGzYsAERERGy1uQ30C0WC4xGo/ex0WhEY2OjT5utW7fiJz/5Cb71rW9hdHQU3/nOdyYM9IqKClRUVAAAysvLYTKZfK53dXVBrfaUpFQqoVBMvppKoVD4K30cpVLp/fwPolKpAMCn7djnTfZ5Jvu4Tqcb9/XOJrVaLevrT4Z1TQ/rmp5A1DUwMIBjx47hiy++gMlkwp/92Z8hOztb9rqAKQT6RCPhsWF6+fJlZGRk4G//9m/R1dWFH/7wh1iyZMm4+aPS0lKUlpZ6H4+9D9PhcHjDNG+FfvKiH2H73Kk8TxAEn7Ymkwl37txBQkICurq6YDQaJ/w8D6rL4XAE5L7TqTKZTLK+/mRY1/SwrumZyboEQcClS5dQW1sLwDNbkZ+fD5VKNe3XeJS6kpOTJ73md17EaDT6TDH09fUhNjbWp82JEydQXFwMhUKBxMRExMfHo729/aGKDUZlZWX48MMPAQAffvghnn76aZkrIqLZ1NbWhv/4j//A2bNnkZGRgW984xsoKiryDkCDhd9Az87ORkdHB7q7u+F2u1FdXY2ioiKfNiaTCVevXgXg+XWkvb0d8fHxgak4wCbaPnffvn04efIk1q1bh5MnT2Lfvn1yl0lEs2B4eBjHjh3D//zP/0AURezcuRPbt29HZGSk3KVNyO+Ui0qlwvPPP4+DBw9CFEVs2rQJaWlpOH78OADP6PXZZ5/F4cOH8dd//dcAgK9//euIiooKbOUBMtn2uf/1X/81y5UQkVwEQcDly5dx7tw5iKKI4uJiFBYWTuk9ODlNqbqCggIUFBT4fKysrMz777i4OLz22mszWxkRkQzu3LmDyspK9PX1ISMjAyUlJXNmIWFw/7ghIpolNpsNp0+fxu9//3tERkZi+/btyMrKeqg76uTCQCeieU0URVy9ehVnz56F2+1GUVERVq5cGbAtbgOJgU5E81ZnZydOnDiBnp4epKWloaSkBHFxcQF9TUmS4HYH5sQiBjoRzTujo6Oorq5GQ0MDwsPDsXXrVuTk5AR0emXUJqLthhOtN5x4/Ak1ktJn/jUY6EQ0b0iShIaGBlRXV8PhcCA/Px/FxcXQarUBeT1RlNDd4catZge6O92ABJgS1Ig1agGMzvjrMdDHmO72uT/72c/wq1/9CkqlEj/+8Y+xYcMGOcsnokl0d3ejsrISnZ2dSE5OxpNPPhmw7QpGrAJabzjRdsMJh12CTq/AoiU6pGdpER6hgskUjt7emQ90HhI9xnS2zzWbzfj444/x2Wef4YMPPsD3vvc977YBRBQcHA4HKisr8Z//+Z8YHBzEli1b8Oyzz854mAuChNu3nKg+MYzPfmdF0+8diIlTYeX6cJTuiMJjT4QhPCKwK0s5Qh9j9erVaGtr8/nYp59+il//+tcAPNvnfuUrX8Grr76KTz/9FLt27YJOp0N6ejoWLlyIurq6cStpiWj2SZKE+vp6HDt2DHa7HcuWLcOaNWug0+lm9HWGBgS0tjhw+5YLLqcEQ7gSi5fpkZapRZhhdsfMQRvoJ0+eRE9Pz4TXHnb73AULFmDjxo3Tft5k2+d2dnb6LLhKSkpCZ2fntD8/Ec2svr4+nDhxAu3t7UhISMCuXbtmdDsSt0vCnVYnWlucGLAIUCqBxBQN0rO0MCWoZbt3PWgDfS6Yyk6URDR7nE4nzp07h/r6euh0OuzatQvp6ekz8n0pSRIGLAJaW5y40+qE4AYiopTIW6FHaqYWOp38M9hBG+gPGkk/yva5D8NkMqGrq8tn+1zAMyK/f1fJjo4O70ieiGaPJElobGzEqVOnMDIygscffxxr165FWlraI2+f63SIuH3LhdYWB6yDIlQqIDldi/QsLWKNqqAaxAVtoAeTe9vnfvvb3/bZPresrAz79u3DCy+8gK6uLrS0tCA/P1/maonml/7+flRWVqKtrQ0LFizAtm3bkJSU9EifU5Ik9HW70driRMdtF0QRiI5V4YmiMCSna6HRBE+I34+BPsbevXtx9uxZWCwWFBYW4rvf/S727duHPXv24Je//CVSUlLw85//HACwePFi7NixA5s2bYJKpUJ5eXnQ7Y9MFKpcLhdqa2tx6dIlqNVqlJSUYNmyZY90/KV9VETbTSfaWpwYGRah0SiQnuUZjUfHBn9cKqSHeXdxhow9BMNms03plOzZnnKZqgfVNdWvLVDmw4kyM4l1Tc9s1iVJElpaWnDy5ElYrVYsWbIE69evn/D7ayp1SaKE7k7PaLyr3QVJAuIWqJCRpUNSqgYq9cyPxgN1YlHw/8ghIrprcHAQVVVVuHnzJoxGI5599lmkpKQ81OeyjXje4Gy74YR9VIJWp0DWYs/in4jIufmbNgOdiIKe2+3GxYsXceHCBSiVSqxfvx7Lly+f9hSnKEjobHehtcWJnk7Pb9MLEtVYWqBFQpIGSlVwzo1PVVAFuoyzPwEXyl8bUSDdvHkTVVVVGBwcRE5ODjZs2ICIiIhpfQ7rkGc0fvumE06HBL1BgdzH9UhbqIUhXP7bDWdKUAW6UqmE2+0O+mOepsvtdj/SGzVE85HVasXJkyfR3NyM2NhY7N69G+npU9+i0O2W0Pj7ITRctqK/V4BCASSkaJCRpcWCBDUUyrk9Gp9IUCWnXq+H3W6Hw+F44L2dOp0ODodjFiubmonqkiQJSqUSer1epqqI5hZBEFBXV4fz588DANasWYP8/PwpD/QGLG7v4h+3CwiPVOKx5Z6l+Dp9aA+sgirQFQoFwsLC/LbjO/1EoamtrQ2VlZXo7+9HVlYWNm7cOKUD511Oz1L8W81ODA0IUKqA5FQNlhXEQ6UZDqrFP4EUVIFORPPT8PAwTp8+DbPZjKioKOzcuROZmZkPfI4kSbD0ejbGam9zQRSAqBgllhaEISVDA61WCZMpDL29I7PzRQQBBjoRyUYURVy+fBk1NTUQRRGrVq1CUVHRA6dXHHYRt296NsYatopQq4G0zHuLf4JrKf5sY6ATkSza29tx4sQJ9PX1ISMjAyUlJd6DY8aSJAk9XZ658c47LkgiEGtUYfnKMCSnaaEO0qX4s42BTkSzymaz4cyZM/jiiy8QERGB7du3Iysra8KRtfcczhYHRm0SNFoFFi7yLP6JjJ6bi38CiYFORLNCFEVcu3YNZ8+ehcvlQmFhIVatWgWNRjOmnYSuu4t/7j+H87HlWiSmaKCa44t/AomBTkQB19nZicrKSnR3dyM1NRVPPvkk4uLifNqMWO8uxb/pOYdTH6ZAzmM6pC3UBvzotlDBQCeigBkdHcXZs2dx7do1hIeHY+vWrcjJyfFOrwiChI7bntF4X7cbCgUQn6RGepYO8UlqKENw8U8gMdCJaMZJkoTPP/8cZ86cgcPhwIoVK1BcXOw9z3OicziXLPMsxdeHhfbin0BioBPRjOrp6cGJEyfQ2dmJ5ORkPPnkkzCZTHC7JNxqdview5nqWYpvjJfvHM5QwkAnohnhcDhQU1ODK1euQK/XY8uWLVi8eDEGLSIun7fhTpvnHM7IKCUezw9DaoYG2iA4hzOUMNCJ6JFIkoTLly/j2LFjGB0dxbJly1BYUIyeTiVOHh/2nMOpBlLSPIt/YoLsHM5QwkAnooficDjQ3NyMhoYG7wHpG9dtw/BANE5+6oAoAjFxwX8OZyhhoBPRlLlcLty4cQNmsxk3b96EKIqIjIxCwfLNsFtTcf2KBI3GjYxsLdIW6hAdy9sNZxMDnYgeyO12o7W1FWazGS0tLXC73dBpDYg3Pga9OgMQjOjvVMAYr8KSpdqAncNJ/k0p0Ovr63H06FGIoojNmzdj9+7d49o0NDTg/fffhyAIiIyMxN/93d/NeLFENDvcLgGNZk+I32m/AbfghFKpQ7guC+GRmdBr4xEVpUZUrArRMSosWRoPl3tI7rLnPb+BLooijhw5gtdeew1GoxEHDhxAUVERUlNTvW1GRkbw3nvv4dVXX4XJZMLg4GBAiyaimeNyShgaEDDQ78bttnbcbm/CwNBNCKIdCoUGEfp0xC/IQkpyGmKNGkTHqBAZrfIZhUfHaMGjAOTnN9CbmpqQmJiIhIQEAMDatWtRW1vrE+inT59GcXExTCYTACA6OjpA5RLRw5IkCfZRT3gP9gsYHBAwaHFjYLAXI/YbGLbfhCDaoFSosMCUgYULc5Cbm4noGG1IHtcWivwGusVigdFo9D42Go1obGz0adPR0QG3243XX38do6Oj2LZtG0pKSsZ9roqKClRUVAAAysvLvT8Apl20Wv3Qzw2kYK0LCN7aWNf0TLUuUZQwNOBCX68Dll4HLD0O9PU64LCLAACnewAuqRVW2w2M2gehVCqRlbUIy5cvw5IlS7wrOme6rtk23+ryG+gTnVY/9h5SQRBw48YN/M3f/A2cTidee+015OTkIDk52addaWkpSktLvY8f9ri2YD3qLVjrAoK3NtY1PRPV5XZLsA56Rt33Rt9DgwJEwXNdqQQio1WIirVhyHYTnV1NGBi0QKFQIDU1Fbm5RcjOzvaee2u1WmG1Wh+5rmAQinWNzdX7+Q10o9GIvr4+7+O+vj7ExsaOaxMZGQm9Xg+9Xo/HHnsMt27deuALE9H02UcF9HS6fMJ7eFgE7o67NBoFomJVyMzWISpGBbVuFB2dLWhsNKOrqwsAkJSUhOUrSrBo0SKEh4fL+NXQTPMb6NnZ2ejo6EB3dzfi4uJQXV2N/fv3+7QpKirCv/3bv0EQBLjdbjQ1NWH79u0BK5oo1EmSBNuI6DvqHhBgHx3wtgkzeMI7OV2DqBgVomPVCDMoYLfb0dTUhPOXzLhz5w4AYMGCBVi3bh1ycnKmdOgyzU1+A12lUuH555/HwYMHIYoiNm3ahLS0NBw/fhwAUFZWhtTUVKxYsQLf/e53oVQq8dRTTyE9PT3gxROFAkGQMDx035TJgIChfgFut+e6QgFERClhilcjKTUKao0dUTEqn31QHA4HWlquw2w2o7W1FZIkITY2FsXFxcjNzR33WzWFpindh15QUICCggKfj5WVlfk83rlzJ3bu3DlzlRGFIJdTxOCAeHfU7cbQgADrkAjJ814lVGogKlqF1Ezt3VH33VsE757SYzLFeude763abGxsxM2bNyEIAqKiolBQUIDc3FyYTCbumTLPcKUoUQDcu0Vw7JSJbUT0ttHpFYiKUSE+yXNvd1SsCuERygeGsNvtRktLCxobG9HS0gKXy4Xw8HAsW7YMOTk5SExMZIjPYwx0okckihJGrKL33u57Ae5yfnmHWHikEjFxKqRnaz3hHaOa8kEOoiji9u3b3qX3drsder0eixcvRk5ODlJSUqBUchtaYqATTYvb7VmYc/+oe6JbBJNSvxx1R0WroJ7mToOSJKGjowNmsxlNTU2w2WzQaDTIy8tDRkYG0tLSoFJx4yvyxUAnmoTD7nuXyeCAgBHrl1MmGq0C0TGeWwSjYz2j7ogo5UOfgylJEnp6emA2m9HY2Air1QqVSoWFCxciNzcXmZmZSExMDMr7qik4MNBpXnO7JdiGRdhGRIwMC7ANixgZFjE8ZMWoTfC2CwtXIjpGhdQMz5uVUTEqhBkUMzJfbbFYYDabYTabMTAwAKVSifT0dKxZswZZWVnQarWP/Bo0PzDQKaTde3PSNiLeDW4BI8OiN8Qddt+V0GoNYAhXITnNAH2YyzNlEqOCVjuzc9SDg4NobGyE2Wz2jrhTU1NRUFCARYsWeVdtEk0HA53mPLdbwuiIZ2TtCW7hy3+PiN75bQCAAggLU8AQoUJCsgaGcCUMEUqE3/1bo/WMugOxZHx4eBhNTU0wm83o7OwEACQmJqKkhKs2aWYw0CnoSZIEh13yTofYRu5OjdwddY8dZavUQHiEEhFRKiQkaWCIUMIQrkR4hBJhBiWUqtm7rW90dBTNzc0wm824ffs2AM8+Hly1SYHAQKegILgl2Gzil6E9LNyd155glA1Ab1AgPMJzD/f9I2xDhBJa7czMbT8sz6pNz73ira2tEEXRu2ozJycHcXFxstVGoY2BTrPCO8oeEX3ehHQ57BgccMA+OsEoO1yJiEgV4hM1CI/4MrDDDErvyslg4Xa7fc7avHdyV35+Pldt0qxhoNOMEYSxgf3lSNs2LEIYO8oOUyA6VoUFCRpvWN8baWt18o6yp0IQBJ+zNl0uFwwGA5YuXYrc3Fyu2qRZx0CnKZMkCU7H/XPZd6dI7s5pjxtlq+ANam9o35vLDveMsoN1v+rJiKKIO3fueBf8OBwO6HQ65ObmIjc3l6s2SVYMdPIhCBJGbeKXt/bdf4/2iAjB7dteH6aAIUIJU4Ia4RGqL+8amSOj7KmQJAmdnZ3eBT/3Vm1mZWUhNzcX6enpXLVJQYGBPs94R9ne0fW94BYwMiLCbvMdZStV8E6DmOLVMESovpzPNih9DgoOJfev2jSbzd5Vm5mZmd5VmxqNRu4yiXww0EOMJElwOT2j7FGb5/5sm02E29WBAcsobMOid5/te3T6u6PsBZ7Avn8uW6cPjVH2VDkcDly+fBlNTU3o7e31rtpcvXo1srKypn3WJtFsUkgTHRo6S9rb26f9HPFX/wp15224XK4AVPRoNBpNwOsSoYBDFYFRdRRGVdEYVUdiVBUN+92/R9VREJS+S8VFSYRLcsAh2uEUHXDd/dspOeAUHZAgTvJqgadUKiCKsv0X/JIkQS0MQePsgQICRJUBblUk3KpIQBE80ylB019jsK7p2RAjYP1Xtj3Ucx/pTFGaXW6FBqOqKNjVUXdD2/dvhyoSksL3TTdRdMIhuTAEEX2SA0OCHcMQMCwJsEsuRApWaCD6HvitAKDQAEp5pw0UCgUkpbzfcDphBPGOVuiEYYyqwtGtz4BTHTHhAelyC4b+mgjrmp5BaTggn3fOjdCBuXuS9717sT3TISJGRzx/22wiRkc8H79/D20AnuDVAC6ViGEI6He70eV0YUB0Y1gSMAIRUAKJERokRWqRHOn52/NvLYwGNVTK4L2bRM66RkdHUVNTg6tXryIsLAzr1q3DY489FrCl/zOBdU1PKNbFEfosEQQJI1bBE9Ijd+ewx4S3OGZ2Q6UGlDpAUAG2MAEDOjd6nC7ctjvRL7gxChGSC1ArFZ7QjtNiUaQOSZGR40KbpkYURTQ0NODs2bNwOBxYsWIFiouLOT9Ocx4DfYome7Px/rB22AfGPU8XpoBGr4AUJkERDoxIbvS6XGi3u3DTZseIXQTsnrbe0I7RoigyfMKRNj2ajo4OVFZWoqenBykpKSgpKYHJZJK7LKIZwUC/SxQ926zeH9D3/tjuPh57D7ZSBYQZPEvRo+NVUBq06LXbYHG70OFwos3mRPuwE07rl9Mo3tCO0qI0Jdob2kmRGpgMGoZ2gNhsNpw5cwZffPEFwsPDsXXrVuTk5MyrO3go9M2bQHe7pPvmq8VxUyH2UQlj303Q6hQIM3j2EzElqAENMKIQMCAI6HY6ccfmRMewCx2dTjiFCUI7UovlSQaGtowEQcCVK1dw7tw5uN1uFBYWYuXKlTw0gkJSSAT6w7zZqFAAeoMSBoMCxgVqhIUroQ9TQlCLGBAF9Lrd6BxxoN3qRMegCx23Jw/tFYme0F6SugAG0cbQDhJtbW2oqqqCxWJBRkYGNm7ciNjYWLnLIgqYORfo1kEBbS196Ou1+Yy0x77ZqFZ7jg0LMygRa/QcZBBmUEJnUMCpFNHrcqFz2I1Gq9MT2n0udFj9h/aDRtomUwx6e8fMy9Css1qtOH36NBobGxEVFYVnnnkGCxcu5PQKhbw5F+jDVgH1tVbowzzTIdGxKiSmahBmUHpDWx+mgFUQPEFtdeH3Vic6epxotz44tJcnGpDM6ZE5y+12o66uDrW1tZAkCcXFxSgsLIRaPef+mxM9lDn3Pz0hSYP/sycbFksvLKNutFuduG11oMPqREcHQ3u+unnzJqqqqjA4OIjs7Gxs2LCBpwHRvDPnAr2+awS/+H9taBsYZWgTBgYGcOrUKdy4cQOxsbHYtWsXMjIy5C6LSBZzLtDDtSokRunx+AIdQ3sec7lcuHjxIi5evAilUol169ZhxYoV3MaW5rU5F+iLTWH4yZK0oFzOS4EnSRKam5tx6tQpWK1WLF68GOvWrUNERITcpRHJbs4FOs1fFosFVVVVaGtrg8lkQllZGVJSUuQuiyhoMNAp6DmdTpw/fx719fXQaDQoKSnBsmXLeNQb0RgMdApakiTh+vXrOHPmDEZGRpCXl4e1a9fCYDDIXRpRUGKgU1Dq6elBVVUV2tvbkZCQgO3btyMxMVHusoiCGgOdgordbvfuUa7T6bB582bk5eVxlSfRFDDQKShIkoSGhgZUV1fD4XBg2bJlWL16NfR6vdylEc0ZUwr0+vp6HD16FKIoYvPmzdi9e/eE7ZqamvDqq6/iO9/5DlavXj2jhVLo6uzsRFVVFbq6upCcnIySkhIsWLBA7rKI5hy/gS6KIo4cOYLXXnsNRqMRBw4cQFFREVJTU8e1++CDD7BixYqAFUuhxWaz4aOPPsKlS5cQHh6Op59+Grm5uZxeIXpIfgO9qakJiYmJSEhIAACsXbsWtbW14wL92LFjKC4uRnNzc2AqpZAhiiKuXLmCmpoauN1uFBQUYNWqVdyjnOgR+Q10i8UCo9HofWw0GtHY2Diuzfnz5/GDH/wA//zP/zzp56qoqEBFRQUAoLy8/KGP/lKr1UF5bFiw1gUET203btzA7373O3R1dSE7Oxs7duxAXFyc3GWNEyz9NRbrmp75VpffQJfGHuMDjPuV+P3338fXv/51vws9SktLUVpa6n38sMv3Q/Ek70CTu7bh4WGcPn0aZgxRFqQAAA6lSURBVLMZkZGR2L59O7KyshAXFxeUfSZ3f02GdU1PKNaVnJw86TW/gW40GtHX1+d93NfXN+7Ul+bmZvzjP/4jAGBoaAh1dXVQKpVYtWrVQxVMoUMQBO8e5aIoYtWqVSgsLIRGo5G7NKKQ4zfQs7Oz0dHRge7ubsTFxaG6uhr79+/3aXPo0CGffxcWFjLMCbdu3UJVVRUGBgaQlZWFDRs2IDo6Wu6yiEKW30BXqVR4/vnncfDgQYiiiE2bNiEtLQ3Hjx8HAJSVlQW8SJpbBgcHcerUKbS0tCAmJgY7d+5EZmam3GURhbwp3YdeUFCAgoICn49NFuT79u179KpoTnK73bh48SIuXLgAhUKBtWvXYsWKFTwCjmiW8DuNHpkkSWhpacGpU6cwNDSE3NxcrFu3DpGRkXKXRjSvMNDpkfT396Oqqgqtra0wGo34wz/8w3FrFIhodjDQ6aE4nU7U1tairq4OarUaGzduxBNPPME9yolkxECnaZEkCWazGadPn+Ye5URBhoFOU9bb24vKykq0t7cjPj4e27ZtQ1JSktxlEdFdDHTyy+FwoKamBleuXIFOp8NTTz2FvLw8Tq8QBRkGOk1KkiR8/vnnqK6uht1ux9KlS7FmzRruUU4UpBjoNKGuri5UVlaiq6sLSUlJKCkpQXx8vNxlEdEDMNDJx+joKKqrq9HQ0ACDwYAtW7ZgyZIl3KOcaA5goBMAzx7lV69eRU1NDVwuF/Lz87Fq1SrodDq5SyOiKWKgE9rb21FZWYne3l6kpaVh48aNPnvgE9HcwECfx4aHh3HmzBlcv34dERER+IM/+AMsWrSI0ytEcxQDfR4SBAGXL1/GuXPnIAgCVq5ciaKiIu5RTjTHMdDnmdbWVlRVVaG/vx+ZmZnYuHEjYmJi5C6LiGYAA32eGBgYwG9/+1s0NzcjOjoaO3bswMKFC+Uui4hmEAM9xLlcLtTV1eHixYuQJAlr1qxBfn4+9ygnCkH8rg5RbrcbDQ0NqK2thc1mw+OPP45Vq1Zxj3KiEMZADzGCIOCLL77A+fPnMTw8jJSUFGzbtg1PPPFEUJ5+TkQzh4EeIkRRhNlsxrlz5zA4OIiEhARs2bIFqampvA2RaJ5goM9xkiShubkZNTU1sFgsMJlM2LFjBzIzMxnkRPMMA32OkiQJt27dwtmzZ9HT04PY2FguDCKa5xjoc1BbWxtqamrQ0dGBqKgobNmyBYsXL+b+5ETzHAN9Duno6EBNTQ3a2toQHh6OTZs2IS8vDyqVSu7SiCgIMNDngO7ubtTU1ODmzZsICwvDhg0bsGzZMt5LTkQ+mAhBzGKxoKamBk1NTdDpdFizZg2WL18OrVYrd2lEFIQY6EFoYGAA58+fx/Xr16FWq7Fq1Srk5+dzb3IieiAGehCxWq2ora3F559/DqVSifz8fBQWFiIsLEzu0ohoDmCgBwGbzYYLFy7gypUrAIClS5di5cqVCA8Pl7kyIppLGOgystvtuHjxIi5fvgxBEPDYY49h1apViIqKkrs0IpqDGOgycDgcqK+vR11dHZxOJxYvXozi4mLuS05Ej4SBPotcLheuXLmCixcvwm63Izs7G6tXr+b5nUQ0Ixjos2DsVrYZGRlYvXo1EhIS5C6NiEIIAz2AJtvKNjk5We7SiCgEMdADYKKtbEtLS5GWlsaNs4goYKYU6PX19Th69ChEUcTmzZuxe/dun+unTp3Cxx9/DADQ6/X45je/iczMzBkvNthJkoSmpiafrWyfeeYZLFy4kEFORAHnN9BFUcSRI0fw2muvwWg04sCBAygqKkJqaqq3TXx8PF5//XVERESgrq4O//Iv/4If//jHAS08mNzbyvbDDz9ER0cHYmNjsXXrVuTk5DDIiWjW+A30pqYmJCYmet/AW7t2LWpra30CffHixd5/5+TkoK+vLwClBqf7t7KNjY3lVrZEJBu/gW6xWHxuqzMajWhsbJy0/WeffYb8/PwJr1VUVKCiogIAUF5eDpPJNN16AQBqtfqhnztT2tra8L//+79oaWlBVFQUduzYgZUrV8pa04MEQ59NhHVND+uanvlWl99AlyRp3Mcmm0a4du0aTpw4gb//+7+f8HppaSlKS0u9jx/20GKTySTbgcc9PT04e/bspFvZButBzHL22YOwrulhXdMTinU96C45v4FuNBp9plD6+voQGxs7rt2tW7fw85//HAcOHEBkZORDFRrMuJUtEQU7v4GenZ2Njo4OdHd3Iy4uDtXV1di/f79Pm97eXrz55pv49re/HXL3WA8ODuLcuXPcypaIgp7fQFepVHj++edx8OBBiKKITZs2IS0tDcePHwcAlJWV4de//jWGh4fx3nvveZ9TXl4e2MoD7P6tbBUKBbeyJaKgN6X70AsKClBQUODzsbKyMu+/9+zZgz179sxsZTK5t5Xt1atXIUkSt7IlojmDK0Xv4la2RDTXzftAn2gr21WrVk34xi8RUTCbt4E+0Va2xcXFQXnPKhHRVMy7QOdWtkQUquZNoHMrWyIKdSEf6NzKlojmi5ANdEmS0NzczK1siWjeCLlAv7eV7dmzZ9HT08OtbIlo3gipQL99+zbOnj2Ljo4OREVFcStbIppXQiLQOzo6UFNTg7a2NoSHh2PTpk3Iy8uDSqWSuzQiolkzpwPd31a2RETzyZxMvp6eHhw7doxb2RIR3WfOBfr169dx/PhxbmVLRDTGnAv0tLQ0rFu3DkuWLIHBYJC7HCKioDHnbv8wGAwoKytjmBMRjTHnAp2IiCbGQCciChEMdCKiEMFAJyIKEQx0IqIQwUAnIgoRDHQiohDBQCciChEMdCKiEMFAJyIKEQx0IqIQwUAnIgoRDHQiohDBQCciChEMdCKiEMFAJyIKEQx0IqIQwUAnIgoRDHQiohDBQCciChHqqTSqr6/H0aNHIYoiNm/ejN27d/tclyQJR48eRV1dHXQ6Hfbu3YusrKyAFExERBPzO0IXRRFHjhzBK6+8gp/+9Kc4c+YMbt++7dOmrq4OnZ2deOedd/DCCy/gvffeC1jBREQ0Mb+B3tTUhMTERCQkJECtVmPt2rWora31aXPhwgVs3LgRCoUCubm5GBkZQX9/f8CKJiKi8fxOuVgsFhiNRu9jo9GIxsbGcW1MJpNPG4vFgtjYWJ92FRUVqKioAACUl5cjOTn5oQt/lOcGUrDWBQRvbaxreljX9MynuvyO0CVJGvcxhUIx7TYAUFpaivLycpSXl0+nxnG+//3vP9LzAyVY6wKCtzbWNT2sa3rmW11+A91oNKKvr8/7uK+vb9zI22g0ore394FtiIgosPwGenZ2Njo6OtDd3Q23243q6moUFRX5tCkqKsLJkychSRLMZjMMBgMDnYholqlef/311x/UQKlUIjExET/72c/wySefYMOGDVi9ejWOHz+O5uZmZGdnIzExEWazGe+//z7q6+vxrW99C3FxcQEtPFhviwzWuoDgrY11TQ/rmp75VJdCmmgCnIiI5hyuFCUiChEMdCKiEDGlpf9yOXz4MC5duoTo6Gi89dZb467LteWAv7oaGhrwk5/8BPHx8QCA4uJifOUrXwloTb29vTh06BAGBgagUChQWlqKbdu2+bSRo7+mUpcc/eV0OvGDH/wAbrcbgiBg9erVeO6553zayNFfU6lLjv66RxRFfP/730dcXNy4W+/k3ALkQXXJ2V/79u2DXq+HUqmESqUad8v2jPeZFMQaGhqk5uZm6aWXXprw+sWLF6WDBw9KoihK169flw4cOBAUdV27dk36h3/4h1mp5R6LxSI1NzdLkiRJNptN2r9/v9TW1ubTRo7+mkpdcvSXKIrS6OioJEmS5HK5pAMHDkjXr1/3aSNHf02lLjn6657f/OY30ttvvz3h68v1/eivLjn7a+/evdLg4OCk12e6z4J6yiUvLw8RERGTXpdrywF/dckhNjbW+5M9LCwMKSkpsFgsPm3k6K+p1CUHhUIBvV4PABAEAYIgjFsMJ0d/TaUuufT19eHSpUvYvHnzhNfl+n70V1cwm+k+C+opF3+muuWAHMxmM15++WXExsbiG9/4BtLS0mbttbu7u3Hjxg0sWrTI5+Ny99dkdQHy9Jcoivje976Hzs5OPP3008jJyfG5Lld/+asLkKe/3n//ffzpn/4pRkdHJ7wuV3/5qwuQ9/vx4MGDAIAtW7agtLTU59pM99mcDnRpilsOzLaFCxfi8OHD0Ov1uHTpEt544w288847s/Ladrsdb731Fv78z/8cBoPB55qc/fWguuTqL6VSiTfeeAMjIyN488030draivT0dO91ufrLX11y9NfFixcRHR2NrKwsNDQ0TNhGjv6aSl1yfj/+8Ic/RFxcHAYHB/GjH/0IycnJyMvL816f6T4L6ikXf4J1ywGDweD9tbmgoACCIGBoaCjgr+t2u/HWW29hw4YNKC4uHnddrv7yV5dc/XVPeHg48vLyUF9f7/Nxuf9/TVaXHP11/fp1XLhwAfv27cPbb7+Na9eujQtFOfprKnXJ+f/r3gLL6OhorFy5Ek1NTT7XZ7rP5nSgB+uWAwMDA96fvE1NTRBFEZGRkQF9TUmS8O677yIlJQXPPPPMhG3k6K+p1CVHfw0NDWFkZASA586Sq1evIiUlxaeNHP01lbrk6K+vfe1rePfdd3Ho0CG8+OKLWLp0Kfbv3+/TRo7+mkpdcvQX4Pmt9N40kN1ux5UrV3x+0wJmvs+Cesrl7bffxueffw6r1Yo9e/bgueeeg9vtBgCUlZUhPz8fly5dwv79+6HVarF3796gqKumpgbHjx+HSqWCVqvFiy++GPBfPa9fv46TJ08iPT0dL7/8MgDgT/7kT7w//eXqr6nUJUd/9ff349ChQxBFEZIkYc2aNSgsLMTx48e9dcnRX1OpS47+mozc/TWVuuTqr8HBQbz55psAPG9wr1+/HitWrAhon3HpPxFRiJjTUy5ERPQlBjoRUYhgoBMRhQgGOhFRiGCgExGFCAY6EVGIYKATEYWI/w8fZq+tiPs7AgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "to_plot = copy.deepcopy(rnd_walk_states)\n",
    "to_plot.remove(rnd_walk_state_idxs[TERMINAL])\n",
    "\n",
    "for n_episode in [0, 1, 10, 100]:\n",
    "    state_values = run_rnd_td0(n_episode)\n",
    "    plt.plot(\n",
    "        to_plot, \n",
    "        [state_values[s] for s in to_plot],\n",
    "        label=f\"{n_episode}\",\n",
    "    )\n",
    "    \n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

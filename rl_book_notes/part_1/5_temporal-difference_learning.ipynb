{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import enum\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable\n",
    "from typing import Mapping\n",
    "from typing import Optional\n",
    "from typing import Sequence\n",
    "from typing import Tuple\n",
    "from typing import TypeVar\n",
    "\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Difference Learning\n",
    "\n",
    "## <p style=\"color:red\">Text content copied near verbatim from: <a href=\"http://incompleteideas.net/book/the-book-2nd.html\">Sutton and Barto</a>. Code is my own unless otherwise stated.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "- TD relation to Monte Carlo and DP methods.\n",
    "\n",
    "- Batch MC vs Batch TD:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "**TD(0) / _n-step_ TD**: \n",
    "\n",
    "**TD error**:\n",
    "\n",
    "**Markov Reward Process**:\n",
    "\n",
    "**Batch Updating**:\n",
    "\n",
    "**Certainty-equivalence estimate**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Temporal-difference (TD) learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas.\n",
    "\n",
    "Like Monte Carlo methods, TD methods can directly learn from raw experience without a model of the environment's dynamics.\n",
    "\n",
    "Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they _bootstrap_).\n",
    "\n",
    "DP, TD, and Monte Carlo methods all use some form of generalized policy iteration (GPI) to solve the control problem. The differences in the methods are primarily due to differences in their approaches to the prediction problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Prediction\n",
    "\n",
    "Both TD and Monte Carlo methods use experience to solve the prediction problem. Given some experience following a policy $\\pi$, both methods update their estimate $V$ of $v_\\pi$ for the nonterminal states $S_t$ occurring in that experience. \n",
    "\n",
    "---\n",
    "\n",
    "Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V(S_t)$. A simple every-visit Monte Carlo method suitable for nonstationary environments is the _constant-$\\alpha$ MC_:\n",
    "\n",
    "\\begin{equation}\n",
    "    V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ G_t - V(S_t) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "where $G_t$ is the actual return following time $t$ and $\\alpha$ is a constant step-size parameter.\n",
    "\n",
    "Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_t)$ (only then is $G_t$ known).\n",
    "\n",
    "---\n",
    "\n",
    "TD methods only need to wait until the next time step to determine the increment. At time $t + 1$ they immediately form a target and make a useful update using the observed reward $R_{t + 1}$ and the estimate $V(S_{t + 1})$. The simplest TD method makes the update\n",
    "\n",
    "\\begin{equation}\n",
    "    V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "immediately on transition to $S_{t + 1}$ and receiving $R_{t + 1}$. \n",
    "\n",
    "---\n",
    "\n",
    "In effect, the target for the Monte Carlo update is $G_t$ whereas the target for the TD update is $R_{t + 1} + \\gamma V(S_{t + 1})$. \n",
    "\n",
    "This TD method is called $TD(0)$ or _one-step_ TD because it is a special case of the TD($\\lambda$) and _n-step_ TD methods developed in Chapters 12 and 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "StateT = TypeVar(\"StateT\")\n",
    "ActionT = TypeVar(\"ActionT\")\n",
    "RewardT = TypeVar(\"RewardT\", int, float)\n",
    "\n",
    "TraceT = Sequence[Tuple[StateT, ActionT, RewardT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_td0(\n",
    "    policy: Mapping[StateT, Mapping[ActionT, float]],\n",
    "    init_state: Callable[[], StateT],\n",
    "    is_terminal: Mapping[StateT, bool],\n",
    "    simulate: Callable[[StateT, ActionT], Tuple[StateT, RewardT]],\n",
    "    n_episode: int,\n",
    "    alpha: float = 0.1,\n",
    "    gamma: float = 1.0,\n",
    "    init_fn: Optional[Callable[[StateT], float]] = None\n",
    ") -> Mapping[StateT, float]:\n",
    "    state_values = {}\n",
    "    \n",
    "    for state in policy.keys():\n",
    "        state_values[state] = 0 if init_fn is None else init_fn(state) \n",
    "        \n",
    "    for _ in tqdm.trange(n_episode):\n",
    "        state = init_state()\n",
    "        \n",
    "        while not is_terminal[state]:\n",
    "            action = random.choices(\n",
    "                population=list(policy[state].keys()), \n",
    "                weights=policy[state].values()\n",
    "            )[0]   # random.choices returns a 1-element list\n",
    "            next_state, reward = simulate(state, action)\n",
    "\n",
    "            state_values[state] += alpha*(reward + gamma*state_values[next_state] - state_values[state])\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Confirming this works on the simple grid-world environment from Example 4.1 in Chapter 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_states: Sequence[int] = list(range(4*4))   # 4x4 grid, states numbered left to right top to bottom \n",
    "\n",
    "    \n",
    "class GWAction(enum.Enum):\n",
    "    UP    = enum.auto()\n",
    "    DOWN  = enum.auto()\n",
    "    RIGHT = enum.auto()\n",
    "    LEFT  = enum.auto()\n",
    "\n",
    "    \n",
    "def gw_init_state() -> int:\n",
    "    return random.choice(gw_states)\n",
    "\n",
    "\n",
    "gw_is_terminal: Mapping[int, bool] = {\n",
    "    state: state == 0 or state == 15\n",
    "    for state in gw_states\n",
    "}\n",
    "\n",
    "\n",
    "def gw_simulate(state: int, action: GWAction) -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "        \n",
    "    \"\"\"\n",
    "    if action == GWAction.LEFT:\n",
    "        if state % 4 == 0:\n",
    "            #  unable to move further left\n",
    "            return state, -1\n",
    "        return state - 1, -1\n",
    "    \n",
    "    if action == GWAction.RIGHT:\n",
    "        if (state + 1) % 4 == 0:\n",
    "            # unable to move further right\n",
    "            return state, -1\n",
    "        return state + 1, -1\n",
    "    \n",
    "    if action == GWAction.UP:\n",
    "        if state < 4:\n",
    "            return state, -1\n",
    "        return state - 4, -1\n",
    "    \n",
    "    if action == GWAction.DOWN:\n",
    "        if state > 11:\n",
    "            return state, -1\n",
    "        return (state + 4) % 15, -1 \n",
    "\n",
    "    raise ValueError(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_equiprob_policy = {}\n",
    "for state in gw_states:\n",
    "    gw_equiprob_policy[state] = {}\n",
    "    for action in GWAction:\n",
    "        gw_equiprob_policy[state][action] = 1.0 / len(GWAction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:41<00:00, 24020.41it/s]\n"
     ]
    }
   ],
   "source": [
    "state_values = tabular_td0(\n",
    "    policy=gw_equiprob_policy,\n",
    "    init_state=gw_init_state,\n",
    "    is_terminal=gw_is_terminal,\n",
    "    simulate=gw_simulate,\n",
    "    n_episode=int(1e6),\n",
    "    alpha=0.0001,\n",
    "    gamma=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000 -13.902 -19.942 -21.965 \n",
      "-14.028 -17.943 -19.976 -20.052 \n",
      "-20.013 -19.993 -18.052 -14.233 \n",
      "-22.011 -19.972 -13.981   0.000 \n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        print(f\"{state_values[i*4 + j]: 7.3f}\", end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the actual state-value function for this problem is:\n",
    "\n",
    "```\n",
    "| 0.0 | -14 | -20 | -22 |\n",
    "| -14 | -18 | -20 | -20 |\n",
    "| -20 | -20 | -18 | -14 |\n",
    "| -22 | -20 | -14 | 0.0 |\n",
    "```\n",
    "\n",
    "so we see that the method works!\n",
    "\n",
    "Notice that $\\alpha$ must be small for the computed values to get close to their true value.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD(0) bases its update in part on an existing estimate so it is a _bootstrapping_ method like DP. We know from Chapter 3 that:\n",
    "\n",
    "\\begin{align}\n",
    "    v_\\pi(s) &\\doteq \\mathbb{E}_\\pi \\left[ G_t | S_t = s \\right] \\\\\n",
    "             &= \\mathbb{E}_\\pi \\left[ R_{t + 1} + \\gamma G_{t + 1} | S_t = s \\right] \\\\\n",
    "             &= \\mathbb{E}_\\pi \\left[ R_{t + 1} + \\gamma v_\\pi(S_{t + 1}) | S_t = s \\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo methods use an estimate of $\\mathbb{E}_\\pi[G_t | S_t = s]$ as the target. It is an estimate as the expected value is not known; a sample return is used in place of the real expected return.\n",
    "\n",
    "DP methods use an estimate of $\\mathbb{E}_\\pi[R_{t + 1} + \\gamma v_\\pi(S_{t + 1}) | S_t = s]$ as the target. It is an estimate not because of the expected values, which are assumed to be completely provided by a model of the environment, but because $v_\\pi(S_{t + 1})$ is not known and the current estimate $V(S_{t + 1})$ is used instead.\n",
    "\n",
    "The TD target is an estimate for both reasons:\n",
    "\n",
    "1. It samples the expected values.\n",
    "2. It uses the current estimate $V$ instead of the true $v_\\pi$.\n",
    "\n",
    "**TD methods combine the sampling of Monte Carlo with the bootstrapping of DP.**\n",
    "\n",
    "---\n",
    "\n",
    "Note that in the update rule for TD the quantity in brackets, $\\left[ R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t) \\right]$, is a sort of error measuring the difference between the estimated value of $S_t$ and the better estimate $R_{t + 1} + \\gamma V(S_{t + 1})$. This is called the **TD error** and arises in various forms throughout reinforcement learning:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_t \\doteq R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t)\n",
    "\\end{equation}\n",
    "\n",
    "This quantity depends on the next state and reward so is not available until one time step later. If the array V does not change during the episode (e.g. Monte Carlo methods) then the Monte Carlo error can be written as a sum of TD errors:\n",
    "\n",
    "\\begin{align}\n",
    "    G_t - V(S_t) &= R_{t + 1} + \\gamma G_{t + 1} - V(S_t) \\\\\n",
    "                 &= R_{t + 1} + \\gamma G_{t + 1} - V(S_t) + \\gamma V(S_{t + 1}) - \\gamma V(S_{t + 1}) \\\\\n",
    "                 &= \\delta_t + \\gamma \\delta_{t + 1} + \\gamma^2(G_{t + 2} - V(S_{t + 2})) \\\\\n",
    "                 &= \\delta_t + \\gamma \\delta_{t + 1} + \\gamma^2\\delta_{t + 2} + \\dots + \\gamma^{T - t - 1} \\delta_{T - 1} + \\gamma^{T - t}(0 - 0) \\\\\n",
    "                 &= \\sum_{k = t}^{T - 1} \\gamma^{k - t} \\delta_k\n",
    "\\end{align}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 6.1: If V changes during the episode then the above only holds approximately; what would the difference be between the two sides? Let $V_t$ denote the array of state values used at time $t$ in the TD error and in the TD update. Redo the derivation above to determine the additional amount that must be added to the sum of TD errors in order to equal the Monte Carlo error.\n",
    "\n",
    "\\begin{align}\n",
    "    V_{t + 1}(S_t) &= V_t(S_t) + \\alpha [R_{t + 1} + \\gamma V_t(S_{t + 1}) - V_t(S_t)] \\\\\n",
    "                   &= V_t(S_t) + \\alpha [R_{t + 1} + \\gamma V_t(S_{t + 1}) - V_t(S_t)] \\\\\n",
    "                   &= V_t(S_t) + \\alpha \\delta_t \\\\\n",
    "\\end{align}\n",
    "\n",
    "Rearranging for $V_t(S_t)$\n",
    "\n",
    "\\begin{align}\n",
    "    V_t(S_t) &= V_{t + 1}(S_t) - \\alpha \\delta_t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following through with the proof similar to the steps in (6.6):\n",
    "\n",
    "\\begin{align}\n",
    "    G_t - V_t(S_t) &\\doteq R_{t + 1} + \\gamma G_{t + 1} - V_t(S_t) \\\\\n",
    "                   &=      R_{t + 1} + \\gamma V_t(S_{t + 1}) - V_t(S_t) + \\gamma G_{t + 1} - \\gamma V_t(S_{t + 1}) \\\\\n",
    "                   &= \\delta_t + \\gamma(G_{t + 1} - V_t(S_{t + 1})) \\\\\n",
    "                   &= \\delta_t + \\gamma(G_{t + 1} - V_{t + 1}(S_{t + 1}) + \\alpha \\delta_t) \\\\\n",
    "                   &= \\delta_t + \\gamma \\alpha \\delta_t + \\gamma(G_{t + 1} - V_{t + 1}(S_{t + 1})) \\\\\n",
    "                   &= \\delta_t(1 + \\gamma \\alpha) + \\gamma(G_{t + 1} - V_{t + 1}(S_{t + 1})) \\\\\n",
    "                   &= \\delta_t(1 + \\gamma \\alpha) + \\gamma(\\delta_{t + 1}(1 + \\gamma \\alpha) + \\gamma(G_{t + 2} - V_{t + 2}(S_{t + 2}))) \\\\\n",
    "                   &= \\delta_t(1 + \\gamma \\alpha) + \\gamma(\\delta_{t + 1}(1 + \\gamma \\alpha)) + \\gamma(G_{t + 2} - V_{t + 2}(S_{t + 2})) \\\\\n",
    "                   &= (1 + \\gamma \\alpha)(\\delta_t + \\gamma\\delta_{t + 1}) + \\gamma^2(\\delta_{t + 2}(1 + \\gamma \\alpha) + \\gamma(G_{t + 3} - V_{t + 3}(S_{t + 3}))) \\\\\n",
    "                   &= (1 + \\gamma \\alpha)(\\delta_t + \\gamma\\delta_{t + 1}) + \\gamma^2\\delta_{t + 2}(1 + \\gamma \\alpha) + \\gamma^3(G_{t + 3} - V_{t + 3}(S_{t + 3})) \\\\\n",
    "                   &= (1 + \\gamma \\alpha)(\\delta_t + \\gamma\\delta_{t + 1} + \\gamma^2\\delta_{t + 2}) + \\gamma^3(G_{t + 3} - V_{t + 3}(S_{t + 3})) \\\\\n",
    "                   &= (1 + \\gamma \\alpha)\\sum_{k=t}^{T - 1}\\gamma^{k - t} \\delta_t \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively this makes sense.\n",
    "\n",
    "To see why, first note that the original equation _without_ the time subscript telescopes to produce the sum of discounted rewards equation. This is a convoluted way of getting from the RHS back to the LHS :-)\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_t \\doteq R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    G_t - V(S_t) &= \\sum_{k = t}^{T - 1} \\gamma^{k - t} \\delta_k \\\\\n",
    "                 &= R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t) + \\gamma ( R_{t + 2} + \\gamma V(S_{t + 2}) - V(S_{t + 1})) + \\sum_{k = t + 2}^{T - 1} \\gamma^{k - t} \\delta_k \\\\\n",
    "                 &= R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t) + \\gamma R_{t + 2} + \\gamma^2 V(S_{t + 2}) - \\gamma  V(S_{t + 1}) + \\sum_{k = t + 2}^{T - 1} \\gamma^{k - t} \\delta_k \\\\\n",
    "                 &= R_{t + 1} + \\gamma R_{t + 2} - V(S_t) + \\gamma^2 V(S_{t + 2})+ \\sum_{k = t + 2}^{T - 1} \\gamma^{k - t} \\delta_k \\\\\n",
    "                 &= \\left[\\sum_{k = t + 1}^{T} \\gamma^{k - t - 1}R_{k}\\right] - V(S_t)\n",
    "\\end{align}\n",
    "\n",
    "Now, from the above we also know:\n",
    "\n",
    "\\begin{equation}\n",
    "    V_{t + 1}(S_t) - V_t(S_t) = \\alpha \\delta_t\n",
    "\\end{equation}\n",
    "\n",
    "Distributing the summation gives:\n",
    "\n",
    "\\begin{align}\n",
    "    G_t - V_t(S_t) &= (1 + \\gamma \\alpha) \\sum_{k = t}^{T - 1} \\gamma^{k - t}\\delta_t \\\\\n",
    "                   &= \\sum_{k = t}^{T - 1} \\gamma^{k - t}\\delta_t + \\gamma \\alpha \\sum_{k = t}^{T - 1} \\gamma^{k - t}\\delta_t \\\\\n",
    "                   &= \\sum_{k = t}^{T - 1} \\gamma^{k - t}\\delta_t + \\sum_{k = t}^{T - 1} \\gamma^{k - t + 1} \\alpha \\delta_t \\\\\n",
    "                   &= \\sum_{k = t}^{T - 1} \\gamma^{k - t} \\delta_t + \\gamma^{k - t + 1} \\alpha \\delta_t\n",
    "\\end{align}\n",
    "\n",
    "It is the original sum of discounted TD errors plus another term. This other term accounts for the sum of discounted differences/deltas between the state-value estimate at step $t + 1$ and at step $t$. i.e. it is the difference, $\\alpha \\delta_k$, weighted by the discounting factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Example 6.1: Driving Home (/Exercise 6.2)\n",
    "\n",
    "Consider driving home and at various points estimating the time remaining. This can be considered an RL problem where the state is the weather, your location, current time etc. The rewards are the negated time taken to get to the point on your journey home. The value function is the negated estimate of how long you think it should take to get home from the current state.\n",
    "\n",
    "Let $\\gamma = 1$ and $\\alpha = 1$. \n",
    "\n",
    "For Monte Carlo methods the error, $G_t - V(S_t)$ is the difference between the actual time taken and the estimated time remaining. This requires knowing how long the journey took in total so updates can only be done offline. It also means that changes that occur in just one state can have big impacts as to the predicted times of all other states, even if in practice they are unchanged. \n",
    "\n",
    "For methods that use temporal difference learning the error is $R_t + \\gamma V(S_{t + 1}) - V(S_t)$. The estimated return, in this case time remaining, can be updated as soon as the next state and reward are known. In the case of driving home, if you predict it will take 30 minutes to get home and it takes 5 minutes to reach the next state where you now think it will take 20 minutes, the prior estimate can be updated by $5 + 20 - 30 = -5$ minutes _without_ needing to actually reach your house.\n",
    "\n",
    "Note that this means if your next state has an accurate estimate of the time taken to reach home then you are able to quickly converge the current state's estimate to the true expected value. Consider changing jobs to one that is in a different location but after a slightly different, longer start uses the same route home. For MC Methods the error for all states will initially be increased despite it only being the few starting states that have changed and they will need to be decreased again as the starting states converge.For TD methods only these initial states will be updated. \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of TD Prediction Methods\n",
    "\n",
    "**Bootstrap**: Learning to guess from a guess. TD methods update their estimates based in part on other estimates.\n",
    "\n",
    "TD Method Advantages:\n",
    "\n",
    "1. Compared to DP methods, they do not require a model of the environment (DP methods use to compute expectation over all possible next states and rewards). \n",
    "2. Compared to Monte Carlo methods, they can be implemented in an online, fully incremental fashion (Monte Carlo methods must wait until the end of the episode to get the final reward). Some applications have very long episodes where delaying all learning until the end of the episode is too slow. Other applications have no episodes at all! \n",
    "3. Compared to Monte Carlo methods, TD methods can learn from each transition regardless of what subsequent actions are taken. Some Monte Carlo methods must ignore or discount episodes on which experimental actions are taken, which can greatly slow learning (if action is non-greedy towards end of episode, importance sampling ratio is zero and therefore all updates at time before the step become zero).\n",
    "\n",
    "\n",
    "Convergence can be guaranteed for TD methods: for any fixed policy $\\pi$, $TD(0)$ has been proved to converged to $v_\\pi$. In the mean for a constant step size parameter if it is sufficiently small or with probability 1 if it decreases. \n",
    "\n",
    "\n",
    "It is an open question as to which of Monte Carlo or TD methods are more data efficient/learns faster. In practice TD methods have usually been found to converge faster than constant-$\\alpha$ MC methods on stochastic tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"662pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 662.00 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-40 658,-40 658,4 -4,4\"/>\n",
       "<!-- endL -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>endL</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"54,-36 0,-36 0,0 54,0 54,-36\"/>\n",
       "</g>\n",
       "<!-- A -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>A</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"127\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"127\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">A</text>\n",
       "</g>\n",
       "<!-- endL&#45;&gt;A -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>endL&#45;&gt;A</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M64.1466,-18C72.4881,-18 81.3415,-18 89.6896,-18\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"64.0615,-14.5001 54.0614,-18 64.0614,-21.5001 64.0615,-14.5001\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"89.917,-21.5001 99.9169,-18 89.9169,-14.5001 89.917,-21.5001\"/>\n",
       "<text text-anchor=\"middle\" x=\"77\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0</text>\n",
       "</g>\n",
       "<!-- B -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>B</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"227\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"227\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">B</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;B -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>A&#45;&gt;B</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M164.1466,-18C172.4881,-18 181.3415,-18 189.6896,-18\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"164.0615,-14.5001 154.0614,-18 164.0614,-21.5001 164.0615,-14.5001\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"189.917,-21.5001 199.9169,-18 189.9169,-14.5001 189.917,-21.5001\"/>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0</text>\n",
       "</g>\n",
       "<!-- C -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>C</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"327\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"327\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">C</text>\n",
       "</g>\n",
       "<!-- B&#45;&gt;C -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>B&#45;&gt;C</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M264.1466,-18C272.4881,-18 281.3415,-18 289.6896,-18\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"264.0615,-14.5001 254.0614,-18 264.0614,-21.5001 264.0615,-14.5001\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"289.917,-21.5001 299.9169,-18 289.9169,-14.5001 289.917,-21.5001\"/>\n",
       "<text text-anchor=\"middle\" x=\"277\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0</text>\n",
       "</g>\n",
       "<!-- D -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>D</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"427\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"427\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">D</text>\n",
       "</g>\n",
       "<!-- C&#45;&gt;D -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>C&#45;&gt;D</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M364.1466,-18C372.4881,-18 381.3415,-18 389.6896,-18\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"364.0615,-14.5001 354.0614,-18 364.0614,-21.5001 364.0615,-14.5001\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"389.917,-21.5001 399.9169,-18 389.9169,-14.5001 389.917,-21.5001\"/>\n",
       "<text text-anchor=\"middle\" x=\"377\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0</text>\n",
       "</g>\n",
       "<!-- E -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>E</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"527\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"527\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">E</text>\n",
       "</g>\n",
       "<!-- D&#45;&gt;E -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>D&#45;&gt;E</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M464.1466,-18C472.4881,-18 481.3415,-18 489.6896,-18\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"464.0615,-14.5001 454.0614,-18 464.0614,-21.5001 464.0615,-14.5001\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"489.917,-21.5001 499.9169,-18 489.9169,-14.5001 489.917,-21.5001\"/>\n",
       "<text text-anchor=\"middle\" x=\"477\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0</text>\n",
       "</g>\n",
       "<!-- endR -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>endR</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"654,-36 600,-36 600,0 654,0 654,-36\"/>\n",
       "</g>\n",
       "<!-- E&#45;&gt;endR -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>E&#45;&gt;endR</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M564.1466,-18C572.4881,-18 581.3415,-18 589.6896,-18\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"564.0615,-14.5001 554.0614,-18 564.0614,-21.5001 564.0615,-14.5001\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"589.917,-21.5001 599.9169,-18 589.9169,-14.5001 589.917,-21.5001\"/>\n",
       "<text text-anchor=\"middle\" x=\"577\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fbc77930190>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 6.2: Random Walk\n",
    "\n",
    "# A Markov reward process (MRP) is a MDP without actions.\n",
    "#\n",
    "# MRPs are often useful when focusing on the prediction problem is which there is no need\n",
    "# to distinguish between the dynamics due to the environment and those due to the agent.\n",
    "# (i.e. the state->action, action->next state probabilities collapse to state->next state)\n",
    "#\n",
    "# Consider the MRP:\n",
    "\n",
    "g = graphviz.Digraph(graph_attr={\"rankdir\": \"LR\"})\n",
    "\n",
    "nodes = [\n",
    "    \"endL\", \"A\", \"B\", \"C\", \"D\", \"E\", \"endR\"\n",
    "]\n",
    "for n in nodes:\n",
    "    end = \"end\" in n\n",
    "    g.node(\n",
    "        n,\n",
    "        label=None if not end else \"\",\n",
    "        shape=\"box\" if end else None,\n",
    "        style=\"filled\" if end else None,\n",
    "    )\n",
    "    \n",
    "for l, r in zip(nodes, nodes[1:]):\n",
    "    g.edge(\n",
    "        l, r,\n",
    "        dir=\"both\", \n",
    "        label=\"0\" if r != \"endR\" else \"1\"\n",
    "    )\n",
    "    \n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The starting state is always C and it proceeds left or right both with probability 0.5 until\n",
    "# either end state is reaching. The reward is always 0 except when the episode terminates on\n",
    "# the right where it is +1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandWalkAction(enum.Enum):\n",
    "    LEFT = enum.auto()\n",
    "    RIGHT = enum.auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TERMINAL = \".\"\n",
    "rnd_walk_state_idxs = {\n",
    "    s: i for i, s in enumerate([TERMINAL] + list(\"ABCDE\"))\n",
    "}\n",
    "rnd_walk_states = list(rnd_walk_state_idxs.values())\n",
    "\n",
    "rnd_walk_policy = {\n",
    "    k: {a: 0.5 for a in RandWalkAction}\n",
    "    for k in rnd_walk_states\n",
    "}\n",
    "\n",
    "rnd_init_state = lambda: rnd_walk_state_idxs[\"C\"]\n",
    "\n",
    "rnd_is_terminal = {\n",
    "    s: s == rnd_walk_state_idxs[TERMINAL] \n",
    "    for s in rnd_walk_states\n",
    "}\n",
    "\n",
    "def rnd_simulate(state, action):\n",
    "    if state == rnd_walk_state_idxs[TERMINAL]:\n",
    "        return rnd_walk_state_idxs[TERMINAL], 0\n",
    "    \n",
    "    if action == RandWalkAction.LEFT:\n",
    "        return state - 1, 0\n",
    "    \n",
    "    if state == rnd_walk_state_idxs[\"E\"] and action == RandWalkAction.RIGHT:\n",
    "        return rnd_walk_state_idxs[TERMINAL], 1\n",
    "    \n",
    "    return state + 1, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rnd_td0(n_episode):\n",
    "    return tabular_td0(\n",
    "        policy=rnd_walk_policy,\n",
    "        init_state=rnd_init_state,\n",
    "        is_terminal=rnd_is_terminal,\n",
    "        simulate=rnd_simulate,\n",
    "        n_episode=n_episode,\n",
    "        alpha=0.1,\n",
    "        gamma=1.0,\n",
    "        init_fn=lambda s: 0.0 if s == rnd_walk_state_idxs[TERMINAL] else 0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = {\n",
    "    \"A\": 1/6,\n",
    "    \"B\": 2/6,\n",
    "    \"C\": 3/6,\n",
    "    \"D\": 4/6,\n",
    "    \"E\": 5/6\n",
    "}\n",
    "true_values = {rnd_walk_state_idxs[s]: v for s, v in true_values.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2341.88it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 35365.13it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 44197.09it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "to_plot = copy.deepcopy(rnd_walk_states)\n",
    "to_plot.remove(rnd_walk_state_idxs[TERMINAL])\n",
    "\n",
    "for n_episode in [0, 1, 10, 100]:\n",
    "    state_values = run_rnd_td0(n_episode)\n",
    "    plt.plot(\n",
    "        to_plot, \n",
    "        [state_values[s] for s in to_plot],\n",
    "        label=f\"{n_episode}\",\n",
    "    )\n",
    "    \n",
    "plt.plot(\n",
    "    list(true_values.keys()), \n",
    "    list(true_values.values()),\n",
    "    label=f\"True Values\",\n",
    ")\n",
    "    \n",
    "plt.ylim(0, 1)\n",
    "plt.xticks([1, 2, 3, 4, 5], labels=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimality of TD(0)\n",
    "\n",
    "*Batch Updating*: Given a small finite amount of experience, the increments specified by the update rules (e.g. Monte Carlo or TD) are summed and applied at the end of seeing all episodes rather than incrementally after each. \n",
    "\n",
    "Under batch updating, TD(0) converges deterministically to a single answer independent of the step-size parameter $\\alpha$ as long as $\\alpha$ is chosen to be sufficiently small. \n",
    "\n",
    "The constant-$\\alpha$ MC method also converges deterministically under the same conditions but to a different answer. \n",
    "\n",
    "Understanding these two conditions will help us understand the difference between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6.4: You are the Predictor\n",
    "#\n",
    "# Given the following eight episodes:\n",
    "#     A, 0, B, 0\n",
    "#     B, 1\n",
    "#     B, 1\n",
    "#     B, 1\n",
    "#     B, 1\n",
    "#     B, 1\n",
    "#     B, 1\n",
    "#     B, 0\n",
    "#\n",
    "# What are the values of states A and B?\n",
    "#\n",
    "# The optimal value for B, V(B), is 6/8=3/4 because 6 of the eight episodes the process terminates\n",
    "# in state B with a reward of 1.\n",
    "#\n",
    "# What about V(A)? Following the same logic we arrive at V(A)=0. A different viewpoint is to \n",
    "# say that when in state A we always transitioned to state B, and state B has value 3/4 \n",
    "# therefore state A has value V(A)=3/4\n",
    "#\n",
    "# The later is the answer TD(0) gives, the former is that MC methods give."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Monte Carlo methods always find the estimates that minimize the mean-squared error on the training set.\n",
    "\n",
    "Batch TD(0) always finds the estimates that would be exactly correct for the maximum-likelihood model of the Markov process. The _maximum-likelihood estimate_ of a parameter is a parameter whose probability of generating the dataset is greatest. In this case, the maximum-likelihood estimate is the model of the Markov process formed in the obvious way from the observed episodes: the estimated transition probabilities from i to j is the fraction of observed transitions from i that went to j, and the associated expected reward is the average of the rewards observed on those transitions.\n",
    "\n",
    "Given this model, we can compute the estimate of the value function that would be exactly correct if the model were exactly correct. This is called the **certainty-equivalence estimate** because it is equivalent assuming that the estimate of the underlying process was known with certainty rather than being approximated. \n",
    "\n",
    "This helps explain why TD methods converge more quickly than Monte Carlo methods. In batch form, TD(0) is faster than MC methods because it computes the true certainty-equivalence estimate. This may also help explain the speed advantage of nonbatch estimates, although they do not achieve either the certainty-equivalence or the minimum squared-error estimates but can be seen as moving in this direction.\n",
    "\n",
    "See: [Learning to Predict by the Methods of Temporal Differences](https://link.springer.com/content/pdf/10.1007%2FBF00115009.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Although certainty-equivalence estimate is in some sense optimal, it is almost never feasible to compute directly. If $n = |\\mathcal{S}|$ is the number of states, then just forming the maximum-likelihood estimate of the process may require on the order of $n^2$ memory and computing the corresponding value function requires on the order of $n^3$ computational steps if done conventionally. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa: On-policy TD Control\n",
    "\n",
    "Now to look at the control problem, following the same approach whereby we use generalized policy iteration (GPI). \n",
    "\n",
    "As per MC methods, we need to trade off exploration and exploitation. These approaches again fall into two main classes:\n",
    "\n",
    "1. On-policy methods: Evaluate or improve the policy that is used to generate the data.\n",
    "2. Off-policy methods: Evaluate or improve the policy that is different from the one used to generate the data.\n",
    "\n",
    "As per MC, the first step is to learn the action-value function to avoid requiring a model of the environment:\n",
    "\n",
    "\\begin{align}\n",
    "    Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [ R_{t + 1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\n",
    "\\end{align}\n",
    "\n",
    "This update rule uses every quintuple of events: $(S_t, A_t, R_{t + 1}, S_{t + 1}, A_{t + 1})$\n",
    "\n",
    "The quintuple gives rise to the name _Sarsa_ for the algorithm!\n",
    "\n",
    "#### On-policy Control\n",
    "\n",
    "As in all on-policy methods, we estimate $q_\\pi$ for the behavior policy and at the same time drive $\\pi$ towards greediness with respect to $q_\\pi$. \n",
    "\n",
    "Sarsa converges with probability 1 to an optimal policy and action-value function as long as all state-action pairs are visited an infinite number of times and the policy converges in the limit to the greedy policy (which can be arranged, for example, with $\\epsilon$-greedy policies by setting $\\epsilon = 1/t$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_td0(\n",
    "    policy: Mapping[StateT, Mapping[ActionT, float]],\n",
    "    init_state: Callable[[], StateT],\n",
    "    is_terminal: Mapping[StateT, bool],\n",
    "    simulate: Callable[[StateT, ActionT], Tuple[StateT, RewardT]],\n",
    "    n_episode: int,\n",
    "    alpha: float = 0.1,\n",
    "    gamma: float = 1.0,\n",
    "    init_fn: Optional[Callable[[StateT], float]] = None\n",
    ") -> Mapping[StateT, float]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_on_policy_td_control(\n",
    "    state_actions: Mapping[StateT, Sequence[ActionT]],\n",
    "    init_state: Callable[[], StateT],\n",
    "    is_terminal: Mapping[StateT, bool],\n",
    "    simulate: Callable[[StateT, ActionT], Tuple[StateT, RewardT]],\n",
    "    n_episode: int,\n",
    "    alpha: float,\n",
    "    epsilon: float,\n",
    "    gamma: float,\n",
    "    init_fn: Optional[Callable[[StateT, ActionT], float]] = None\n",
    ") -> Mapping[StateT, Mapping[ActionT, float]]:\n",
    "    action_values = {}\n",
    "    \n",
    "    for state, actions in state_actions.items():\n",
    "        action_values[state] = {}\n",
    "        for action in actions:\n",
    "            action_values[state][action] = init_fn(state, action) if init_fn is not None else 0.0\n",
    "        \n",
    "    for _ in tqdm.trange(n_episode):\n",
    "        state = init_state()\n",
    "        action = epsilon_greedy_sample(action_values[state], epsilon)\n",
    "        \n",
    "        while not is_terminal[state]:\n",
    "            next_state, reward = simulate(state, action)\n",
    "            next_action = epsilon_greedy_sample(action_values[next_state], epsilon)\n",
    "            action_values[state][action] += (\n",
    "                alpha * (\n",
    "                    reward + gamma*action_values[next_state][next_action] \n",
    "                    - action_values[state][action]\n",
    "                )\n",
    "            )\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "    \n",
    "    return action_values\n",
    "\n",
    "        \n",
    "def epsilon_greedy_sample(\n",
    "    values: Mapping[ActionT, float],\n",
    "    epsilon: float,\n",
    ") -> ActionT:\n",
    "    # epsilon percent of the time sample action randomly\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(list(values.keys()))\n",
    "    \n",
    "    # remaining (1.0 - epsilon) percent of time choose greedy action\n",
    "    greedy_act = None\n",
    "    greedy_val = -float(\"inf\")\n",
    "    for action, value in values.items():\n",
    "        if value > greedy_val:\n",
    "            greedy_act = action\n",
    "            greedy_val = value\n",
    "    return greedy_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify this appears to work by reconstructing the grid world state \n",
    "# values from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_state_actions = {state: list(GWAction) for state in gw_states}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:07<00:00, 140882.18it/s]\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.01\n",
    "\n",
    "action_values = sarsa_on_policy_td_control(\n",
    "    state_actions=gw_state_actions,\n",
    "    init_state=gw_init_state,\n",
    "    is_terminal=gw_is_terminal,\n",
    "    simulate=gw_simulate,\n",
    "    n_episode=int(1e6),\n",
    "    alpha=0.0001,\n",
    "    epsilon=epsilon,\n",
    "    gamma=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quiver_xs = list(range(4))\n",
    "quiver_ys = list(range(4))\n",
    "\n",
    "us = np.zeros((4, 4))\n",
    "vs = np.zeros((4, 4))\n",
    "\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        state = 4*row + col\n",
    "        if gw_is_terminal[state]:\n",
    "            continue\n",
    "            \n",
    "        act_vals = action_values[state]\n",
    "        greedy_act = max(act_vals.items(), key=lambda av: av[1])[0]\n",
    "        \n",
    "        if greedy_act == GWAction.LEFT:\n",
    "            us[row][col] = -1\n",
    "        elif greedy_act == GWAction.RIGHT:\n",
    "            us[row][col] = 1\n",
    "        elif greedy_act == GWAction.UP:\n",
    "            vs[row][col] = -1\n",
    "        elif greedy_act == GWAction.DOWN:\n",
    "            vs[row][col] = 1\n",
    "\n",
    "plt.quiver(quiver_xs, quiver_ys, us, vs, scale_units=\"xy\", scale=1.2)\n",
    "plt.title(f\"Grid World Sarsa Policy, $\\epsilon = {epsilon}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6.8: Show than an action-value version of (6.6) holds for the action-value \n",
    "# form of the TD error, again assuming that the values don't change from step to step.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

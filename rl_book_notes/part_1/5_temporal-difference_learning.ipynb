{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "from typing import Callable\n",
    "from typing import Mapping\n",
    "from typing import Sequence\n",
    "from typing import Tuple\n",
    "from typing import TypeVar\n",
    "\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Difference Learning\n",
    "\n",
    "## <p style=\"color:red\">Text content copied near verbatim from: <a href=\"http://incompleteideas.net/book/the-book-2nd.html\">Sutton and Barto</a>. Code is my own unless otherwise stated.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "- TD relation to Monte Carlo and DP methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "**TD(0) / _n-step_ TD**: \n",
    "\n",
    "**TD error**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Temporal-difference (TD) learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas.\n",
    "\n",
    "Like Monte Carlo methods, TD methods can directly learn from raw experience without a model of the environment's dynamics.\n",
    "\n",
    "Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they _bootstrap_).\n",
    "\n",
    "DP, TD, and Monte Carlo methods all use some form of generalized policy iteration (GPI) to solve the control problem. The differences in the methods are primarily due to differences in their approaches to the prediction problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Prediction\n",
    "\n",
    "Both TD and Monte Carlo methods use experience to solve the prediction problem. Given some experience following a policy $\\pi$, both methods update their estimate $V$ of $v_\\pi$ for the nonterminal states $S_t$ occurring in that experience. \n",
    "\n",
    "---\n",
    "\n",
    "Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V(S_t)$. A simple every-visit Monte Carlo method suitable for nonstationary environments is the _constant-$\\alpha$ MC_:\n",
    "\n",
    "\\begin{equation}\n",
    "    V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ G_t - V(S_t) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "where $G_t$ is the actual return following time $t$ and $\\alpha$ is a constant step-size parameter.\n",
    "\n",
    "Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_t)$ (only then is $G_t$ known).\n",
    "\n",
    "---\n",
    "\n",
    "TD methods only need to wait until the next time step to determine the increment. At time $t + 1$ they immediately form a target and make a useful update using the observed reward $R_{t + 1}$ and the estimate $V(S_{t + 1})$. The simplest TD method makes the update\n",
    "\n",
    "\\begin{equation}\n",
    "    V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "immediately on transition to $S_{t + 1}$ and receiving $R_{t + 1}$. \n",
    "\n",
    "---\n",
    "\n",
    "In effect, the target for the Monte Carlo update is $G_t$ whereas the target for the TD update is $R_{t + 1} + \\gamma V(S_{t + 1})$. \n",
    "\n",
    "This TD method is called $TD(0)$ or _one-step_ TD because it is a special case of the TD($\\lambda$) and _n-step_ TD methods developed in Chapters 12 and 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "StateT = TypeVar(\"StateT\")\n",
    "ActionT = TypeVar(\"ActionT\")\n",
    "RewardT = TypeVar(\"RewardT\", int, float)\n",
    "\n",
    "TraceT = Sequence[Tuple[StateT, ActionT, RewardT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_td0(\n",
    "    policy: Mapping[StateT, Mapping[ActionT, float]],\n",
    "    init_state: Callable[[], StateT],\n",
    "    is_terminal: Mapping[StateT, bool],\n",
    "    simulate: Callable[[StateT, ActionT], Tuple[StateT, RewardT]],\n",
    "    n_episode: int,\n",
    "    alpha: float = 0.1,\n",
    "    gamma: float = 1.0\n",
    ") -> None:\n",
    "    state_values = {}\n",
    "    \n",
    "    for state in policy.keys():\n",
    "        state_values[state] = 0\n",
    "        \n",
    "    for _ in tqdm.trange(n_episode):\n",
    "        state = init_state()\n",
    "        \n",
    "        while not is_terminal[state]:\n",
    "            action = random.choices(\n",
    "                population=list(policy[state].keys()), \n",
    "                weights=policy[state].values()\n",
    "            )[0]   # random.choices returns a 1-element list\n",
    "            next_state, reward = simulate(state, action)\n",
    "            \n",
    "            state_values[state] += alpha*(reward + gamma*state_values[next_state] - state_values[state])\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Confirming this works on the simple grid-world environment from Example 4.1 in Chapter 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_states: Sequence[int] = list(range(4*4))   # 4x4 grid, states numbered left to right top to bottom \n",
    "\n",
    "    \n",
    "class GWAction(enum.Enum):\n",
    "    UP    = enum.auto()\n",
    "    DOWN  = enum.auto()\n",
    "    RIGHT = enum.auto()\n",
    "    LEFT  = enum.auto()\n",
    "\n",
    "    \n",
    "def gw_init_state() -> int:\n",
    "    return random.choice(gw_states)\n",
    "\n",
    "\n",
    "gw_is_terminal: Mapping[int, bool] = {\n",
    "    state: state == 0 or state == 15\n",
    "    for state in gw_states\n",
    "}\n",
    "\n",
    "\n",
    "def gw_simulate(state: int, action: GWAction) -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "        \n",
    "    \"\"\"\n",
    "    if action == GWAction.LEFT:\n",
    "        if state % 4 == 0:\n",
    "            #  unable to move further left\n",
    "            return state, -1\n",
    "        return state - 1, -1\n",
    "    \n",
    "    if action == GWAction.RIGHT:\n",
    "        if (state + 1) % 4 == 0:\n",
    "            # unable to move further right\n",
    "            return state, -1\n",
    "        return state + 1, -1\n",
    "    \n",
    "    if action == GWAction.UP:\n",
    "        if state < 4:\n",
    "            return state, -1\n",
    "        return state - 4, -1\n",
    "    \n",
    "    if action == GWAction.DOWN:\n",
    "        if state > 11:\n",
    "            return state, -1\n",
    "        return (state + 4) % 15, -1 \n",
    "\n",
    "    raise ValueError(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_equiprob_policy = {}\n",
    "for state in gw_states:\n",
    "    gw_equiprob_policy[state] = {}\n",
    "    for action in GWAction:\n",
    "        gw_equiprob_policy[state][action] = 1.0 / len(GWAction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:45<00:00, 22136.10it/s]\n"
     ]
    }
   ],
   "source": [
    "state_values = tabular_td0(\n",
    "    policy=gw_equiprob_policy,\n",
    "    init_state=gw_init_state,\n",
    "    is_terminal=gw_is_terminal,\n",
    "    simulate=gw_simulate,\n",
    "    n_episode=int(1e6),\n",
    "    alpha=0.0001,\n",
    "    gamma=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000 -14.090 -20.001 -21.981 \n",
      "-13.972 -18.012 -19.994 -19.978 \n",
      "-19.946 -19.995 -17.980 -14.001 \n",
      "-21.984 -20.044 -14.042   0.000 \n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        print(f\"{state_values[i*4 + j]: 7.3f}\", end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the actual state-value function for this problem is:\n",
    "\n",
    "```\n",
    "| 0.0 | -14 | -20 | -22 |\n",
    "| -14 | -18 | -20 | -20 |\n",
    "| -20 | -20 | -18 | -14 |\n",
    "| -22 | -20 | -14 | 0.0 |\n",
    "```\n",
    "\n",
    "so we see that the method works!\n",
    "\n",
    "Notice that $\\alpha$ must be small for the computed values to get close to their true value.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD(0) bases its update in part on an existing estimate so it is a _bootstrapping_ method like DP. We know from Chapter 3 that:\n",
    "\n",
    "\\begin{align}\n",
    "    v_\\pi(s) &\\doteq \\mathbb{E}_\\pi \\left[ G_t | S_t = s \\right] \\\\\n",
    "             &= \\mathbb{E}_\\pi \\left[ R_{t + 1} + \\gamma G_{t + 1} | S_t = s \\right] \\\\\n",
    "             &= \\mathbb{E}_\\pi \\left[ R_{t + 1} + \\gamma v_\\pi(S_{t + 1}) | S_t = s \\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo methods use an estimate of $\\mathbb{E}_\\pi[G_t | S_t = s]$ as the target. It is an estimate as the expected value is not known; a sample return is used in place of the real expected return.\n",
    "\n",
    "DP methods use an estimate of $\\mathbb{E}_\\pi[R_{t + 1} + \\gamma v_\\pi(S_{t + 1}) | S_t = s]$ as the target. It is an estimate not because of the expected values, which are assumed to be completely provided by a model of the environment, but because $v_\\pi(S_{t + 1})$ is not known and the current estimate $V(S_{t + 1})$ is used instead.\n",
    "\n",
    "The TD target is an estimate for both reasons:\n",
    "\n",
    "1. It samples the expected values.\n",
    "2. It uses the current estimate $V$ instead of the true $v_\\pi$.\n",
    "\n",
    "**TD methods combine the sampling of Monte Carlo with the bootstrapping of DP.**\n",
    "\n",
    "---\n",
    "\n",
    "Note that in the update rule for TD the quantity in brackets, $\\left[ R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t) \\right]$, is a sort of error measuring the difference between the estimated value of $S_t$ and the better estimate $R_{t + 1} + \\gamma V(S_{t + 1})$. This is called the **TD error** and arises in various forms throughout reinforcement learning:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_t \\doteq R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t)\n",
    "\\end{equation}\n",
    "\n",
    "This quantity depends on the next state and reward so is not available until one time step later. If the array V does not change during the episode (e.g. Monte Carlo methods) then the Monte Carlo error can be written as a sum of TD errors:\n",
    "\n",
    "\\begin{align}\n",
    "    G_t - V(S_t) &= R_{t + 1} + \\gamma G_{t + 1} - V(S_t) \\\\\n",
    "                 &= R_{t + 1} + \\gamma G_{t + 1} - V(S_t) + \\gamma V(S_{t + 1}) - \\gamma V(S_{t + 1}) \\\\\n",
    "                 &= \\delta_t + \\gamma \\delta_{t + 1} + \\gamma^2(G_{t + 2} - V(S_{t + 2})) \\\\\n",
    "                 &= \\delta_t + \\gamma \\delta_{t + 1} + \\gamma^2\\delta_{t + 2} + \\dots + \\gamma^{T - t - 1} \\delta_{T - 1} + \\gamma^{T - t}(0 - 0) \\\\\n",
    "                 &= \\sum_{k = t}^{T - 1} \\gamma^{k - t} \\delta_k\n",
    "\\end{align}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 6.1: If V changes during the episode then the above only holds approximately; what would the difference be between the two sides? Let $V_t$ denote the array of state values used at time $t$ in the TD error and in the TD update. Redo the derivation above to determine the additional amount that must be added to the sum of TD errors in order to equal the Monte Carlo error.\n",
    "\n",
    "\\begin{align}\n",
    "    V_{t + 1}(S_t) &= V_t(S_t) + \\alpha [R_{t + 1} + \\gamma V_t(S_{t + 1}) - V_t(S_t)] \\\\\n",
    "                   &= V_t(S_t) + \\alpha [R_{t + 1} + \\gamma V_t(S_{t + 1}) - V_t(S_t)] \\\\\n",
    "                   &= V_t(S_t) + \\alpha \\delta_t \\\\\n",
    "\\end{align}\n",
    "\n",
    "Rearranging for $V_t(S_t)$\n",
    "\n",
    "\\begin{align}\n",
    "    V_t(S_t) &= V_{t + 1}(S_t) - \\alpha \\delta_t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following through with the proof similar to the steps in (6.6):\n",
    "\n",
    "\\begin{align}\n",
    "    G_t - V_t(S_t) &\\doteq R_{t + 1} + \\gamma G_{t + 1} - V_t(S_t) \\\\\n",
    "                   &=      R_{t + 1} + \\gamma V_t(S_{t + 1}) - V_t(S_t) + \\gamma G_{t + 1} - \\gamma V_t(S_{t + 1}) \\\\\n",
    "                   &= \\delta_t + \\gamma(G_{t + 1} - V_t(S_{t + 1})) \\\\\n",
    "                   &= \\delta_t + \\gamma(G_{t + 1} - V_{t + 1}(S_{t + 1}) + \\alpha \\delta_t) \\\\\n",
    "                   &= \\delta_t + \\gamma \\alpha \\delta_t + \\gamma(G_{t + 1} - V_{t + 1}(S_{t + 1})) \\\\\n",
    "                   &= \\delta_t(1 + \\gamma \\alpha) + \\gamma(G_{t + 1} - V_{t + 1}(S_{t + 1})) \\\\\n",
    "                   &= \\delta_t(1 + \\gamma \\alpha) + \\gamma(\\delta_{t + 1}(1 + \\gamma \\alpha) + \\gamma(G_{t + 2} - V_{t + 2}(S_{t + 2}))) \\\\\n",
    "                   &= \\delta_t(1 + \\gamma \\alpha) + \\gamma(\\delta_{t + 1}(1 + \\gamma \\alpha)) + \\gamma(G_{t + 2} - V_{t + 2}(S_{t + 2})) \\\\\n",
    "                   &= (1 + \\gamma \\alpha)(\\delta_t + \\gamma\\delta_{t + 1}) + \\gamma^2(\\delta_{t + 2}(1 + \\gamma \\alpha) + \\gamma(G_{t + 3} - V_{t + 3}(S_{t + 3}))) \\\\\n",
    "                   &= (1 + \\gamma \\alpha)(\\delta_t + \\gamma\\delta_{t + 1}) + \\gamma^2\\delta_{t + 2}(1 + \\gamma \\alpha) + \\gamma^3(G_{t + 3} - V_{t + 3}(S_{t + 3})) \\\\\n",
    "                   &= (1 + \\gamma \\alpha)(\\delta_t + \\gamma\\delta_{t + 1} + \\gamma^2\\delta_{t + 2}) + \\gamma^3(G_{t + 3} - V_{t + 3}(S_{t + 3})) \\\\\n",
    "                   &= (1 + \\gamma \\alpha)\\sum_{k=t}^{T - 1}\\gamma^{k - t} \\delta_t \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively this makes sense.\n",
    "\n",
    "To see why, first note that the original equation _without_ the time subscript telescopes to produce the sum of discounted rewards equation. This is a convoluted way of getting from the RHS back to the LHS :-)\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_t \\doteq R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    G_t - V(S_t) &= \\sum_{k = t}^{T - 1} \\gamma^{k - t} \\delta_k \\\\\n",
    "                 &= R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t) + \\gamma ( R_{t + 2} + \\gamma V(S_{t + 2}) - V(S_{t + 1})) + \\sum_{k = t + 2}^{T - 1} \\gamma^{k - t} \\delta_k \\\\\n",
    "                 &= R_{t + 1} + \\gamma V(S_{t + 1}) - V(S_t) + \\gamma R_{t + 2} + \\gamma^2 V(S_{t + 2}) - \\gamma  V(S_{t + 1}) + \\sum_{k = t + 2}^{T - 1} \\gamma^{k - t} \\delta_k \\\\\n",
    "                 &= R_{t + 1} + \\gamma R_{t + 2} - V(S_t) + \\gamma^2 V(S_{t + 2})+ \\sum_{k = t + 2}^{T - 1} \\gamma^{k - t} \\delta_k \\\\\n",
    "                 &= \\left[\\sum_{k = t + 1}^{T} \\gamma^{k - t - 1}R_{k}\\right] - V(S_t)\n",
    "\\end{align}\n",
    "\n",
    "Now, from the above we also know:\n",
    "\n",
    "\\begin{equation}\n",
    "    V_{t + 1}(S_t) - V_t(S_t) = \\alpha \\delta_t\n",
    "\\end{equation}\n",
    "\n",
    "Distributing the summation gives:\n",
    "\n",
    "\\begin{align}\n",
    "    G_t - V_t(S_t) &= (1 + \\gamma \\alpha) \\sum_{k = t}^{T - 1} \\gamma^{k - t}\\delta_t \\\\\n",
    "                   &= \\sum_{k = t}^{T - 1} \\gamma^{k - t}\\delta_t + \\gamma \\alpha \\sum_{k = t}^{T - 1} \\gamma^{k - t}\\delta_t \\\\\n",
    "                   &= \\sum_{k = t}^{T - 1} \\gamma^{k - t}\\delta_t + \\sum_{k = t}^{T - 1} \\gamma^{k - t + 1} \\alpha \\delta_t \\\\\n",
    "                   &= \\sum_{k = t}^{T - 1} \\gamma^{k - t} \\delta_t + \\gamma^{k - t + 1} \\alpha \\delta_t\n",
    "\\end{align}\n",
    "\n",
    "It is the original sum of discounted TD errors plus another term. This other term accounts for the sum of discounted differences/deltas between the state-value estimate at step $t + 1$ and at step $t$. i.e. it is the difference, $\\alpha \\delta_k$, weighted by the discounting factor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

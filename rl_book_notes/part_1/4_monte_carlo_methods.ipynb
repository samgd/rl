{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import enum\n",
    "import multiprocessing\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from itertools import product\n",
    "from typing import Callable\n",
    "from typing import Iterable\n",
    "from typing import Mapping\n",
    "from typing import Optional\n",
    "from typing import Sequence\n",
    "from typing import Tuple\n",
    "from typing import TypeVar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # doctest to catch regressions, copy below relevant code section\n",
    "# import doctest\n",
    "# doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods\n",
    "\n",
    "## <p style=\"color:red\">Text content copied near verbatim from: <a href=\"http://incompleteideas.net/book/the-book-2nd.html\">Sutton and Barto</a>. Code is my own unless otherwise stated.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "**Monte Carlo (MC) Methods**: \n",
    "- Generic definition: Repeated random sampling to obtain numerical results. \n",
    "- RL definition: A way of solving the RL problem based on averaging sample returns.\n",
    "\n",
    "**Control Problem** (Reminder?): Approximate optimal policies (**through GPI?**).\n",
    "\n",
    "**First-visit MC Method**:\n",
    "\n",
    "**Every-visit MC Method**:\n",
    "\n",
    "**Maintaining Exploration**: \n",
    "\n",
    "**Exploring Starts**:\n",
    "\n",
    "**Monte Carlo ES**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo methods only require experience - sample sequences of states, actions, and rewards from actual or simulated interaction with an environment. They do _not_ require complete knowledge of the environment dynamics. \n",
    "\n",
    "For simulated learning a model is required but it only needs to generate sample transitions and not the complete probability distribution of all possible transitions. There are a surprising number of places where it is easy to generate experience sampled according to the desired probability distribution but infeasible to obtain the distributions in explicit form.\n",
    "\n",
    "Monte Carlo methods solve the RL problem by averaging sample returns. Here we only consider Monte Carlo methods for episodic tasks. Only on completion of an episode are value estimates and policies changed. This means they can be used incrementally in an episode-by-episode nature but not in an online step-by-step way. \n",
    "\n",
    "In the previous notebook we looked at dynamic programming which _computed_ value functions from knowledge of the MDP. Monte Carlo methods here _learn_ value functions from sample returns with the MDP. The value functions and corresponding policies still interact to obtain optimality via general policy iteration. \n",
    "\n",
    "As per the previous chapter, we start by looking at the prediction problem, then the control problem and its solution by GPI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1: Monte Carlo Prediction\n",
    "\n",
    "The \"obvious\" way to learn the state-value function - expected cumulative sum of future discounted reward - for a given policy is to average the returns observed after visits to a state. As more returns are observed, the average should converge to the expected value.\n",
    "\n",
    "To estimate $v_\\pi(s)$ we define:\n",
    "\n",
    "- _visit_ to $s$: each occurrence of state $s$ in an episode\n",
    "- _first visit_: the first time s is visited in an episode\n",
    "\n",
    "This leads to the following MC methods:\n",
    "\n",
    "- _first visit MC method_: estimates $v_\\pi(s)$ as the average returns following first visits to $s$\n",
    "- _every visit MC method_: averages returns following _all_ visits to $s$\n",
    "\n",
    "These are similar but have different theoretical properties.\n",
    "\n",
    "This chapter looks at first-visit MC. Every-visit MC extends more naturally to function approximation and eligibility tracing (ch 9/12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "StateT = TypeVar(\"StateT\")\n",
    "ActionT = TypeVar(\"ActionT\")\n",
    "RewardT = TypeVar(\"RewardT\", int, float)\n",
    "\n",
    "TraceT = Sequence[Tuple[StateT, ActionT, RewardT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc(\n",
    "    states: Sequence[StateT],\n",
    "    episodes: Iterable[TraceT],\n",
    "    gamma: float = 0.9\n",
    ") -> Mapping[StateT, RewardT]:\n",
    "    \"\"\"\n",
    "    \n",
    "    Example:\n",
    "        >>> def gen_ep(n_episodes):\n",
    "        ...     for _ in range(n_episodes):\n",
    "        ...         yield [(0, 0, 0.5), (0, 0, 0.75), (1, 0, 1.0), (0, 0, 0.75)]\n",
    "        >>> value_map = first_visit_mc(states=[0, 1], episodes=gen_ep(1), gamma=0.5)\n",
    "        >>> value_map[0] == 1.21875\n",
    "        True\n",
    "        >>> value_map[1] == 1.375\n",
    "        True\n",
    "    \"\"\"\n",
    "    visits = defaultdict(int)\n",
    "    values = dict((s, 0.0) for s in states)\n",
    "    \n",
    "    for trace in tqdm.tqdm(episodes):\n",
    "        ret = 0.0\n",
    "        fv = first_visits(trace)\n",
    "        for first_visit, (state, _, reward) in reversed(list(zip(fv, trace))):\n",
    "            ret = reward + gamma*ret\n",
    "            \n",
    "            if first_visit:\n",
    "                visits[state] += 1\n",
    "                values[state] += (1.0 / visits[state]) * (ret - values[state])\n",
    "                \n",
    "    return values\n",
    "\n",
    "\n",
    "def first_visits(trace: TraceT, state_action_pairs: bool = False) -> Sequence[bool]:\n",
    "    \"\"\"\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "        >>> first_visits([(0, 0, 0.5)])\n",
    "        [True]\n",
    "        >>> first_visits([(0, 0, 0.5), (1, 0, 0.1), (0, 0, 0.5)])\n",
    "        [True, True, False]\n",
    "    \"\"\"\n",
    "    visited = set()\n",
    "    fv = []\n",
    "    for s, a, _ in trace:\n",
    "        if state_action_pairs:\n",
    "            fv.append((s, a) not in visited)\n",
    "            visited.add((s, a))\n",
    "        else:\n",
    "            fv.append(s not in visited)\n",
    "            visited.add(s)\n",
    "        \n",
    "    return fv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a list containing the returns for each first-visit could be stored for each state, similar to how a list of rewards were stored when computing bandit problems.\n",
    "\n",
    "The above implementation avoids this by using the same trick employed in the bandit notebook: it derives an update equation.\n",
    "\n",
    "Recall that to compute the $k + 1$th estimate of $q$ in update-equation form where $G_i$ is the return received during the $i$th first visit to state-action $(s, a)$:\n",
    "\n",
    "\\begin{align}\n",
    "q_{k + 1}(s, a) &= \\frac{1}{n} \\sum_{i=1}^{n} G_i \\\\\n",
    "                &= \\frac{1}{n} \\left[G_{n} + \\sum_{i=1}^{n - 1} G_k \\right] \\\\\n",
    "                &= \\frac{1}{n} \\left[G_{n} + (n - 1) \\frac{1}{n-1}\\sum_{i=1}^{n-1} G_k \\right] \\\\\n",
    "                &= \\frac{1}{n} \\left[G_{n} + (n - 1) q_k(s, a) \\right] \\\\\n",
    "                &= \\frac{1}{n} \\left[G_{n} + n q_k(s, a) - q_k(s, a) \\right] \\\\\n",
    "                &= q_k(s, a) + \\frac{1}{n} \\left[G_{n} - q_k(s, a) \\right] \\\\\n",
    "\\end{align}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both first-visit MC and every-visit MC converge to $v_\\pi(s)$ as the number of visits (or first visits) to $s$ goes to infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5.1: Blackjack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BJAction(enum.Enum):\n",
    "    STICK = 0\n",
    "    HIT = 1\n",
    "    \n",
    "    \n",
    "class BJCard(enum.IntEnum):\n",
    "    ACE   = 11\n",
    "    TWO   = 2\n",
    "    THREE = 3\n",
    "    FOUR  = 4\n",
    "    FIVE  = 5\n",
    "    SIX   = 6\n",
    "    SEVEN = 7\n",
    "    EIGHT = 8\n",
    "    NINE  = 9\n",
    "    TEN   = 10\n",
    "    \n",
    "    @staticmethod\n",
    "    def _prob(card) -> float:\n",
    "        if card == BJCard.TEN:\n",
    "            return 4.0 / 13\n",
    "        return 1.0 / 13\n",
    "\n",
    "    @staticmethod\n",
    "    def hit(n_cards=None):\n",
    "        return np.random.choice(BJCard, size=n_cards, p=[BJCard._prob(c) for c in BJCard]).tolist()\n",
    "\n",
    "    \n",
    "@dataclass\n",
    "class Hand:\n",
    "    value: int\n",
    "    usable_ace: bool\n",
    "        \n",
    "    def __init__(self, cards):\n",
    "        self.value = 0\n",
    "        self.usable_ace = False\n",
    "        for card in cards:\n",
    "            if card != BJCard.ACE:\n",
    "                self.value += card\n",
    "            else:\n",
    "                if self.usable_ace:\n",
    "                    # cannot have more than one usable ace (11 + 11 > 21)\n",
    "                    self.value += 1\n",
    "                else:\n",
    "                    # attempt to use max value, correct later if > 21\n",
    "                    self.value += 11\n",
    "                    self.usable_ace = True\n",
    "\n",
    "            if self.value > 21 and self.usable_ace:\n",
    "                self.value -= 10\n",
    "                self.usable_ace = False\n",
    "        \n",
    "    def add(self, card):\n",
    "        self.value += card\n",
    "        if self.usable_ace and self.value > 21:\n",
    "            self.value -= 10\n",
    "            self.usable_ace = False        \n",
    "        return self\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.value) + hash(self.usable_ace)\n",
    "\n",
    "    \n",
    "@dataclass\n",
    "class BJState:\n",
    "    player: Hand\n",
    "    dealer: Hand\n",
    "        \n",
    "    @staticmethod\n",
    "    def new():\n",
    "        cards = BJCard.hit(3)\n",
    "        player = Hand(cards[:2])\n",
    "        dealer = Hand(cards[2:])\n",
    "        return BJState(player, dealer)\n",
    "    \n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.player) + hash(self.dealer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bj_states() -> Sequence[BJState]:\n",
    "    states = []\n",
    "    for player_value in range(4, 21 + 1):\n",
    "        for usable_ace in [False, True]:\n",
    "            for dealer_value in range(2, 11 + 1):\n",
    "                player = Hand([])\n",
    "                player.value = player_value\n",
    "                player.usable_ace = usable_ace\n",
    "                \n",
    "                dealer = Hand([])\n",
    "                dealer.value = dealer_value\n",
    "                dealer.usable_ace = dealer_value == 11\n",
    "                \n",
    "                states.append(BJState(player, dealer))\n",
    "    return states\n",
    "\n",
    "\n",
    "def bj_state_actions() -> Mapping[BJState, Sequence[BJAction]]:\n",
    "    state_actions = {}\n",
    "    for state in bj_states():\n",
    "        state_actions[state] = []\n",
    "        for action in BJAction:\n",
    "            state_actions[state].append(action)\n",
    "    return state_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stick_20_21(state: BJState, action: BJAction) -> float:\n",
    "    if state.player.value in {20, 21}:\n",
    "        if action == BJAction.STICK:\n",
    "            return 1.0\n",
    "        return 0.0\n",
    "    if action == BJAction.HIT:\n",
    "        return 1.0\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bj_simulate(\n",
    "    policy: Callable[[BJState, BJAction], float]\n",
    ") -> Sequence[Tuple[BJState, BJAction, int]]:\n",
    "    \"\"\"...\"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    state = BJState.new()\n",
    "    \n",
    "    # simulate player until sticks or goes bust\n",
    "    while True:\n",
    "        states.append(state.copy())\n",
    "        \n",
    "        action = np.random.choice(BJAction, p=[policy(state, a) for a in BJAction])\n",
    "        actions.append(action)\n",
    "        \n",
    "        if action == BJAction.HIT:\n",
    "            state.player.add(BJCard.hit())\n",
    "        \n",
    "        if action == BJAction.STICK or state.player.value > 21:\n",
    "            break\n",
    "            \n",
    "        rewards.append(0)   # 0 whilst player still playing\n",
    "    \n",
    "    # compute final reward\n",
    "    if state.player.value > 21:\n",
    "        # player bust\n",
    "        reward = -1\n",
    "    else:\n",
    "        # player not bust, compute dealer value using fixed policy\n",
    "        while state.dealer.value < 17:\n",
    "            state.dealer.add(BJCard.hit())\n",
    "        if state.dealer.value > 21:\n",
    "            # dealer bust\n",
    "            reward = 1\n",
    "        else:\n",
    "            if state.player.value == state.dealer.value:\n",
    "                reward = 0\n",
    "            elif state.player.value > state.dealer.value:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "           \n",
    "    states.append(state.copy())\n",
    "    actions.append(None)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    return list(zip(states, actions, rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(f, q_in, q_out, seed):\n",
    "    # force different seed per process to avoid \n",
    "    # processes simulating the same episodes\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    to_gen = q_in.get()\n",
    "    for _ in range(to_gen):\n",
    "        q_out.put(f())\n",
    "        \n",
    "\n",
    "def bj_simulate_n(policy, n_episodes, n_procs=10):\n",
    "    assert n_procs > 0\n",
    "    if n_procs == 1:\n",
    "        for _ in range(n_episodes):\n",
    "            yield bj_simulate(policy)\n",
    "        return\n",
    "        \n",
    "    q_in = multiprocessing.Queue()\n",
    "    q_out = multiprocessing.Queue()\n",
    "\n",
    "    proc = [\n",
    "        multiprocessing.Process(\n",
    "            target=generate, \n",
    "            args=(lambda: bj_simulate(policy), q_in, q_out, seed)\n",
    "        )\n",
    "        for seed in range(n_procs)\n",
    "    ]\n",
    "    \n",
    "    for p in proc:\n",
    "        p.daemon = True\n",
    "        p.start()\n",
    "        \n",
    "    for pid in range(n_procs):\n",
    "        to_proc = n_episodes // n_procs\n",
    "        if pid == n_procs - 1:\n",
    "            to_proc += n_episodes % n_procs\n",
    "        q_in.put(to_proc)\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        yield q_out.get()\n",
    "\n",
    "    [p.join() for p in proc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(n_procs, n_episodes):\n",
    "    state_values = first_visit_mc(\n",
    "        states=bj_states(),\n",
    "        episodes=bj_simulate_n(stick_20_21, n_episodes, n_procs=n_procs),\n",
    "        gamma=1.0\n",
    "    )\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:01, 5993.13it/s]\n",
      "500000it [01:22, 6079.29it/s] \n"
     ]
    }
   ],
   "source": [
    "state_values = {}\n",
    "for n_episodes in [10000, 500000]:\n",
    "    state_values[n_episodes] = run(n_procs=6, n_episodes=n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f03673010c048a2b5d8418eb4d23475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close(\"all\")\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i, n_episodes in enumerate(sorted(state_values.keys())):\n",
    "    for j, usable_ace in enumerate([True, False]):\n",
    "        values = np.empty((10, 10))\n",
    "\n",
    "        for state, value in state_values[n_episodes].items():\n",
    "            if state.player.usable_ace != usable_ace:\n",
    "                continue\n",
    "            values[ state.player.value - 12, state.dealer.value - 2] = value\n",
    "\n",
    "        X, Y = np.meshgrid(range(2, 11 + 1), range(12, 21 + 1))\n",
    "        xs = X.flatten()\n",
    "        ys = Y.flatten()\n",
    "        zs = values.flatten()\n",
    "\n",
    "        ax = fig.add_subplot(2, 2, i + j*2 + 1, projection=\"3d\")\n",
    "        surf = ax.plot_trisurf(xs, ys, zs)\n",
    "        ax.set_zlim3d(-1, 1)\n",
    "        \n",
    "        ax.set_xlabel(\"Dealer showing\")\n",
    "        ax.set_ylabel(\"Player sum\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Exercise 5.1: The value function jumps up for the last two rows as the policy is to stick at a value of 20 or 21 and the probability of the player winning when in either of these states is high. It drops off for the far right row (left in the book's figures) as the dealer has an ace (valued 11). An ace combined with a 10 means the player cannot and the probability of drawing a card with a value of 10 is relatively high (16/52). The frontmost values are higher when there is a usable ace as the player has less of a chance of going bust - if the player draws a high card to go bust in the lower diagrams they lose but in the upper diagrams the ace takes the value of 1 rather than 10 allowing the player to continue.\n",
    "\n",
    "Exercise 5.2: The results will be identical as no state is visited twice in an episode of black jack (assuming \"usable ace\" is part of the state).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backup Diagrams for MC Methods\n",
    "\n",
    "The general idea of backup diagrams is to show at the top the root node to be updated and to show below all the transitions and leaf nodes whose rewards and estimated values contribute to the update. \n",
    "\n",
    "For MC estimation of $v_\\pi$ the root node is a state node and below is the trace for a single episode. \n",
    "\n",
    "This is in contrast to DP algorithms that include only one-step transitions.\n",
    "\n",
    "Another difference is that for MC methods the estimate of each state is independent - one estimate does not build upon another estimate. MC methods do not bootstrap.\n",
    "\n",
    "Estimation of a state's value using MC methods is therefore independent of all other states. This allows only the state space of interest to be explored (which may be significantly smaller than the entire space). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Estimation of Action Values\n",
    "\n",
    "If a model is not available then it is useful to estimate _action_ values rather than _state_ values.\n",
    "\n",
    "With a model all that is required are state values as you can look ahead to find which actions lead to the best reward and next states. \n",
    "\n",
    "Without a model you are unable to see the distribution of rewards and next states given an action meaning the action-value function is required.\n",
    "\n",
    "To best exploit Monte Carlo methods - which do not need to know the dynamics (model) - we will look at estimating the optimal action value function $q_*$. \n",
    "\n",
    "---\n",
    "\n",
    "Recall the policy evaluation problem for action values: estimate $q_\\pi(s, a)$ (the expected return when in state $s$ taking action $a$ and thereafter following policy $\\pi$). \n",
    "\n",
    "The method will essentially be the same as in the previous section but now recording state-action pairs. A state-action pair is visited in an episode if the action $a$ is taken in state $s$. \n",
    "\n",
    "- _every visit MC method_ for action-values: estimates by averaging all returns that have followed visits to it.\n",
    "- _first visit MC method_ for action-values: estimates by averaging all returns that have followed from first visits to it. \n",
    "\n",
    "These methods **converge quadratically to the true expected values as the number of visits to each state-action pair approaches infinity**.\n",
    "\n",
    "---\n",
    "\n",
    "Problem: many state-action pairs will never be visited.\n",
    "\n",
    "If $\\pi$ is a deterministic policy, then returns will only be observed for one action for each state. This means all other action values will not improve with experience. The use of policy evaluation is for control and hence it is important to learn action values! \n",
    "\n",
    "Solution: we need to estimate the value of _all_ actions from each state, not just the one we currently favor.\n",
    "\n",
    "This is the general problem of _maintaining exploration_. For policy evaluation to work for action values, we must assure continual exploration. \n",
    "\n",
    "More concrete solutions:\n",
    "\n",
    "1. *Exploring starts*: Specify that the episodes start in a state-action pair and guarantee that every pair has a nonzero probability of being selected as the start.\n",
    "2. Consider only policies that are stochastic with a nonzero probability of selecting all actions in each state. \n",
    "\n",
    "For now the sections assume exploring starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Control\n",
    "\n",
    "To approximate optimal policies we proceed as in the previous chapter on DPs: use generalized policy iteration (GPI).\n",
    "\n",
    "Recall that in GPI you maintain an approximate policy and approximate value function. The value function is repeatedly altered to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to the current value function. \n",
    "\n",
    "These two changes appear to work against each other but together they cause the policy and value function to approach optimality (policy improvement theorem).\n",
    "\n",
    "Assuming an infinite number of episodes are observed using exploring starts the Monte Carlo policy evaluation methods will compute each $q_{\\pi_k}$ for the policy $\\pi_k$. \n",
    "\n",
    "Policy improvement is done by making the policy greedy with respect to the current value function:\n",
    "\n",
    "\\begin{align}\n",
    "    \\pi(s) \\doteq \\arg \\max_a q(s, a)\n",
    "\\end{align}\n",
    "\n",
    "Policy improvement theorem applies to $\\pi_k$ and $\\pi_{k + 1}$ because, for all $s \\in S$:\n",
    "\n",
    "\\begin{align}\n",
    "    q_{\\pi_k}(s, \\pi_{k + 1}(s)) &= q_{\\pi_k}(s, \\arg \\max_a q_{\\pi_k}(s, a)) \\\\\n",
    "                                 &= \\max_a q_{\\pi_k}(s, {\\pi_k}(s, a)) \\\\\n",
    "                                 &\\geq q_{\\pi_k}(s, \\pi_k(s)) \\\\\n",
    "                                 &\\geq v_k(s)\n",
    "\\end{align}\n",
    "\n",
    "This shows that each $\\pi_{k + 1}$ is uniformly better than $\\pi_k$ or just as good in which case they are both optimal policies. \n",
    "\n",
    "---\n",
    "\n",
    "The above shows that Monte Carlo methods can be used to find optimal policies given only sample episodes and no other knowledge of the environment's dynamics.\n",
    "\n",
    "Two assumptions were made in order to obtain this guarantee of convergence:\n",
    "\n",
    "1. Episodes have exploring starts.\n",
    "2. Policy evaluation could be done with an infinite number of episodes.\n",
    "\n",
    "To obtain a practically useful algorithm both assumptions need to be removed.\n",
    "\n",
    "---\n",
    "\n",
    "Removing assumption 2: policy evaluation is done with an infinite number of episodes.\n",
    "\n",
    "The first approach is to make measurements and assumptions and bound the on the magnitude and probability of error and then to ensure sufficient steps are taken each iteration to make sure they are small. This is likely to require too many episodes to be useful in practice. \n",
    "\n",
    "A second approach is to avoid the infinite number of episodes required for policy evaluation. As in the previous chapter when using value iteration or in-place value iteration that alternates single steps of policy evaluation and policy improvement for each state.\n",
    "\n",
    "The second approach is taken for Monte Carlo: policy iteration switches between policy evaluation and policy improvement on an episode-by-episode basis. This algorithm is called **Monte Carlo ES** where ES means exploring starts.\n",
    "\n",
    "Note that this is very similar to `first_visit_mc` but it now includes a policy iteration step and state-action pairs rather than just states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_es(\n",
    "    state_actions: Mapping[StateT, Sequence[ActionT]],\n",
    "    episodes: Iterable[TraceT],\n",
    "    gamma: float = 0.9\n",
    "    policy: Optional[Mapping[Tuple[StateT, ActionT], float]] = None\n",
    ") -> Tuple[Mapping[StateT, RewardT], Mapping[Tuple[StateT, ActionT], float]]:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    if policy is None:\n",
    "        policy = random_policy(state_actions)\n",
    "    \n",
    "    visits = defaultdict(int)\n",
    "    action_values = {}\n",
    "    for state, actions in state_actions.items():\n",
    "        action_values[state] = {}\n",
    "        for action in actions:\n",
    "            action_values[state][action] = 0.0\n",
    "    \n",
    "    for trace in tqdm.tqdm(episodes):\n",
    "        ret = 0.0\n",
    "        fv = first_visits(trace, state_action_pairs=True)\n",
    "        for first_visit, (state, action, reward) in reversed(list(zip(fv, trace))):\n",
    "            ret = reward + gamma*ret\n",
    "            \n",
    "            if first_visit:\n",
    "                visits[(state, action)] += 1\n",
    "                action_values[state][action] += (\n",
    "                    (1.0 / visits[(state, action)]) * (ret - action_values[state][action])\n",
    "                )\n",
    "                update_policy_(state, action_values, policy)\n",
    "                \n",
    "    return action_values, policy\n",
    "\n",
    "\n",
    "def random_policy(\n",
    "    state_actions: Mapping[StateT, Sequence[ActionT]],\n",
    ") -> Mapping[Tuple[StateT, ActionT], float]:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    policy = {}\n",
    "    \n",
    "    for state, actions in state_actions.items():\n",
    "        for action in actions:\n",
    "            policy[(state, action)] = 0.0\n",
    "        policy[(state, random.choice(actions))] = 1.0\n",
    "        \n",
    "    return policy\n",
    "\n",
    "\n",
    "def update_policy_(\n",
    "    state: StateT, \n",
    "    action_values: Mapping[StateT, Mapping[ActionT, float]],\n",
    "    policy: Mapping[Tuple[StateT, ActionT], float]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    \n",
    "    Example:\n",
    "        >>> p = {(1, \"a\"): 0.0, (1, \"b\"): 1.0}\n",
    "        >>> update_policy_(\n",
    "        ...     state=1,\n",
    "        ...     action_values={1: {\"a\": 100.0, \"b\": 5.0}},\n",
    "        ...     policy=p\n",
    "        ... )\n",
    "        >>> p\n",
    "        {(1, 'a'): 1.0, (1, 'b'): 0.0}\n",
    "    \"\"\"\n",
    "    best_action = None\n",
    "    best_value = -float(\"inf\")\n",
    "    \n",
    "    for action, value in action_values[state].items():\n",
    "        if value > best_value:\n",
    "            best_value = value\n",
    "            best_action = action\n",
    "        policy[(state, action)] = 0.0\n",
    "    policy[(state, best_action)] = 1.0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500000it [01:38, 5056.30it/s]\n"
     ]
    }
   ],
   "source": [
    "%pdb 0\n",
    "action_values, policy = monte_carlo_es(\n",
    "    state_actions=bj_state_actions(),\n",
    "    episodes=bj_simulate_n(stick_20_21, 500000, n_procs=6),\n",
    "    gamma=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((11, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
